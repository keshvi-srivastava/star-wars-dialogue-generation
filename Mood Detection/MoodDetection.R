library(tidyverse) # data manipulation
library(tm) # text mining
library(wordcloud) # word cloud generator
library(wordcloud2) # word cloud generator
library(tidytext) # text mining for word processing and sentiment analysis
library(RWeka) # data mining tasks
library(knitr)
library(ggplot2)

#Loading the files which have generates sentences from the model. Each file has 10 sentences for each character.
yoda_data <- read_csv("/Users/localadmin/Documents/SNLP Project/Results/YodaGeneratedSen.csv")
obiwan_data <- read_csv("/Users/localadmin/Documents/SNLP Project/Results/ObiWanGeneratedSen.csv")
threepio_data <- read_csv("/Users/localadmin/Documents/SNLP Project/Results/ThreepioGeneratedSen.csv")

#Loading all emotion related words from Afinn, Bing and NRC files. These files have lexcions with sentiments
#labelled for each word. 
bing <- read.csv("/Users/localadmin/Documents/SNLP Project/Bing.csv")
nrc <- read.csv("/Users/localadmin/Documents/SNLP Project/NRC.csv")
afinn <- read.csv("/Users/localadmin/Documents/SNLP Project/Afinn.csv")

#Function to clean the corpus for stop word removal, white spaces, punctuation etc.
# cleanCorpus <- function(corpus){
#   
#   corpus.tmp <- tm_map(corpus, removePunctuation)
#   corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
#   corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
#   v_stopwords <- c(stopwords("english"), c("thats","weve","hes","theres","ive","im",
#                                            "will","can","cant","dont","youve","us",
#                                            "youre","youll","theyre","whats","didnt"))
#   corpus.tmp <- tm_map(corpus.tmp, removeWords, v_stopwords)
#   corpus.tmp <- tm_map(corpus.tmp, removeNumbers)
#   return(corpus.tmp)
#   
# }

#This function creates the term-document matrix, that describes the frequency of terms that occur in a documents.
# frequentTerms <- function(text){
#   
#   s.cor <- Corpus(VectorSource(text))
#   s.cor.cl <- cleanCorpus(s.cor)
#   s.tdm <- TermDocumentMatrix(s.cor.cl)
#   s.tdm <- removeSparseTerms(s.tdm, 0.999)
#   m <- as.matrix(s.tdm)
#   word_freqs <- sort(rowSums(m), decreasing=TRUE)
#   dm <- data.frame(word=names(word_freqs), freq=word_freqs)
#   return(dm)
#   
# }
# 
# tokenizer  <- function(x){
#   NGramTokenizer(x, Weka_control(min=2, max=2))
# }
# 
# frequentBigrams <- function(text){
#   s.cor <- VCorpus(VectorSource(text))
#   s.cor.cl <- cleanCorpus(s.cor)
#   s.tdm <- TermDocumentMatrix(s.cor.cl, control=list(tokenize=tokenizer))
#   s.tdm <- removeSparseTerms(s.tdm, 0.999)
#   m <- as.matrix(s.tdm)
#   word_freqs <- sort(rowSums(m), decreasing=TRUE)
#   dm <- data.frame(word=names(word_freqs), freq=word_freqs)
#   return(dm)
# }

# Transforms the text to a tidy data structure with one token per row
tokens_yoda <- yoda_data %>%  
  tidytext::unnest_tokens(word, sentence)
tokens_obiwan <- obiwan_data %>%  
  tidytext::unnest_tokens(word, sentence)
tokens_threepio <- threepio_data %>%  
  tidytext::unnest_tokens(word, sentence)

# The below uses the general purpose lexicon Bing.csv to label the words with sentiments
tokens_yoda %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort=TRUE) %>%
  acast(word ~ sentiment, value.var="n", fill=0) %>%
  comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)

tokens_obiwan %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort=TRUE) %>%
  acast(word ~ sentiment, value.var="n", fill=0) %>%
  comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)

tokens_threepio %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort=TRUE) %>%
  acast(word ~ sentiment, value.var="n", fill=0) %>%
  comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)

# Now, NRC is used to categorize words into positive, negative,anger,anticipation,disgust,fear, joy, sadness, surprise and trust
sentiments_yoda <- tokens_yoda %>% 
  inner_join(nrc, "word") %>%
  dplyr::count(word, sentiment, sort=TRUE) 

sentiments_obiwan <- tokens_obiwan %>% 
  inner_join(nrc, "word") %>%
  dplyr::count(word, sentiment, sort=TRUE) 

sentiments_threepio <- tokens_threepio %>% 
  inner_join(nrc, "word") %>%
  dplyr::count(word, sentiment, sort=TRUE) 

# The words in sentiments_<charactername> form the central point for mood detection of sentences. We have used the parts 
# of sentences 

#ggplot(data=sentiments_threepio, aes(x=reorder(sentiment, -n, sum), y=n)) + 
#  geom_bar(stat="identity", aes(fill=sentiment), show.legend=FALSE) +
#  labs(x="Sentiment", y="Frequency") +
#  theme_bw()

#Visual representation of the words and mood distribution for the words(of sentences) generated by Threepio
sentiments_threepio %>%
  group_by(sentiment) %>%
  arrange(desc(n)) %>%
  slice(1:10) %>%
  ggplot(aes(x=reorder(word, n), y=n)) +
  geom_col(aes(fill=sentiment), show.legend=FALSE) +
  facet_wrap(~sentiment, scales="free_y") +
  labs(y="Frequency", x="Terms") +
  coord_flip() +
  theme_bw()

sentiments_obiwan %>%
  group_by(sentiment) %>%
  arrange(desc(n)) %>%
  slice(1:10) %>%
  ggplot(aes(x=reorder(word, n), y=n)) +
  geom_col(aes(fill=sentiment), show.legend=FALSE) +
  facet_wrap(~sentiment, scales="free_y") +
  labs(y="Frequency", x="Terms") +
  coord_flip() +
  theme_bw()

sentiments_yoda %>%
  group_by(sentiment) %>%
  arrange(desc(n)) %>%
  slice(1:10) %>%
  ggplot(aes(x=reorder(word, n), y=n)) +
  geom_col(aes(fill=sentiment), show.legend=FALSE) +
  facet_wrap(~sentiment, scales="free_y") +
  labs(y="Frequency", x="Terms") +
  coord_flip() +
  theme_bw()
