{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation Model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDW7wel8Eg9R"
      },
      "source": [
        "Model to generate a sequence of following words:\n",
        "1. Extract relevant characters data\n",
        "2. Convert data to token sequence of size 5\n",
        "3. Encode the sentence \n",
        "4. Create a bidirectional LSTM model\n",
        "5. Add Glove word embeddings\n",
        "\n",
        "Reference:\n",
        "\n",
        "~ https://medium.com/@plusepsilon/the-bidirectional-language-model-1f3961d1fb27\n",
        "\n",
        "~ https://www.kaggle.com/guidant/mimicking-star-wars-characters-using-a-i-rnn#2.-Data-Preparation\n",
        "\n",
        "~ https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
        "\n",
        "~ https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "~ https://towardsdatascience.com/simple-text-generation-d1c93f43f340\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVbk-xlwrTER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0616c1-769f-494e-b620-c96e61ae497a"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import os\n",
        "import re\n",
        "from random import randint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import GlobalMaxPool1D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n",
        "\n",
        "# Get Pre-trained GLOVE embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip\n",
        "!ls\n",
        "!pwd\n",
        "\n",
        "#load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('/content/glove.6B.300d.txt')\n",
        "\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-24 00:40:58--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-04-24 00:40:58--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-04-24 00:40:58--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.07MB/s    in 2m 40s  \n",
            "\n",
            "2021-04-24 00:43:38 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "drive\t\t   glove.6B.200d.txt  glove.6B.50d.txt\tsample_data\n",
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "/content\n",
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh7sOK-PsA4O",
        "outputId": "6dbdc930-59a2-4926-a303-f670b139a2f6"
      },
      "source": [
        "# Add file path to where the filtered data may be\n",
        "# One can also add the folder temporarily in the runtime location\n",
        "# path_to_file = '/content/Filtered Data/' \n",
        "# --> Contains all the generated csv files \n",
        "# (SW_EpisodeI.csv, SW_EpisodeII.csv, SW_EpisodeIII.csv, SW_EpisodeIV.csv, SW_EpisodeV.csv, SW_EpisodeVI.csv)\n",
        "# Currently pointing to private drive folder\n",
        "path_to_file = '/content/drive/MyDrive/SNLP Project/Filtered Data/'\n",
        "\n",
        "# Create Dataframe of all characters and dialogues\n",
        "data = pd.DataFrame(columns = ['character', 'dialogue'])\n",
        "\n",
        "for file in os.listdir(path_to_file):\n",
        "    print(file)\n",
        "    df = pd.read_csv(path_to_file+file)\n",
        "    data = data.append(df, ignore_index=True)\n",
        "\n",
        "# Lower case all the characters\n",
        "data['character'] = data[\"character\"].str.lower()\n",
        "\n",
        "# Unify all the characters\n",
        "data['character'] = data.character.replace(\"obi-wan\", \"ben\", regex=True)\n",
        "data['character'] = data.character.replace(\"c-3po\", \"threepio\", regex=True)\n",
        "\n",
        "# Get all the unique characters\n",
        "unique_characters = data.character.unique()\n",
        "\n",
        "# data_dict :\n",
        "#     key   -> character name\n",
        "#     value -> string list all dialogues spoken by the character\n",
        "data_dict = data.groupby('character')['dialogue'].apply(lambda g: g.values.tolist()).to_dict()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SW_EpisodeI.csv\n",
            "SW_EpisodeII.csv\n",
            "SW_EpisodeIII.csv\n",
            "SW_EpisodeIV.csv\n",
            "SW_EpisodeV.csv\n",
            "SW_EpisodeVI.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0k32vUqubpV"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "\n",
        "    # Remove ....\n",
        "    sentence = re.sub('\\.+', ' ', sen)\n",
        "\n",
        "    # Remove punctuations\n",
        "    sentence = re.sub('[%s]' % re.escape(string.punctuation), '', sentence)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    sentence = re.sub(' +', ' ', sentence)\n",
        "\n",
        "    # Remove numbers\n",
        "    sentence = ''.join(filter(lambda x: not x.isdigit(), sentence))\n",
        "\n",
        "    # Lower case\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Return a list of tokens (words)\n",
        "    sentence = sentence.split()\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0zR780bujQj"
      },
      "source": [
        "# Function to retrive all the dialogues spoken by the character\n",
        "# Params : \n",
        "#     name                      -> Name of the character \n",
        "#                                 (eg. ben, yoda, threepio)\n",
        "#     preprocessed_sen_filename -> Name of the output file to write the filtered \n",
        "#                                  data (eg. ObiwanProcessedData, \n",
        "#                                            YodaProcessedData,\n",
        "#                                            ThreepioProcessedData)\n",
        "\n",
        "def get_char_df(name, preprocessed_sen_filename):\n",
        "\n",
        "  global tokens, char_data, token_list\n",
        "\n",
        "  tokens = [preprocess_text(row) for row in data_dict[name]]\n",
        "  char_data = [' '.join(row) for row in tokens]\n",
        "  token_list = [item for sublist in tokens for item in sublist]\n",
        "\n",
        "  preprocessed_output_file = open(preprocessed_sen_filename,\"a\")\n",
        "  for row in tokens:\n",
        "    str_val = ' '.join(row)\n",
        "    preprocessed_output_file.writelines(str_val + '\\n')\n",
        "  preprocessed_output_file.close() "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaGNgcurvVUm"
      },
      "source": [
        "# Function to generate the model and train the same \n",
        "def model_gen(name):\n",
        "\n",
        "  global tokenizer, reverse_word_dict, dataX\n",
        "\n",
        "  # integer encode sequences of words\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(token_list)\n",
        "  unique_words = set(token_list)\n",
        "  sequences_tokenised = tokenizer.texts_to_sequences(tokens)\n",
        "\n",
        "  vocab_size = len(unique_words)+1\n",
        "  n_sentences = len(tokens)\n",
        "\n",
        "  reverse_word_dict = {v: k for k, v in tokenizer.word_index.items()} \n",
        "\n",
        "  # create a weight matrix for words in training docs (wrt GLOVE embeddings)\n",
        "  embedding_matrix = np.zeros((vocab_size, 300))\n",
        "\n",
        "  for word, i in tokenizer.word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  #Create sequences of size 5\n",
        "  seq_length = 5\n",
        "  dataX = []\n",
        "  dataY = []\n",
        "  for dialogue in sequences_tokenised:\n",
        "    for i in range(len(dialogue)-5):\n",
        "      dataX.append(dialogue[i:i+5])\n",
        "      dataY.append(dialogue[i+5])\n",
        "\n",
        "  y = np.zeros((len(dataY), vocab_size), dtype=np.bool)\n",
        "  for i, sentence in enumerate(dataY):\n",
        "    y[i, dataY[i]] = 1\n",
        "\n",
        "  X = pad_sequences(dataX, maxlen=5)\n",
        "\n",
        "  learning_rate = 0.001\n",
        "  embedding_layer = Embedding(vocab_size,\n",
        "                                300,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=seq_length,\n",
        "                                trainable=False)\n",
        "  inp = Input(shape=(seq_length,))\n",
        "  x = embedding_layer(inp)\n",
        "  x = Bidirectional(LSTM(200,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
        "  x = GlobalMaxPool1D()(x)\n",
        "  x = Dense(vocab_size,activation='relu')(x)\n",
        "  x = Dense(vocab_size,activation='sigmoid')(x)\n",
        "  model = Model(inputs=inp,outputs=x)\n",
        "  optimizer = Adam(lr=learning_rate)\n",
        "\n",
        "  #call the functions in the metrics \n",
        "  model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  batch_size = 32 # minibatch size\n",
        "  num_epochs = 100 # number of epochs\n",
        "\n",
        "  callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
        "              ModelCheckpoint(filepath=\"./\" + name + 'model_gen_sentences.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n",
        "                              monitor='val_loss', verbose=0, mode='auto', period=2)]\n",
        "  #fit the model\n",
        "  history = model.fit(X, y,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True,\n",
        "                    epochs=num_epochs,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_split=0.1)\n",
        "\n",
        "  #save the model\n",
        "  model.save(\"/content/drive/MyDrive/Colab Notebooks/\" + name + \"_model_generate_sentences.h5\")\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "996E0XhExCfk"
      },
      "source": [
        "# Function to draw out the most likely next word from predicted matrix \n",
        "def sample(preds, temperature=2.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMtVyVAFw95c"
      },
      "source": [
        "# Function to generate the sentences based on random seed words\n",
        "def generate_sentence(model):\n",
        "\n",
        "  #initiate sentences\n",
        "  generated = ''\n",
        "  sentence = dataX[randint(0,len(dataX))]\n",
        "  sentence = [' '.join([reverse_word_dict[word] for word in sentence])]\n",
        "  generated += sentence[0]\n",
        "\n",
        "  # Generate the next five words\n",
        "  for i in range(5):\n",
        "    \n",
        "      seq = tokenizer.texts_to_sequences(sentence)\n",
        "      padded = pad_sequences(seq, maxlen=5)\n",
        "\n",
        "      #calculate next word\n",
        "      preds = model.predict(padded, verbose=0)[0]\n",
        "      next_index = sample(preds, 0.33)\n",
        "      next_word = reverse_word_dict[next_index]\n",
        "\n",
        "      #add the next word to the text\n",
        "      generated += \" \" + next_word\n",
        "      sentence = [' '.join(sentence[0].split()[1:]) + \" \" + next_word]\n",
        "\n",
        "  return (generated)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnLNMVrlxkjH"
      },
      "source": [
        "# Get the average BLEU score for the generated dialogues\n",
        "# Params : \n",
        "#     ref -> List of tokenised training dialogues \n",
        "#     gen -> List of generated dialogues\n",
        "\n",
        "def get_bleu_score(ref, gen):\n",
        "  bleu_score = 0\n",
        "  for sen in gen:\n",
        "    bleu_score += sentence_bleu(ref, sen.split())\n",
        "  return (bleu_score/10)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nLBZkvFxVSk"
      },
      "source": [
        "# Get the average BLEU score for the generated dialogues\n",
        "# Params : \n",
        "#     name                      -> Name of character\n",
        "#     preprocessed_sen_filename -> Name of filtered dialogues output file\n",
        "#     output_sen_filename       -> Name of generated dialgoues output file\n",
        "\n",
        "def main(name, preprocessed_sen_filename, output_sen_filename):\n",
        "\n",
        "  # Get individual character Dataframe\n",
        "  get_char_df(name, preprocessed_sen_filename)\n",
        "\n",
        "  # Prepare and train the model\n",
        "  model_gen(name)\n",
        "\n",
        "  # Load the trained model\n",
        "  model = load_model(\"/content/drive/MyDrive/Colab Notebooks/\" + name + \"_model_generate_sentences.h5\")\n",
        "\n",
        "  # Generate 10 random sentences from 10 randomly selected seed sentences from original tokens\n",
        "  n_sentences_gen = 10\n",
        "  generated_sentences = []\n",
        "  for i in range(n_sentences_gen):\n",
        "    generated_sentences.append(generate_sentence(model))\n",
        "\n",
        "  # Create output file for generated sentences\n",
        "  output_file = open(output_sen_filename,\"a\")\n",
        "  for row in generated_sentences:\n",
        "    output_file.writelines(row + '\\n')\n",
        "  \n",
        "\n",
        "  # Check Bleu Score of the output\n",
        "  output_file.writelines('\\n')\n",
        "  output_file.writelines('BLEU score for {} -> {}'.format(name, get_bleu_score(tokens, generated_sentences)))\n",
        "\n",
        "  output_file.close()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPDkiC-CT5UD"
      },
      "source": [
        "Below are the commands to run to generate the dialogues for three characters ( Obiwan-Kenobi, Yoda and C3PO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W59E3bRf1zyS",
        "outputId": "1517c113-2b3f-4349-fba4-dc3cfdd35b87"
      },
      "source": [
        "main(\"ben\", \"ObiWanProcessedData.txt\",\"ObiWanGeneratedSen.txt\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 5)]               0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 5, 300)            435300    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 5, 400)            801600    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1451)              581851    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1451)              2106852   \n",
            "=================================================================\n",
            "Total params: 3,925,603\n",
            "Trainable params: 3,490,303\n",
            "Non-trainable params: 435,300\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/100\n",
            "102/102 [==============================] - 18s 125ms/step - loss: 6.7106 - accuracy: 0.0400 - val_loss: 6.4071 - val_accuracy: 0.0583\n",
            "Epoch 2/100\n",
            "102/102 [==============================] - 12s 118ms/step - loss: 5.8807 - accuracy: 0.0574 - val_loss: 6.3386 - val_accuracy: 0.0778\n",
            "Epoch 3/100\n",
            "102/102 [==============================] - 12s 117ms/step - loss: 5.2769 - accuracy: 0.0933 - val_loss: 6.6067 - val_accuracy: 0.0861\n",
            "Epoch 4/100\n",
            "102/102 [==============================] - 12s 115ms/step - loss: 4.3622 - accuracy: 0.1296 - val_loss: 7.1187 - val_accuracy: 0.0889\n",
            "Epoch 5/100\n",
            "102/102 [==============================] - 12s 118ms/step - loss: 3.1586 - accuracy: 0.2611 - val_loss: 8.2154 - val_accuracy: 0.0778\n",
            "Epoch 6/100\n",
            "102/102 [==============================] - 12s 114ms/step - loss: 1.6803 - accuracy: 0.5850 - val_loss: 9.6735 - val_accuracy: 0.0472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO7LVZJI3R8A",
        "outputId": "4e66e686-a555-4c08-c8f1-4c4e516d61ce"
      },
      "source": [
        "main(\"yoda\", \"YodaProcessedData.txt\",\"YodaGeneratedSen.txt\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 5)]               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 5, 300)            190500    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 5, 400)            801600    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 635)               254635    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 635)               403860    \n",
            "=================================================================\n",
            "Total params: 1,650,595\n",
            "Trainable params: 1,460,095\n",
            "Non-trainable params: 190,500\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 9s 111ms/step - loss: 6.1655 - accuracy: 0.0383 - val_loss: 5.5544 - val_accuracy: 0.0896\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 4s 94ms/step - loss: 5.3693 - accuracy: 0.0694 - val_loss: 5.6322 - val_accuracy: 0.0970\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 5.0151 - accuracy: 0.0997 - val_loss: 5.6267 - val_accuracy: 0.0896\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 4.6079 - accuracy: 0.1288 - val_loss: 5.5821 - val_accuracy: 0.1493\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 4s 92ms/step - loss: 3.8493 - accuracy: 0.2069 - val_loss: 5.8017 - val_accuracy: 0.1269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q521x36GD4hz",
        "outputId": "fb93fed0-6764-4cea-d098-35b158d7d140"
      },
      "source": [
        "main(\"threepio\", \"ThreepioProcessedData.txt\",\"ThreepioGeneratedSen.txt\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 5)]               0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 5, 300)            308700    \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 5, 400)            801600    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1029)              412629    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1029)              1059870   \n",
            "=================================================================\n",
            "Total params: 2,582,799\n",
            "Trainable params: 2,274,099\n",
            "Non-trainable params: 308,700\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/100\n",
            "72/72 [==============================] - 14s 115ms/step - loss: 6.5292 - accuracy: 0.0234 - val_loss: 6.1628 - val_accuracy: 0.0627\n",
            "Epoch 2/100\n",
            "72/72 [==============================] - 7s 102ms/step - loss: 5.8048 - accuracy: 0.0449 - val_loss: 6.2382 - val_accuracy: 0.0588\n",
            "Epoch 3/100\n",
            "72/72 [==============================] - 8s 106ms/step - loss: 5.2745 - accuracy: 0.0674 - val_loss: 6.1995 - val_accuracy: 0.0824\n",
            "Epoch 4/100\n",
            "72/72 [==============================] - 8s 105ms/step - loss: 4.3407 - accuracy: 0.1212 - val_loss: 6.6749 - val_accuracy: 0.0980\n",
            "Epoch 5/100\n",
            "72/72 [==============================] - 7s 104ms/step - loss: 3.0007 - accuracy: 0.3116 - val_loss: 7.3713 - val_accuracy: 0.0902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZyLHcauEAfi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}