{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model2 - Sliding Window.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1jbhGM-ZCONubRW0Bo_C5oe8qpoVsMBQ3",
      "authorship_tag": "ABX9TyMCPLReiQmJoFAF6xRG7xCZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keshvi-srivastava/star-wars-dialogue-generation/blob/main/Model2_Sliding_Window.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5_xnpckxX9k"
      },
      "source": [
        "Model to generate a sequence of following words:\n",
        "1. Convert the data into token list\n",
        "2. Convert data to token sentences with sliding windows\n",
        "3. Encode the sentence\n",
        "4. Simple LSTM model\n",
        "\n",
        "- Makes sentences sequence from the whole token list\n",
        "- Make a sliding window of size 5 each\n",
        "\n",
        "Reference:\n",
        "\n",
        "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "https://www.kaggle.com/guidant/mimicking-star-wars-characters-using-a-i-rnn#2.-Data-Preparation\n",
        "\n",
        "https://towardsdatascience.com/simple-text-generation-d1c93f43f340"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU_VCC32nkbr"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "import string\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVqDYVkzxWcv",
        "outputId": "2b460c63-3408-4a38-ca69-9dd271ee39af"
      },
      "source": [
        "path_to_file = '/content/drive/MyDrive/SNLP Project/Filtered_Data/'\n",
        "\n",
        "data = pd.DataFrame(columns = ['character', 'dialogue'])\n",
        "\n",
        "for file in os.listdir(path_to_file):\n",
        "    print(file)\n",
        "    df = pd.read_csv(path_to_file+file)\n",
        "    data = data.append(df, ignore_index=True)\n",
        "\n",
        "data['character'] = data[\"character\"].str.lower()\n",
        "\n",
        "data['character'] = data.character.replace(\"anakin\", \"vader\", regex=True)\n",
        "data['character'] = data.character.replace(\"obi-wan\", \"ben\", regex=True)\n",
        "data['character'] = data.character.replace(\"c-3po\", \"threepio\", regex=True)\n",
        "\n",
        "unique_characters = data.character.unique()\n",
        "\n",
        "data_dict = data.groupby('character')['dialogue'].apply(lambda g: g.values.tolist()).to_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SW_EpisodeI.csv\n",
            "SW_EpisodeII.csv\n",
            "SW_EpisodeIII.csv\n",
            "SW_EpisodeIV.csv\n",
            "SW_EpisodeV.csv\n",
            "SW_EpisodeVI.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-urz_i6iyDrE"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "\n",
        "    # Remove numbers\n",
        "    sentence = re.sub(\" \\d+\", \" \", sen)\n",
        "\n",
        "    # # Single character removal\n",
        "    # sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sen)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    # Remove ....\n",
        "    sentence = re.sub('\\.+', ' ', sentence)\n",
        "\n",
        "    # Remove punctuations\n",
        "    sentence = re.sub('[%s]' % re.escape(string.punctuation), '', sentence)\n",
        "\n",
        "    # Lower case\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Return a list of tokens (words)\n",
        "    sentence = sentence.split()\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtXWNgPFx8wr",
        "outputId": "e57468ae-d8a9-4493-8e0e-23e08062d5bb"
      },
      "source": [
        "obi_wan_tokens = [preprocess_text(row) for row in data_dict['ben']]\n",
        "print(obi_wan_tokens[:5])\n",
        "\n",
        "obi_wan_data = [' '.join(row) for row in obi_wan_tokens]\n",
        "print(obi_wan_data)\n",
        "\n",
        "obi_wan_token_list = [item for sublist in obi_wan_tokens for item in sublist]\n",
        "print(obi_wan_token_list)\n",
        "\n",
        "print(\"Total # of tokens(words)\")\n",
        "print(len(obi_wan_token_list))\n",
        "\n",
        "print(\"Total # of unique tokens(words)\")\n",
        "print(len(set(obi_wan_token_list)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['i', 'have', 'a', 'bad', 'feeling', 'about', 'this'], ['its', 'not', 'about', 'the', 'mission', 'master', 'its'], ['master', 'yoda', 'says', 'i', 'should', 'be', 'mindful', 'of', 'the', 'future'], ['yes', 'master', 'how', 'do', 'you', 'think', 'the', 'trade', 'viceroy', 'will', 'deal', 'with'], ['offhand', 'id', 'say', 'this', 'mission', 'is', 'past', 'the', 'negotiaion', 'stage']]\n",
            "['i have a bad feeling about this', 'its not about the mission master its', 'master yoda says i should be mindful of the future', 'yes master how do you think the trade viceroy will deal with', 'offhand id say this mission is past the negotiaion stage', 'they have shield generators', 'its an invisible army', 'you were right about one thing master the negotiations were', 'sorry master the water fried my weapon', 'yes master', 'whats this', 'when they find us they will crush us grind us into little', 'dont worry this has not been our day for warm welcomes', 'after those droids take control of the surface they will come', 'you and the naboo form a symbiont circle what happens to noe of', 'master whats a bongo', 'we are short of time master', 'master why do you keep dragging these pathetic life forms along', 'why were you banished jar jar', 'they banished you because youre clumsy', 'give me the controls', 'were losing power', 'powers back', 'you overdid it', 'what is it', 'get up here', 'that was close', 'ill take care of that obiwan heads toward the group of', 'now stay here and keep out of trouble', 'here master tatooine its small out of the way poor the', 'its risky but theres no alternative', 'tahyts it tatooine', 'sorry quigons right youll make things less obvious', 'cantd the hyperdrive generator is gone we will need a new', 'i fell it also master', 'v o a few containers of supplies the queens wardrobe maybe', 'this storms going to slow them down', 'its a trick send no reply send no transmission of any kind', 'the queen is upset but absolutly no reply was sent', 'what if it is true and the people are dying', 'what if this plan fails master we could be stuck here for a', 'yes master', 'wait a minute', 'all right ive got it', 'something must be wrong with the transmission', 'strange the transmission seems to be in good order but the', 'even master yoda doesnt have a midichlorian count that high', 'what does it mean', 'why do i sense weve picked up another pathetic life form', 'over there fly low', 'what was it', 'the boy will not pass the councils tests master and you know', 'dont defy the council master not again', 'master you could be sitting on the council by now if you would', 'iam ready to face the trials', 'it is not disrespect master it is the truth', 'the boy is dangerous they all sense it why cant you', 'the wars over no need for it now', 'a droid control ship', 'jar jar is on his way to the gungan city master', 'do you think the queens idea will work', 'im im sorry for my behavior master it is not my place to', 'more likely they were wiped out', 'and theres an even bigger danger if the vicroy escapes your', 'master master', 'no', 'yes master', 'quigon believed in him i believe in quigon', 'master yoda igave quigon my word i will train anakin without', 'he is one with the force anakin you must let go', 'i am your master now you will become a jedi i promise', 'its good to see you too jar jar', 'its a great pleasure to see you againmlady', 'im sure the jedi council havetheir reasons', 'our presence will be invisiblemlady', 'frowningwere here to protect yousenator not to start aninvestigation', 'we are not going to exceed ourmandate my young padawan learner', 'we are not going through thisexercise again anakin you willpay attention to my lead', 'what', 'we will do as the council hasinstructed and you will learnyour place young one', 'anakin youre focusing on thenegative again be mindful of yourthoughts she was glad to see us now lets check the security here', 'captain typho has more than enoughmen downstairs no assassin willtry that way any activity uphere', 'whats going on', 'what is she thinking', 'its not an intruder im worriedabout there are many other waysto kill a senator', 'youre using her as bait', 'its too risky and your sensesarent that attuned youngapprentice', 'possibly', 'you look tired', 'because of your mother', 'dreams pass in time', 'mind your thoughts anakin theybetray you youve made acommitment to the jedi order acommitment not easily broken and dont forget shes apolitician theyre not to betrusted', 'its been my experience thatsenators are only focused onpleasing those who fund theircampaigns and they are morethan willing to forget theniceties of democracy to get thosefunds', 'palpatines a politician iveobserved that he is very clever atfollowing the passions andprejudices of the senators', 'i sense it too', 'i have a bad feeling about this', 'what took you so long', 'if youd spend as much timeworking on your saber skills asyou do on your wit young padawanyou would rival master yoda as aswordsman', 'only in your mind my very youngapprentice careful hey easy', 'i dont mind flying butwhat youre doing is suicide', 'just slow down', 'continuingwait dont go in there', 'continuingyou know i dont like it when youdo that', 'what are you doing hes gonnablast me', 'that was too close', 'what', 'whooooaaa dont do that idont like it when you do that', 'its dangerous near those powercouplings slow down dont gothrough there', 'continuingyiiii what are you doing', 'sarcasticallyoh that was good', 'stop', 'im crazy im crazy imcrazy', 'angrilyit didnt work weve stalledand you almost got us killed', 'very angrilyit was stupid', 'furiousbut you didnt and now wevelost him', 'where are you going he wentdown there the other way', 'what do you mean \\x91you think\\x92what kind of shortcut he wentcompletely the other way you\\x92velost him', 'sarcasticoh so that\\x92s why we\\x92re going inthe wrong direction', 'continuingwell you lost him', 'anakin', 'patience', 'continuinghere next time try not to lose it', 'a jedis saber is his mostprecious possession', 'he must keep it with him at alltimes', 'this weapon is your life', 'but you havent learned anythinganakin', 'why do i think you are going to bethe death of me', 'then why dont you listen to me', 'do you see him him', 'then be extra careful nods to a roomcheck it out', 'to get a drink', 'you dont want to sell me deathsticks', 'you want to go home and rethink your life', 'do you know who it was you weretrying to kill', 'who hired you', 'this wounds going to needtreatment', 'toxic dart', 'anakin you stay put on naboo do not attract any attention doabsolutely nothing withoutchecking in with me or the council', 'to padmei will get to the bottom of thisplot quickly mlady youll beback here in no time', 'may the force be with you', 'i hope he doesnt try anythingfoolish', 'its a toxic dart i need to knowwhere it came from and who made it', 'excuse me could you try againplease', 'thanks for your assistance youmay not be able to figure thisout but i think i know someonewho might', 'im looking for dexter', 'hes not in trouble itspersonal', 'hey dex', 'thank you', 'hey dex', 'you can tell me what this is', 'do you know where it came from', 'kamino kyberdart i wonder whyit didnt show up in any analysisarchive', 'well dex if droids could think wewouldnt be here would welaughingkamino doesnt sound familiar is it part of the republic', 'cloners are they friendly', 'on what dex', 'distracted in thoughtyes yes i did', 'i never understood why he quit only twenty jedi have ever leftthe order', 'what happened', 'surprisedreally', 'interesting im still not surei understand', 'yes im trying to find a planetsystem called kamino it doesntseem to show upon any of thearchive charts', 'noddingaccording to my information itshould be in this quadrantsomewhere just south of therishi maze', 'all three actually', 'absolutely', 'wouldnt that be on record', 'thats impossible perhaps thearchives are incomplete', 'i am sorry to disturb you master', 'i\\x92m looking for a planet describedto me by an old friend i trusthim but the system doesn\\x92t showup on the archive maps', 'this is where it ought to be but it isn\\x92t gravity is pullingall the stars in this area inwardto this spot there should be astar here but there isn\\x92t', 'but master yoda who could haveerased information from thearchives that\\x92s impossibleisn\\x92t it', 'i am concerned for my padawan heis not ready to be on his own', 'but he still has much to learn and his abilities have made him well arrogant i realise nowwhat you and master yoda knew fromthe beginning the boy was tooold to start the training and', 'master he should not have beengiven this assignment im afraidanakin wont be able to protectthe senator', 'he has a an emotionalconnection with her its beenthere since he was boy nowhes confused distracted', 'has master yoda gained any insightinto whether or not this war willcome about', 'warilyim expected', 'obiwan kenobi', 'you make me feel most welcome', 'improvisingthat is good news', 'im sorry master', 'oh yes sidodyas', 'thats why im here', 'where is this bounty hunter now', 'unaltered', 'i would like to meet this jangofett', 'you mentioned growthacceleration', 'and these', 'very impressive', 'carefullytell me prime minister when mymaster sidodyas first contactedyou did he say the order wasfor himself or', 'astonishedthe repubic', 'arfour arfour', 'arfour relay this scramble codefive to courscant care of theold folks home', 'your clones are very impressive you must be very proud', 'arent we all', 'continuingever made your way as far into theinterior as coruscant', 'recently', 'then you must know master sidodyas', 'sidodyas isn\\x92t he the jedi whohired you for this job', 'no i thought', 'curious', 'it seems to me its your army being that they are all clones ofyou', 'i look forward to seeing them inaction thank you for your timejango', 'i wont forget', 'guiding light to old folkshome', 'starships from the tradefederation and the commerce guildsare taking deliveries of battledroids from the foundries ongeonosis', 'the droid foundry seems to beworking at full capacity i amgoing to go down and investigate i will bring jango fett back homefor interrogation', 'one more thing jango mentionedhe was recruited by someone nameddarth tyranus any idea who thatmight be', 'do you believe he could be themysterious sidodyas who made thedeal for the clone army', 'let me see if i can figure outwhats going on first', 'alliance have pledged theirarmies to count dooku and areforming an wait ah attack i dont make it aaaggghhh', 'i was beginning to wonder if youhad gotten my message', 'it looks like youre going a goodjob so far', 'take the one the left illtake the one on the right', 'it looks like shes already on topof things', 'someones got to shut down these droids', 'hold on look over there', 'no forget her we have to goafter dooku', 'dont let your personal feelingsget in the way weve got a job todo', 'anakin shes all right look', 'continuingfollow that speeder', 'to anakinwe move in together you slowlyon the', 'anakin no', 'i dont think so', 'you will have to prove it', 'anakin', 'on his way back to naboo he isescorting senator amidala home', 'continuingi must admit without the clonesit would not have been a victory', 'look out four droids inbound', 'weve got to split them up', 'easy for you to say why am i always the bait', 'anakin theyre all over me', 'im going down on the deck', 'hurry up i dont like this', 'continuing ouch', 'continuing dont even try to fix it arfour ive shut it down', 'next time youre the bait now lets find the command ship and get on with it', 'i see it oh this is going to be easy', 'not this time theres too much at stake we need help odd ball do you copy', 'mark my position and form your squad up behind me', 'add five trifighters on the right', 'take it easy anakin', 'five more on the right', 'im going high and right', 'stay with me swing back and right help me engage back off let them pass between us', 'all right engage and hurry these droids are all over me like a rash', 'anakin you have four on your tail', 'four more closing from your left', 'break right and go high', 'to himself he still has much to learn', 'too dangerous first jedi rule', 'just keep me steady hold on not yet now break left', 'continuing youll never get through there anakin its too tight', 'use the force think yourself through the ship will follow', 'no no they are doing their job so we can do ours head for the command ship', 'they overshot us', 'all right arfour no no nothing too fancy', 'flying is for droids', 'continuing im hit anakin', 'arfour be careful you have one', 'oh dear theyre shutting down all the controls', 'the mission get to the command ship get the chancellor im running out of tricks here', 'continuing in the name of', 'anakin hold your fire hold your fire youre not helping here', 'wait wait i cant see a thing my cockpits fogging theyre all over me anakin', 'hold on anakin youre going to get us both killed get out of here theres nothing more you can do', 'blast it i cant see my controls are gone', 'artoo hit the buzz droids center eye', 'great artoo', 'have you noticed the shields are still up', 'oh i have a bad feeling about this', 'artoo locate the chancellor', 'the chancellors signal is coming from right there the observation platform at the top of that spire', 'i sense a trap', 'spring the trap', 'here take this and wait for orders', 'we are here to relieve you of chancellor palpatine not join him', 'anakin', 'fuel the slightest charge from our sabers will send this ship into oblivion thats why theyve stopped shooting', 'your idea of safe is not the same as mine', 'six droids coming our way', 'well never get through that its too small', 'anakin this is no time for jokes were in serious trouble here', 'that wont hold when the fuel hits those power dischargers', 'you still have much to learn anakin', 'all right you win i have much to learn lets go', 'did you press the stop button', 'no', 'we dont want to get out we want to get moving artoo artoo do you copy activate elevator looks at control panel', 'artoo', 'continuing always on the move', 'os artoo switch on the comlink artoo do you hear me artoo we gave you a job to do artoo', 'stop stop artoo we need to be going up', 'os artoo do you copy artoo do you hear me artoo we need to be going up not down', 'stop artoo we need to go up stop stop', 'now thats better', 'oh its you', 'well artoo has been', 'did i say anything', 'i didnt say anything', 'bows chancellor', 'quietly to anakin this time we will do it together', 'chancellor palpatine sith lords are our specialty', 'you wont get away this time dooku', 'did i miss something', 'what is that', 'continuing oops', 'too late jump', 'lets see if we can find something in the hangar bay thats still flyable come on', 'none of those ships will get us anywhere', 'dont look at me i dont know', 'well head toward the bridge and see if we can find an escape pod', 'wait a minute howd this happen were smarter than this', 'oh so all of a sudden its my fault', 'im open to suggestions here', 'patience thats your plan is it', 'do you have a plan b', 'that depends upon your point of view hah', 'anakin try not to upset him we have a job to do', 'not this time and this time you wont escape', 'grievous can you fly a cruiser like this', 'well', 'steady attitude eighteen degrees', 'temp steady hatches open flaps extended drag fins', 'not to worry were still flying half the ship', 'careful were heating uptwelve thousand thirteen thousand', 'eight plus sixtyforty eight plus sixtytwenty eight plus sixty temp ten thousand nine thousand were in the atmosphere', 'steady steady', 'five thousand three thousand two thousand fireships on the left and the right', 'copy that landing strips straight ahead', 'easyeasy', 'another happy landing', 'oh no im not brave enough for politics i have to report to the council besides someone needs to be the poster boy', 'sorry old friend let us not forget that you rescued me from the buzz droids and you killed count dooku and you rescued the chancellor carrying me unconscious on your back and you managed to land that bucket of bolts safely', 'the endless speeches anakin lets be fair today you are the hero and you deserve your glorious day with the politicians', 'ninth time that business on cato nemoidia doesnt count ill see you at the briefing', 'you missed the report on the outer rim sieges', 'in short they are going very well saleucami has fallen and master vos has moved his troops to boz pity', 'the senate is expected to vote more executive powers to the chancellor today', 'anakin be careful of your friend palpatine', 'he has requested your presence', 'he would not say', 'all of this is unusual and its making me feel uneasy youre probably aware that relations between the council and the chancellor are stressed', 'the force grows dark anakin and we are all affected by it be wary of your feelings', 'it may take some time we do not have many ships to spare', 'and yet it would be fatal for us to allow the droid armies to regroup', 'hes right that is a system we cannot afford to lose its the main navigation route for the southwestern quadrant', 'calm down anakin you have been given a great honor to be on the council at your age its never happened before listen to me anakin the fact of the matter is youre too close to the chancellor the council doesnt like it when he interferes in jedi affairs', 'but its what you wanted your friendship with chancellor palpatine seems to have paid off', 'anakin regardless of how it happened you find yourself in a delicate situation', 'i warned you there was tension between the council and the chancellor i was very clear why didnt you listen you walked right into it', 'no it is not anakin i worry when you speak of jealousy and pride those are not jedi thoughts theyre dangerous dark thoughts', 'i hope so', 'anakin the only reason the council has approved your appointment is because the chancellor trusts you', 'anakin look i am on your side i didnt want to see you put in this situation', 'takes a deep breath the council wants you to report on all of the chancellors dealings they want to know what hes up to', 'we are at war anakin the jedi council is sworn to uphold the principles of the republic even if the chancellor does not', 'this assignment is not to be on record the council asked me to approach you on this personally', 'that is why you must help us anakin our allegiance is to the senate not to its leader who has managed to stay in office long after his term has expired', 'yes but use your feelings anakin something is out of place', 'the council is asking you', 'anakin did not take to his assignment with much enthusiasm', 'hell be all right i trust him with my life', 'with all due respect master is he not the chosen one is he not to destroy the sith and bring balance to the force', 'he will not let me down he never has', 'does everyone agree', 'oh no youre quite right but i do have the most experience with his ways of combat', 'has anakin been to see you', 'i know he deserves it he is impatient strong willed very opinionated but truly gifted', 'you should be a jedi padme', 'its anakin hes becoming moody and detached hes been put in a difficult position as the chancellors representative but i think its more than that i was hoping he may have talked to you', 'neither of you is very good at hiding your feelings either', 'i know how he feels about you', 'nothing he didnt have to', 'i know you both too well i can see you two are in love padme im worried about him', 'continuing i fear your relationship has confused him hes changed considerably since we returned', 'continuing padme im not telling the council about any of this i i hope i didnt upset you were all friends i care about both of you', 'please do what you can to help him', 'oh i agree however it may turn out just to be a wild bantha chase', 'you are strong and wise anakin and i am very proud of you i have trained you since you were a small boy i have taught you everything i know and you have become a far greater jedi than i could ever hope to be and you have saved my life more times than i can remember but be patient anakin it wont be long before the council makes you a jedi master', 'continuing dont worry i have enough clones with me to take three systems the size of utapau i think ill be able to handle the situation even without your help', 'goodbye old friend may the force be with you', 'ill keep them distracted until you get there just dont take too long', 'laughing cato nemoidia for starters', 'very well the burden is on me not to destroy all the droids before you get there', 'save your energy', 'dont give up padme dont give up', 'unfortunately the war', 'with your kind permission i should like some fuel and to use your city as a base as i search nearby systems for general grievous', 'i understand', 'tell your people to take shelter if you have warriors now is the time', 'continuing geenine take the fighter back to the ship im staying here tell cody ive made contact', 'i need transportation', 'get it for me', 'this one', 'good girl boga', 'hello there', 'your move', 'you forget i trained the jedi that defeated count dooku', 'i may not defeat your droids but my troops certainly will', 'i dont think so', 'so uncivilized', 'commander contact your troops tell them to move to the higher levels', 'thank you cody smiling now lets get a move on weve got a battle to win here', 'emergency code nine thirteen i have no contact on any frequency are there any jedi out there anywhere', 'continuing ive locked on repeat', 'senator organa my clone troops turned on me i need help', 'you were attacked by your clones also', 'how many other jedi managed to survive', 'have we had any contact from the temple', 'well then we must go back if there are other stragglers they will fall into the trap and be killed', 'yes master there is too much at stake here and we need a clearer picture of what has happened', 'no i dont think so the chancellor will not be able to control the thousands of star systems without keeping the senate intact', 'it would be better if we stayed with the senator', 'there are several battalions of clone troopers on every level many are dressed as jedi', 'not even the younglings survived', 'who who could have done this', 'ive recalibrated the code warning all surviving jedi to stay away', 'wait master there is something i must know', 'i must know the truth master', 'continuing it cant be it cant be', 'i cant watch any more', 'send me to kill the emperor i will not kill anakin', 'he is like my brother i cannot do it', 'how could it have come to this', 'i do not know where the emperor has sent him i dont know where to look', 'may the force be with you master yoda', 'has anakin been here', 'the republic has fallen padme the jedi order is no more', 'i believe we have been part of a plot hundreds of years in the making', 'no padme its over the sith now rule the galaxy as they did before the republic', 'im here looking for anakin when was the last time you saw him', 'and do you know where he is now', 'padme i need your help hes in grave danger', 'from himself padme anakin has turned to the dark side', 'i have seen a security hologram of him killing younglings', 'he was deceived by a lie we all were it appears that the chancellor is behind everything including the war palpatine is the sith lord weve been looking for after the death of count dooku anakin became his new apprentice', 'padme i must find him', 'he has become a very great threat', 'anakin is the father isnt he', 'continuing im so sorry', 'let her go anakin', 'let her go', 'you have done that yourself', 'your anger and your lust for power have already done that', 'continuing you have allowed this dark lord to twist your mind until now until now you have become the very thing you swore to destroy', 'your new empire', 'anakin my allegiance is to the republic to democracy', 'only a sith lord deals in absolutes i will do what i must', 'ive heard that before anakin but i never thought id hear it from you', 'continuing the flaw of power is arrogance', 'i have failed you anakin i was never able to teach you to think', 'from the sith anakin chancellor palpatine is evil', 'well then you are lost', 'its over anakin i have the high ground', 'dont try it', 'continuing you were the chosen one it was said that you would destroy the sith not join them it was you who would bring balance to the force not leave it in darkness', 'you were my brother anakin i loved you', 'shes dying', 'dont give up padme', 'you have twins padme they need you hang on', 'save your energy', 'we must take them somewhere the sith will not sense their presence', 'and what of the boy', 'i will take the child and watch over him master yoda do you think anakins twins will be able to defeat darth sidious', 'training', 'who', 'quigon but how could he accomplish this', 'i will be able to talk with him', 'hello there come here my little friend dont be afraid', 'dont worry hell be all right', 'rest easy son youve had a busy day youre fortunate youre still in one piece', 'the jundland wastes are not to be traveled lightly tell me young luke what brings you out this far', 'obiwan kenobi obiwan now thats a name i havent heard in a long time a long time', 'oh hes not dead no not yet', 'well of course of course i know him hes me i havent gone by the name obiwan since oh before you were born', 'dont seem to remember ever owning a droid very interesting', 'i think we better get indoors the sand people are easily startled but they will soon be back and in greater numbers', 'quickly son theyre on the move', 'thats what your uncle told you he didnt hold with your fathers ideals thought he should have stayed here and not gotten involved', 'yes i was once a jedi knight the same as your father', 'he was the best starpilot in the galaxy and a cunning warrior i understand youve become quite a good pilot yourself and he was a good friend which reminds me', 'i have something here for you your father wanted you to have this when you were old enough but your uncle wouldnt allow it he feared you might follow old obiwan on some damnedfool idealistic crusade like your father did', 'your fathers lightsaber this is the weapon of a jedi knight not as clumsy or as random as a blaster', 'an elegant weapon for a morecivilized time for over a thousand generations the jedi knights were the guardians of peace and justice in the old republic before the dark times before the empire', 'a young jedi named darth vader who was a pupil of mine until he turned to evil helped the empire hunt down and destroy the jedi knights he betrayed and murdered your father now the jedi are all but extinct vader was seduced by the dark side of the force', 'well the force is what gives the jedi his power its an energy field created by all living things it surrounds us and penetrates us it binds the galaxy together', 'now lets see if we cant figure out what you are my little friend and where you come from', 'i seem to have found it', 'you must learn the ways of the force if youre to come with me to alderaan', 'i need your help luke im getting too old for this sort of thing she needs your help', 'thats your uncle talking', 'learn about the force luke', 'you must do what you feel is right of course', 'they didnt but we are meant to think they did these tracks are side by side sand people always ride single file to hide there numbers', 'and these blast points too accurate for sand people only imperial stormtroopers are so precise', 'wait luke its too dangerous', 'theres nothing you could have done luke had you been there youd have been killed too and the droids would now be in the hands of the empire', 'mos eisley spaceport you will never find a more wretched hive of scum and villainy we must be cautious', 'theyre for sale if you want them', 'you dont need to see his identification', 'looking for', 'he can go about his business', 'move along', 'the force can have a strong influence on the weakminded you will find it a powerful ally', 'well most of the best freighter pilots can be found here only watch your step this place can be a little rough', 'this little one isnt worth the effort come let me buy you something', 'this is chewbacca hes firstmate on a ship that might suit our needs', 'yes indeed if its a fast ship', 'should i have', 'only passengers myself the boy two droids and no questions asked', 'lets just say wed like to avoid any imperial entanglements', 'we havent that much with us but we could pay you two thousand now plus fifteen when we reach alderaan', 'ninetyfour', 'youll have to sell your speeder', 'it will be enough', 'if the ships as fast as hes boasting we ought to do well', 'how long before you can make the jump to light speed', 'i felt a great disturbance in the force as if millions of voices suddenly cried out in terror and were suddenly silenced i fear something terrible has happened', 'youd better get on with your exercises', 'remember a jedi can feel the force flowing through him', 'partially but it also obeys your commands', 'i suggest you try it again luke', 'this time let go your conscious self and act on instinct', 'your eyes can deceive you dont trust them', 'stretch out with your feelings', 'you see you can do it', 'in my experience theres no such thing as luck', 'thats good you have taken your first step into a larger world', 'destroyed by the empire', 'its an imperial fighter', 'no its a short range fighter', 'itd be as well to let it go its too far out of range', 'a fighter that size couldnt get this deep into space on its own', 'thats no moon its a space station', 'turn the ship around', 'you cant win but there are alternatives to fighting', 'leave that to me', 'whos the more foolish the fool or the fool who follows him', 'plug in he should be able to interpret the entire imperial computer network', 'i dont think you boys can help i must go alone', 'be patient luke stay and watch over the droids', 'they must be delivered safely or other star systems will suffer the same fate as alderaan your destiny lies along a different path from mine the force will be with you always', 'only a master of evil darth', 'you cant win darth if you strike me down i shall become more powerful than you can possibly imagine', 'run luke run', 'luke the force will be with you', 'use the force luke', 'let go luke', 'luke trust me', 'remember the force will be with you always', 'luke luke', 'you will go to the dagobah system', 'there you will learn from yoda the jedi master who instructed me', 'he will learn patience', 'was i any different when you taught me', 'so was i if youll remember', 'you dont know that', 'even yoda cannot see their fate', 'but you cannot control it this is a dangerous time for you when you will be tempted by the dark side of the force', 'it is you and your abilities the emperor wants that is why your friends are made to suffer', 'luke i dont want to lose you to the emperor the way i lost vader', 'patience', 'if you choose to face vader you will do it alone i cannot interfere', 'luke dont give in to hate that leads to the dark side', 'that boy is our last hope', 'yoda will always be with you', 'you father was seduced by the dark side of the force he ceased to be anakin skywalker and became darth vader when that happened the good man who was your father was destroyed so what i have told you was true from a certain point of view', 'luke youre going to find that many of the truths we cling to depend greatly on our own point of view', 'i dont blame you for being angry if i was wrong in what i did it certainly wouldnt have been for the first time you see what happened to your father was my fault', 'anakin was a good friend', 'when i first knew him your father was already a great pilot but i was amazed how strongly the force was with him i took it upon myself to train him as a jedi i thought that i could instruct him just as well as yoda i was wrong my pride has had terrible consequences for the galaxy', 'i also thought he could be turned back to the good side it couldnt be done he is more machine now than man twisted and evil', 'you cannot escape your destiny', 'vader humbled you when first you met him luke but that experience was part of your training it taught you among other things the value of patience had you not been so impatient to defeat vader then you could have finished your training here with yoda you would have been prepared', 'and did you help them it was they who had to save you you achieved little by rushing back prematurely i fear', 'to be a jedi luke you must confront and then go beyond the dark side the side your father couldnt get past impatience is the easiest door for you like your father only your father was seduced by what he found on the other side of the door and you have held firm youre no longer so reckless now luke you are strong and patient and now you must face darth vader again', 'then the emperor has already won you were our only hope', 'the other he spoke of is your twin sister', 'hmm to protect you both from the emperor you were hidden from your father when you were born the emperor knew as i did if anakin were to have any offspring they would be a threat to him that is the reason why your sister remains safely anonymous', 'your insight serves you well bury your feelings deep down luke they do you credit but they could be made to serve the emperor', 'when your father left he didnt know your mother was pregnant your mother and i knew he would find out eventually but we wanted to keep you both as safe as possible for as long as possible so i took you to live with my brother owen on tatooine and your mother took leia to live as the daughter of senator organa on alderaan', 'the organa household was highborn and politically quite powerful in that system leia became a princess by virtue of lineage no one knew shed been adopted of course but it was a title without real power since alderaan had long been a democracy even so the family continued to be politically powerful and leia following in her foster fathers path became a senator as well thats not all she became of course she became the leader of her cell in the alliance against the corrupt empire and because she had diplomatic immunity she was a vital link for getting information to the rebel cause thats what she was doing when her path crossed yours for her foster parents had always told her to contact me on tatooine if her troubles became desperate', 'she hasnt been trained in the ways of the jedi the way you have luke but the force is strong with her as it is with all of your family there is no avoiding the battle you must face and destroy vader']\n",
            "['i', 'have', 'a', 'bad', 'feeling', 'about', 'this', 'its', 'not', 'about', 'the', 'mission', 'master', 'its', 'master', 'yoda', 'says', 'i', 'should', 'be', 'mindful', 'of', 'the', 'future', 'yes', 'master', 'how', 'do', 'you', 'think', 'the', 'trade', 'viceroy', 'will', 'deal', 'with', 'offhand', 'id', 'say', 'this', 'mission', 'is', 'past', 'the', 'negotiaion', 'stage', 'they', 'have', 'shield', 'generators', 'its', 'an', 'invisible', 'army', 'you', 'were', 'right', 'about', 'one', 'thing', 'master', 'the', 'negotiations', 'were', 'sorry', 'master', 'the', 'water', 'fried', 'my', 'weapon', 'yes', 'master', 'whats', 'this', 'when', 'they', 'find', 'us', 'they', 'will', 'crush', 'us', 'grind', 'us', 'into', 'little', 'dont', 'worry', 'this', 'has', 'not', 'been', 'our', 'day', 'for', 'warm', 'welcomes', 'after', 'those', 'droids', 'take', 'control', 'of', 'the', 'surface', 'they', 'will', 'come', 'you', 'and', 'the', 'naboo', 'form', 'a', 'symbiont', 'circle', 'what', 'happens', 'to', 'noe', 'of', 'master', 'whats', 'a', 'bongo', 'we', 'are', 'short', 'of', 'time', 'master', 'master', 'why', 'do', 'you', 'keep', 'dragging', 'these', 'pathetic', 'life', 'forms', 'along', 'why', 'were', 'you', 'banished', 'jar', 'jar', 'they', 'banished', 'you', 'because', 'youre', 'clumsy', 'give', 'me', 'the', 'controls', 'were', 'losing', 'power', 'powers', 'back', 'you', 'overdid', 'it', 'what', 'is', 'it', 'get', 'up', 'here', 'that', 'was', 'close', 'ill', 'take', 'care', 'of', 'that', 'obiwan', 'heads', 'toward', 'the', 'group', 'of', 'now', 'stay', 'here', 'and', 'keep', 'out', 'of', 'trouble', 'here', 'master', 'tatooine', 'its', 'small', 'out', 'of', 'the', 'way', 'poor', 'the', 'its', 'risky', 'but', 'theres', 'no', 'alternative', 'tahyts', 'it', 'tatooine', 'sorry', 'quigons', 'right', 'youll', 'make', 'things', 'less', 'obvious', 'cantd', 'the', 'hyperdrive', 'generator', 'is', 'gone', 'we', 'will', 'need', 'a', 'new', 'i', 'fell', 'it', 'also', 'master', 'v', 'o', 'a', 'few', 'containers', 'of', 'supplies', 'the', 'queens', 'wardrobe', 'maybe', 'this', 'storms', 'going', 'to', 'slow', 'them', 'down', 'its', 'a', 'trick', 'send', 'no', 'reply', 'send', 'no', 'transmission', 'of', 'any', 'kind', 'the', 'queen', 'is', 'upset', 'but', 'absolutly', 'no', 'reply', 'was', 'sent', 'what', 'if', 'it', 'is', 'true', 'and', 'the', 'people', 'are', 'dying', 'what', 'if', 'this', 'plan', 'fails', 'master', 'we', 'could', 'be', 'stuck', 'here', 'for', 'a', 'yes', 'master', 'wait', 'a', 'minute', 'all', 'right', 'ive', 'got', 'it', 'something', 'must', 'be', 'wrong', 'with', 'the', 'transmission', 'strange', 'the', 'transmission', 'seems', 'to', 'be', 'in', 'good', 'order', 'but', 'the', 'even', 'master', 'yoda', 'doesnt', 'have', 'a', 'midichlorian', 'count', 'that', 'high', 'what', 'does', 'it', 'mean', 'why', 'do', 'i', 'sense', 'weve', 'picked', 'up', 'another', 'pathetic', 'life', 'form', 'over', 'there', 'fly', 'low', 'what', 'was', 'it', 'the', 'boy', 'will', 'not', 'pass', 'the', 'councils', 'tests', 'master', 'and', 'you', 'know', 'dont', 'defy', 'the', 'council', 'master', 'not', 'again', 'master', 'you', 'could', 'be', 'sitting', 'on', 'the', 'council', 'by', 'now', 'if', 'you', 'would', 'iam', 'ready', 'to', 'face', 'the', 'trials', 'it', 'is', 'not', 'disrespect', 'master', 'it', 'is', 'the', 'truth', 'the', 'boy', 'is', 'dangerous', 'they', 'all', 'sense', 'it', 'why', 'cant', 'you', 'the', 'wars', 'over', 'no', 'need', 'for', 'it', 'now', 'a', 'droid', 'control', 'ship', 'jar', 'jar', 'is', 'on', 'his', 'way', 'to', 'the', 'gungan', 'city', 'master', 'do', 'you', 'think', 'the', 'queens', 'idea', 'will', 'work', 'im', 'im', 'sorry', 'for', 'my', 'behavior', 'master', 'it', 'is', 'not', 'my', 'place', 'to', 'more', 'likely', 'they', 'were', 'wiped', 'out', 'and', 'theres', 'an', 'even', 'bigger', 'danger', 'if', 'the', 'vicroy', 'escapes', 'your', 'master', 'master', 'no', 'yes', 'master', 'quigon', 'believed', 'in', 'him', 'i', 'believe', 'in', 'quigon', 'master', 'yoda', 'igave', 'quigon', 'my', 'word', 'i', 'will', 'train', 'anakin', 'without', 'he', 'is', 'one', 'with', 'the', 'force', 'anakin', 'you', 'must', 'let', 'go', 'i', 'am', 'your', 'master', 'now', 'you', 'will', 'become', 'a', 'jedi', 'i', 'promise', 'its', 'good', 'to', 'see', 'you', 'too', 'jar', 'jar', 'its', 'a', 'great', 'pleasure', 'to', 'see', 'you', 'againmlady', 'im', 'sure', 'the', 'jedi', 'council', 'havetheir', 'reasons', 'our', 'presence', 'will', 'be', 'invisiblemlady', 'frowningwere', 'here', 'to', 'protect', 'yousenator', 'not', 'to', 'start', 'aninvestigation', 'we', 'are', 'not', 'going', 'to', 'exceed', 'ourmandate', 'my', 'young', 'padawan', 'learner', 'we', 'are', 'not', 'going', 'through', 'thisexercise', 'again', 'anakin', 'you', 'willpay', 'attention', 'to', 'my', 'lead', 'what', 'we', 'will', 'do', 'as', 'the', 'council', 'hasinstructed', 'and', 'you', 'will', 'learnyour', 'place', 'young', 'one', 'anakin', 'youre', 'focusing', 'on', 'thenegative', 'again', 'be', 'mindful', 'of', 'yourthoughts', 'she', 'was', 'glad', 'to', 'see', 'us', 'now', 'lets', 'check', 'the', 'security', 'here', 'captain', 'typho', 'has', 'more', 'than', 'enoughmen', 'downstairs', 'no', 'assassin', 'willtry', 'that', 'way', 'any', 'activity', 'uphere', 'whats', 'going', 'on', 'what', 'is', 'she', 'thinking', 'its', 'not', 'an', 'intruder', 'im', 'worriedabout', 'there', 'are', 'many', 'other', 'waysto', 'kill', 'a', 'senator', 'youre', 'using', 'her', 'as', 'bait', 'its', 'too', 'risky', 'and', 'your', 'sensesarent', 'that', 'attuned', 'youngapprentice', 'possibly', 'you', 'look', 'tired', 'because', 'of', 'your', 'mother', 'dreams', 'pass', 'in', 'time', 'mind', 'your', 'thoughts', 'anakin', 'theybetray', 'you', 'youve', 'made', 'acommitment', 'to', 'the', 'jedi', 'order', 'acommitment', 'not', 'easily', 'broken', 'and', 'dont', 'forget', 'shes', 'apolitician', 'theyre', 'not', 'to', 'betrusted', 'its', 'been', 'my', 'experience', 'thatsenators', 'are', 'only', 'focused', 'onpleasing', 'those', 'who', 'fund', 'theircampaigns', 'and', 'they', 'are', 'morethan', 'willing', 'to', 'forget', 'theniceties', 'of', 'democracy', 'to', 'get', 'thosefunds', 'palpatines', 'a', 'politician', 'iveobserved', 'that', 'he', 'is', 'very', 'clever', 'atfollowing', 'the', 'passions', 'andprejudices', 'of', 'the', 'senators', 'i', 'sense', 'it', 'too', 'i', 'have', 'a', 'bad', 'feeling', 'about', 'this', 'what', 'took', 'you', 'so', 'long', 'if', 'youd', 'spend', 'as', 'much', 'timeworking', 'on', 'your', 'saber', 'skills', 'asyou', 'do', 'on', 'your', 'wit', 'young', 'padawanyou', 'would', 'rival', 'master', 'yoda', 'as', 'aswordsman', 'only', 'in', 'your', 'mind', 'my', 'very', 'youngapprentice', 'careful', 'hey', 'easy', 'i', 'dont', 'mind', 'flying', 'butwhat', 'youre', 'doing', 'is', 'suicide', 'just', 'slow', 'down', 'continuingwait', 'dont', 'go', 'in', 'there', 'continuingyou', 'know', 'i', 'dont', 'like', 'it', 'when', 'youdo', 'that', 'what', 'are', 'you', 'doing', 'hes', 'gonnablast', 'me', 'that', 'was', 'too', 'close', 'what', 'whooooaaa', 'dont', 'do', 'that', 'idont', 'like', 'it', 'when', 'you', 'do', 'that', 'its', 'dangerous', 'near', 'those', 'powercouplings', 'slow', 'down', 'dont', 'gothrough', 'there', 'continuingyiiii', 'what', 'are', 'you', 'doing', 'sarcasticallyoh', 'that', 'was', 'good', 'stop', 'im', 'crazy', 'im', 'crazy', 'imcrazy', 'angrilyit', 'didnt', 'work', 'weve', 'stalledand', 'you', 'almost', 'got', 'us', 'killed', 'very', 'angrilyit', 'was', 'stupid', 'furiousbut', 'you', 'didnt', 'and', 'now', 'wevelost', 'him', 'where', 'are', 'you', 'going', 'he', 'wentdown', 'there', 'the', 'other', 'way', 'what', 'do', 'you', 'mean', '\\x91you', 'think\\x92what', 'kind', 'of', 'shortcut', 'he', 'wentcompletely', 'the', 'other', 'way', 'you\\x92velost', 'him', 'sarcasticoh', 'so', 'that\\x92s', 'why', 'we\\x92re', 'going', 'inthe', 'wrong', 'direction', 'continuingwell', 'you', 'lost', 'him', 'anakin', 'patience', 'continuinghere', 'next', 'time', 'try', 'not', 'to', 'lose', 'it', 'a', 'jedis', 'saber', 'is', 'his', 'mostprecious', 'possession', 'he', 'must', 'keep', 'it', 'with', 'him', 'at', 'alltimes', 'this', 'weapon', 'is', 'your', 'life', 'but', 'you', 'havent', 'learned', 'anythinganakin', 'why', 'do', 'i', 'think', 'you', 'are', 'going', 'to', 'bethe', 'death', 'of', 'me', 'then', 'why', 'dont', 'you', 'listen', 'to', 'me', 'do', 'you', 'see', 'him', 'him', 'then', 'be', 'extra', 'careful', 'nods', 'to', 'a', 'roomcheck', 'it', 'out', 'to', 'get', 'a', 'drink', 'you', 'dont', 'want', 'to', 'sell', 'me', 'deathsticks', 'you', 'want', 'to', 'go', 'home', 'and', 'rethink', 'your', 'life', 'do', 'you', 'know', 'who', 'it', 'was', 'you', 'weretrying', 'to', 'kill', 'who', 'hired', 'you', 'this', 'wounds', 'going', 'to', 'needtreatment', 'toxic', 'dart', 'anakin', 'you', 'stay', 'put', 'on', 'naboo', 'do', 'not', 'attract', 'any', 'attention', 'doabsolutely', 'nothing', 'withoutchecking', 'in', 'with', 'me', 'or', 'the', 'council', 'to', 'padmei', 'will', 'get', 'to', 'the', 'bottom', 'of', 'thisplot', 'quickly', 'mlady', 'youll', 'beback', 'here', 'in', 'no', 'time', 'may', 'the', 'force', 'be', 'with', 'you', 'i', 'hope', 'he', 'doesnt', 'try', 'anythingfoolish', 'its', 'a', 'toxic', 'dart', 'i', 'need', 'to', 'knowwhere', 'it', 'came', 'from', 'and', 'who', 'made', 'it', 'excuse', 'me', 'could', 'you', 'try', 'againplease', 'thanks', 'for', 'your', 'assistance', 'youmay', 'not', 'be', 'able', 'to', 'figure', 'thisout', 'but', 'i', 'think', 'i', 'know', 'someonewho', 'might', 'im', 'looking', 'for', 'dexter', 'hes', 'not', 'in', 'trouble', 'itspersonal', 'hey', 'dex', 'thank', 'you', 'hey', 'dex', 'you', 'can', 'tell', 'me', 'what', 'this', 'is', 'do', 'you', 'know', 'where', 'it', 'came', 'from', 'kamino', 'kyberdart', 'i', 'wonder', 'whyit', 'didnt', 'show', 'up', 'in', 'any', 'analysisarchive', 'well', 'dex', 'if', 'droids', 'could', 'think', 'wewouldnt', 'be', 'here', 'would', 'welaughingkamino', 'doesnt', 'sound', 'familiar', 'is', 'it', 'part', 'of', 'the', 'republic', 'cloners', 'are', 'they', 'friendly', 'on', 'what', 'dex', 'distracted', 'in', 'thoughtyes', 'yes', 'i', 'did', 'i', 'never', 'understood', 'why', 'he', 'quit', 'only', 'twenty', 'jedi', 'have', 'ever', 'leftthe', 'order', 'what', 'happened', 'surprisedreally', 'interesting', 'im', 'still', 'not', 'surei', 'understand', 'yes', 'im', 'trying', 'to', 'find', 'a', 'planetsystem', 'called', 'kamino', 'it', 'doesntseem', 'to', 'show', 'upon', 'any', 'of', 'thearchive', 'charts', 'noddingaccording', 'to', 'my', 'information', 'itshould', 'be', 'in', 'this', 'quadrantsomewhere', 'just', 'south', 'of', 'therishi', 'maze', 'all', 'three', 'actually', 'absolutely', 'wouldnt', 'that', 'be', 'on', 'record', 'thats', 'impossible', 'perhaps', 'thearchives', 'are', 'incomplete', 'i', 'am', 'sorry', 'to', 'disturb', 'you', 'master', 'i\\x92m', 'looking', 'for', 'a', 'planet', 'describedto', 'me', 'by', 'an', 'old', 'friend', 'i', 'trusthim', 'but', 'the', 'system', 'doesn\\x92t', 'showup', 'on', 'the', 'archive', 'maps', 'this', 'is', 'where', 'it', 'ought', 'to', 'be', 'but', 'it', 'isn\\x92t', 'gravity', 'is', 'pullingall', 'the', 'stars', 'in', 'this', 'area', 'inwardto', 'this', 'spot', 'there', 'should', 'be', 'astar', 'here', 'but', 'there', 'isn\\x92t', 'but', 'master', 'yoda', 'who', 'could', 'haveerased', 'information', 'from', 'thearchives', 'that\\x92s', 'impossibleisn\\x92t', 'it', 'i', 'am', 'concerned', 'for', 'my', 'padawan', 'heis', 'not', 'ready', 'to', 'be', 'on', 'his', 'own', 'but', 'he', 'still', 'has', 'much', 'to', 'learn', 'and', 'his', 'abilities', 'have', 'made', 'him', 'well', 'arrogant', 'i', 'realise', 'nowwhat', 'you', 'and', 'master', 'yoda', 'knew', 'fromthe', 'beginning', 'the', 'boy', 'was', 'tooold', 'to', 'start', 'the', 'training', 'and', 'master', 'he', 'should', 'not', 'have', 'beengiven', 'this', 'assignment', 'im', 'afraidanakin', 'wont', 'be', 'able', 'to', 'protectthe', 'senator', 'he', 'has', 'a', 'an', 'emotionalconnection', 'with', 'her', 'its', 'beenthere', 'since', 'he', 'was', 'boy', 'nowhes', 'confused', 'distracted', 'has', 'master', 'yoda', 'gained', 'any', 'insightinto', 'whether', 'or', 'not', 'this', 'war', 'willcome', 'about', 'warilyim', 'expected', 'obiwan', 'kenobi', 'you', 'make', 'me', 'feel', 'most', 'welcome', 'improvisingthat', 'is', 'good', 'news', 'im', 'sorry', 'master', 'oh', 'yes', 'sidodyas', 'thats', 'why', 'im', 'here', 'where', 'is', 'this', 'bounty', 'hunter', 'now', 'unaltered', 'i', 'would', 'like', 'to', 'meet', 'this', 'jangofett', 'you', 'mentioned', 'growthacceleration', 'and', 'these', 'very', 'impressive', 'carefullytell', 'me', 'prime', 'minister', 'when', 'mymaster', 'sidodyas', 'first', 'contactedyou', 'did', 'he', 'say', 'the', 'order', 'wasfor', 'himself', 'or', 'astonishedthe', 'repubic', 'arfour', 'arfour', 'arfour', 'relay', 'this', 'scramble', 'codefive', 'to', 'courscant', 'care', 'of', 'theold', 'folks', 'home', 'your', 'clones', 'are', 'very', 'impressive', 'you', 'must', 'be', 'very', 'proud', 'arent', 'we', 'all', 'continuingever', 'made', 'your', 'way', 'as', 'far', 'into', 'theinterior', 'as', 'coruscant', 'recently', 'then', 'you', 'must', 'know', 'master', 'sidodyas', 'sidodyas', 'isn\\x92t', 'he', 'the', 'jedi', 'whohired', 'you', 'for', 'this', 'job', 'no', 'i', 'thought', 'curious', 'it', 'seems', 'to', 'me', 'its', 'your', 'army', 'being', 'that', 'they', 'are', 'all', 'clones', 'ofyou', 'i', 'look', 'forward', 'to', 'seeing', 'them', 'inaction', 'thank', 'you', 'for', 'your', 'timejango', 'i', 'wont', 'forget', 'guiding', 'light', 'to', 'old', 'folkshome', 'starships', 'from', 'the', 'tradefederation', 'and', 'the', 'commerce', 'guildsare', 'taking', 'deliveries', 'of', 'battledroids', 'from', 'the', 'foundries', 'ongeonosis', 'the', 'droid', 'foundry', 'seems', 'to', 'beworking', 'at', 'full', 'capacity', 'i', 'amgoing', 'to', 'go', 'down', 'and', 'investigate', 'i', 'will', 'bring', 'jango', 'fett', 'back', 'homefor', 'interrogation', 'one', 'more', 'thing', 'jango', 'mentionedhe', 'was', 'recruited', 'by', 'someone', 'nameddarth', 'tyranus', 'any', 'idea', 'who', 'thatmight', 'be', 'do', 'you', 'believe', 'he', 'could', 'be', 'themysterious', 'sidodyas', 'who', 'made', 'thedeal', 'for', 'the', 'clone', 'army', 'let', 'me', 'see', 'if', 'i', 'can', 'figure', 'outwhats', 'going', 'on', 'first', 'alliance', 'have', 'pledged', 'theirarmies', 'to', 'count', 'dooku', 'and', 'areforming', 'an', 'wait', 'ah', 'attack', 'i', 'dont', 'make', 'it', 'aaaggghhh', 'i', 'was', 'beginning', 'to', 'wonder', 'if', 'youhad', 'gotten', 'my', 'message', 'it', 'looks', 'like', 'youre', 'going', 'a', 'goodjob', 'so', 'far', 'take', 'the', 'one', 'the', 'left', 'illtake', 'the', 'one', 'on', 'the', 'right', 'it', 'looks', 'like', 'shes', 'already', 'on', 'topof', 'things', 'someones', 'got', 'to', 'shut', 'down', 'these', 'droids', 'hold', 'on', 'look', 'over', 'there', 'no', 'forget', 'her', 'we', 'have', 'to', 'goafter', 'dooku', 'dont', 'let', 'your', 'personal', 'feelingsget', 'in', 'the', 'way', 'weve', 'got', 'a', 'job', 'todo', 'anakin', 'shes', 'all', 'right', 'look', 'continuingfollow', 'that', 'speeder', 'to', 'anakinwe', 'move', 'in', 'together', 'you', 'slowlyon', 'the', 'anakin', 'no', 'i', 'dont', 'think', 'so', 'you', 'will', 'have', 'to', 'prove', 'it', 'anakin', 'on', 'his', 'way', 'back', 'to', 'naboo', 'he', 'isescorting', 'senator', 'amidala', 'home', 'continuingi', 'must', 'admit', 'without', 'the', 'clonesit', 'would', 'not', 'have', 'been', 'a', 'victory', 'look', 'out', 'four', 'droids', 'inbound', 'weve', 'got', 'to', 'split', 'them', 'up', 'easy', 'for', 'you', 'to', 'say', 'why', 'am', 'i', 'always', 'the', 'bait', 'anakin', 'theyre', 'all', 'over', 'me', 'im', 'going', 'down', 'on', 'the', 'deck', 'hurry', 'up', 'i', 'dont', 'like', 'this', 'continuing', 'ouch', 'continuing', 'dont', 'even', 'try', 'to', 'fix', 'it', 'arfour', 'ive', 'shut', 'it', 'down', 'next', 'time', 'youre', 'the', 'bait', 'now', 'lets', 'find', 'the', 'command', 'ship', 'and', 'get', 'on', 'with', 'it', 'i', 'see', 'it', 'oh', 'this', 'is', 'going', 'to', 'be', 'easy', 'not', 'this', 'time', 'theres', 'too', 'much', 'at', 'stake', 'we', 'need', 'help', 'odd', 'ball', 'do', 'you', 'copy', 'mark', 'my', 'position', 'and', 'form', 'your', 'squad', 'up', 'behind', 'me', 'add', 'five', 'trifighters', 'on', 'the', 'right', 'take', 'it', 'easy', 'anakin', 'five', 'more', 'on', 'the', 'right', 'im', 'going', 'high', 'and', 'right', 'stay', 'with', 'me', 'swing', 'back', 'and', 'right', 'help', 'me', 'engage', 'back', 'off', 'let', 'them', 'pass', 'between', 'us', 'all', 'right', 'engage', 'and', 'hurry', 'these', 'droids', 'are', 'all', 'over', 'me', 'like', 'a', 'rash', 'anakin', 'you', 'have', 'four', 'on', 'your', 'tail', 'four', 'more', 'closing', 'from', 'your', 'left', 'break', 'right', 'and', 'go', 'high', 'to', 'himself', 'he', 'still', 'has', 'much', 'to', 'learn', 'too', 'dangerous', 'first', 'jedi', 'rule', 'just', 'keep', 'me', 'steady', 'hold', 'on', 'not', 'yet', 'now', 'break', 'left', 'continuing', 'youll', 'never', 'get', 'through', 'there', 'anakin', 'its', 'too', 'tight', 'use', 'the', 'force', 'think', 'yourself', 'through', 'the', 'ship', 'will', 'follow', 'no', 'no', 'they', 'are', 'doing', 'their', 'job', 'so', 'we', 'can', 'do', 'ours', 'head', 'for', 'the', 'command', 'ship', 'they', 'overshot', 'us', 'all', 'right', 'arfour', 'no', 'no', 'nothing', 'too', 'fancy', 'flying', 'is', 'for', 'droids', 'continuing', 'im', 'hit', 'anakin', 'arfour', 'be', 'careful', 'you', 'have', 'one', 'oh', 'dear', 'theyre', 'shutting', 'down', 'all', 'the', 'controls', 'the', 'mission', 'get', 'to', 'the', 'command', 'ship', 'get', 'the', 'chancellor', 'im', 'running', 'out', 'of', 'tricks', 'here', 'continuing', 'in', 'the', 'name', 'of', 'anakin', 'hold', 'your', 'fire', 'hold', 'your', 'fire', 'youre', 'not', 'helping', 'here', 'wait', 'wait', 'i', 'cant', 'see', 'a', 'thing', 'my', 'cockpits', 'fogging', 'theyre', 'all', 'over', 'me', 'anakin', 'hold', 'on', 'anakin', 'youre', 'going', 'to', 'get', 'us', 'both', 'killed', 'get', 'out', 'of', 'here', 'theres', 'nothing', 'more', 'you', 'can', 'do', 'blast', 'it', 'i', 'cant', 'see', 'my', 'controls', 'are', 'gone', 'artoo', 'hit', 'the', 'buzz', 'droids', 'center', 'eye', 'great', 'artoo', 'have', 'you', 'noticed', 'the', 'shields', 'are', 'still', 'up', 'oh', 'i', 'have', 'a', 'bad', 'feeling', 'about', 'this', 'artoo', 'locate', 'the', 'chancellor', 'the', 'chancellors', 'signal', 'is', 'coming', 'from', 'right', 'there', 'the', 'observation', 'platform', 'at', 'the', 'top', 'of', 'that', 'spire', 'i', 'sense', 'a', 'trap', 'spring', 'the', 'trap', 'here', 'take', 'this', 'and', 'wait', 'for', 'orders', 'we', 'are', 'here', 'to', 'relieve', 'you', 'of', 'chancellor', 'palpatine', 'not', 'join', 'him', 'anakin', 'fuel', 'the', 'slightest', 'charge', 'from', 'our', 'sabers', 'will', 'send', 'this', 'ship', 'into', 'oblivion', 'thats', 'why', 'theyve', 'stopped', 'shooting', 'your', 'idea', 'of', 'safe', 'is', 'not', 'the', 'same', 'as', 'mine', 'six', 'droids', 'coming', 'our', 'way', 'well', 'never', 'get', 'through', 'that', 'its', 'too', 'small', 'anakin', 'this', 'is', 'no', 'time', 'for', 'jokes', 'were', 'in', 'serious', 'trouble', 'here', 'that', 'wont', 'hold', 'when', 'the', 'fuel', 'hits', 'those', 'power', 'dischargers', 'you', 'still', 'have', 'much', 'to', 'learn', 'anakin', 'all', 'right', 'you', 'win', 'i', 'have', 'much', 'to', 'learn', 'lets', 'go', 'did', 'you', 'press', 'the', 'stop', 'button', 'no', 'we', 'dont', 'want', 'to', 'get', 'out', 'we', 'want', 'to', 'get', 'moving', 'artoo', 'artoo', 'do', 'you', 'copy', 'activate', 'elevator', 'looks', 'at', 'control', 'panel', 'artoo', 'continuing', 'always', 'on', 'the', 'move', 'os', 'artoo', 'switch', 'on', 'the', 'comlink', 'artoo', 'do', 'you', 'hear', 'me', 'artoo', 'we', 'gave', 'you', 'a', 'job', 'to', 'do', 'artoo', 'stop', 'stop', 'artoo', 'we', 'need', 'to', 'be', 'going', 'up', 'os', 'artoo', 'do', 'you', 'copy', 'artoo', 'do', 'you', 'hear', 'me', 'artoo', 'we', 'need', 'to', 'be', 'going', 'up', 'not', 'down', 'stop', 'artoo', 'we', 'need', 'to', 'go', 'up', 'stop', 'stop', 'now', 'thats', 'better', 'oh', 'its', 'you', 'well', 'artoo', 'has', 'been', 'did', 'i', 'say', 'anything', 'i', 'didnt', 'say', 'anything', 'bows', 'chancellor', 'quietly', 'to', 'anakin', 'this', 'time', 'we', 'will', 'do', 'it', 'together', 'chancellor', 'palpatine', 'sith', 'lords', 'are', 'our', 'specialty', 'you', 'wont', 'get', 'away', 'this', 'time', 'dooku', 'did', 'i', 'miss', 'something', 'what', 'is', 'that', 'continuing', 'oops', 'too', 'late', 'jump', 'lets', 'see', 'if', 'we', 'can', 'find', 'something', 'in', 'the', 'hangar', 'bay', 'thats', 'still', 'flyable', 'come', 'on', 'none', 'of', 'those', 'ships', 'will', 'get', 'us', 'anywhere', 'dont', 'look', 'at', 'me', 'i', 'dont', 'know', 'well', 'head', 'toward', 'the', 'bridge', 'and', 'see', 'if', 'we', 'can', 'find', 'an', 'escape', 'pod', 'wait', 'a', 'minute', 'howd', 'this', 'happen', 'were', 'smarter', 'than', 'this', 'oh', 'so', 'all', 'of', 'a', 'sudden', 'its', 'my', 'fault', 'im', 'open', 'to', 'suggestions', 'here', 'patience', 'thats', 'your', 'plan', 'is', 'it', 'do', 'you', 'have', 'a', 'plan', 'b', 'that', 'depends', 'upon', 'your', 'point', 'of', 'view', 'hah', 'anakin', 'try', 'not', 'to', 'upset', 'him', 'we', 'have', 'a', 'job', 'to', 'do', 'not', 'this', 'time', 'and', 'this', 'time', 'you', 'wont', 'escape', 'grievous', 'can', 'you', 'fly', 'a', 'cruiser', 'like', 'this', 'well', 'steady', 'attitude', 'eighteen', 'degrees', 'temp', 'steady', 'hatches', 'open', 'flaps', 'extended', 'drag', 'fins', 'not', 'to', 'worry', 'were', 'still', 'flying', 'half', 'the', 'ship', 'careful', 'were', 'heating', 'uptwelve', 'thousand', 'thirteen', 'thousand', 'eight', 'plus', 'sixtyforty', 'eight', 'plus', 'sixtytwenty', 'eight', 'plus', 'sixty', 'temp', 'ten', 'thousand', 'nine', 'thousand', 'were', 'in', 'the', 'atmosphere', 'steady', 'steady', 'five', 'thousand', 'three', 'thousand', 'two', 'thousand', 'fireships', 'on', 'the', 'left', 'and', 'the', 'right', 'copy', 'that', 'landing', 'strips', 'straight', 'ahead', 'easyeasy', 'another', 'happy', 'landing', 'oh', 'no', 'im', 'not', 'brave', 'enough', 'for', 'politics', 'i', 'have', 'to', 'report', 'to', 'the', 'council', 'besides', 'someone', 'needs', 'to', 'be', 'the', 'poster', 'boy', 'sorry', 'old', 'friend', 'let', 'us', 'not', 'forget', 'that', 'you', 'rescued', 'me', 'from', 'the', 'buzz', 'droids', 'and', 'you', 'killed', 'count', 'dooku', 'and', 'you', 'rescued', 'the', 'chancellor', 'carrying', 'me', 'unconscious', 'on', 'your', 'back', 'and', 'you', 'managed', 'to', 'land', 'that', 'bucket', 'of', 'bolts', 'safely', 'the', 'endless', 'speeches', 'anakin', 'lets', 'be', 'fair', 'today', 'you', 'are', 'the', 'hero', 'and', 'you', 'deserve', 'your', 'glorious', 'day', 'with', 'the', 'politicians', 'ninth', 'time', 'that', 'business', 'on', 'cato', 'nemoidia', 'doesnt', 'count', 'ill', 'see', 'you', 'at', 'the', 'briefing', 'you', 'missed', 'the', 'report', 'on', 'the', 'outer', 'rim', 'sieges', 'in', 'short', 'they', 'are', 'going', 'very', 'well', 'saleucami', 'has', 'fallen', 'and', 'master', 'vos', 'has', 'moved', 'his', 'troops', 'to', 'boz', 'pity', 'the', 'senate', 'is', 'expected', 'to', 'vote', 'more', 'executive', 'powers', 'to', 'the', 'chancellor', 'today', 'anakin', 'be', 'careful', 'of', 'your', 'friend', 'palpatine', 'he', 'has', 'requested', 'your', 'presence', 'he', 'would', 'not', 'say', 'all', 'of', 'this', 'is', 'unusual', 'and', 'its', 'making', 'me', 'feel', 'uneasy', 'youre', 'probably', 'aware', 'that', 'relations', 'between', 'the', 'council', 'and', 'the', 'chancellor', 'are', 'stressed', 'the', 'force', 'grows', 'dark', 'anakin', 'and', 'we', 'are', 'all', 'affected', 'by', 'it', 'be', 'wary', 'of', 'your', 'feelings', 'it', 'may', 'take', 'some', 'time', 'we', 'do', 'not', 'have', 'many', 'ships', 'to', 'spare', 'and', 'yet', 'it', 'would', 'be', 'fatal', 'for', 'us', 'to', 'allow', 'the', 'droid', 'armies', 'to', 'regroup', 'hes', 'right', 'that', 'is', 'a', 'system', 'we', 'cannot', 'afford', 'to', 'lose', 'its', 'the', 'main', 'navigation', 'route', 'for', 'the', 'southwestern', 'quadrant', 'calm', 'down', 'anakin', 'you', 'have', 'been', 'given', 'a', 'great', 'honor', 'to', 'be', 'on', 'the', 'council', 'at', 'your', 'age', 'its', 'never', 'happened', 'before', 'listen', 'to', 'me', 'anakin', 'the', 'fact', 'of', 'the', 'matter', 'is', 'youre', 'too', 'close', 'to', 'the', 'chancellor', 'the', 'council', 'doesnt', 'like', 'it', 'when', 'he', 'interferes', 'in', 'jedi', 'affairs', 'but', 'its', 'what', 'you', 'wanted', 'your', 'friendship', 'with', 'chancellor', 'palpatine', 'seems', 'to', 'have', 'paid', 'off', 'anakin', 'regardless', 'of', 'how', 'it', 'happened', 'you', 'find', 'yourself', 'in', 'a', 'delicate', 'situation', 'i', 'warned', 'you', 'there', 'was', 'tension', 'between', 'the', 'council', 'and', 'the', 'chancellor', 'i', 'was', 'very', 'clear', 'why', 'didnt', 'you', 'listen', 'you', 'walked', 'right', 'into', 'it', 'no', 'it', 'is', 'not', 'anakin', 'i', 'worry', 'when', 'you', 'speak', 'of', 'jealousy', 'and', 'pride', 'those', 'are', 'not', 'jedi', 'thoughts', 'theyre', 'dangerous', 'dark', 'thoughts', 'i', 'hope', 'so', 'anakin', 'the', 'only', 'reason', 'the', 'council', 'has', 'approved', 'your', 'appointment', 'is', 'because', 'the', 'chancellor', 'trusts', 'you', 'anakin', 'look', 'i', 'am', 'on', 'your', 'side', 'i', 'didnt', 'want', 'to', 'see', 'you', 'put', 'in', 'this', 'situation', 'takes', 'a', 'deep', 'breath', 'the', 'council', 'wants', 'you', 'to', 'report', 'on', 'all', 'of', 'the', 'chancellors', 'dealings', 'they', 'want', 'to', 'know', 'what', 'hes', 'up', 'to', 'we', 'are', 'at', 'war', 'anakin', 'the', 'jedi', 'council', 'is', 'sworn', 'to', 'uphold', 'the', 'principles', 'of', 'the', 'republic', 'even', 'if', 'the', 'chancellor', 'does', 'not', 'this', 'assignment', 'is', 'not', 'to', 'be', 'on', 'record', 'the', 'council', 'asked', 'me', 'to', 'approach', 'you', 'on', 'this', 'personally', 'that', 'is', 'why', 'you', 'must', 'help', 'us', 'anakin', 'our', 'allegiance', 'is', 'to', 'the', 'senate', 'not', 'to', 'its', 'leader', 'who', 'has', 'managed', 'to', 'stay', 'in', 'office', 'long', 'after', 'his', 'term', 'has', 'expired', 'yes', 'but', 'use', 'your', 'feelings', 'anakin', 'something', 'is', 'out', 'of', 'place', 'the', 'council', 'is', 'asking', 'you', 'anakin', 'did', 'not', 'take', 'to', 'his', 'assignment', 'with', 'much', 'enthusiasm', 'hell', 'be', 'all', 'right', 'i', 'trust', 'him', 'with', 'my', 'life', 'with', 'all', 'due', 'respect', 'master', 'is', 'he', 'not', 'the', 'chosen', 'one', 'is', 'he', 'not', 'to', 'destroy', 'the', 'sith', 'and', 'bring', 'balance', 'to', 'the', 'force', 'he', 'will', 'not', 'let', 'me', 'down', 'he', 'never', 'has', 'does', 'everyone', 'agree', 'oh', 'no', 'youre', 'quite', 'right', 'but', 'i', 'do', 'have', 'the', 'most', 'experience', 'with', 'his', 'ways', 'of', 'combat', 'has', 'anakin', 'been', 'to', 'see', 'you', 'i', 'know', 'he', 'deserves', 'it', 'he', 'is', 'impatient', 'strong', 'willed', 'very', 'opinionated', 'but', 'truly', 'gifted', 'you', 'should', 'be', 'a', 'jedi', 'padme', 'its', 'anakin', 'hes', 'becoming', 'moody', 'and', 'detached', 'hes', 'been', 'put', 'in', 'a', 'difficult', 'position', 'as', 'the', 'chancellors', 'representative', 'but', 'i', 'think', 'its', 'more', 'than', 'that', 'i', 'was', 'hoping', 'he', 'may', 'have', 'talked', 'to', 'you', 'neither', 'of', 'you', 'is', 'very', 'good', 'at', 'hiding', 'your', 'feelings', 'either', 'i', 'know', 'how', 'he', 'feels', 'about', 'you', 'nothing', 'he', 'didnt', 'have', 'to', 'i', 'know', 'you', 'both', 'too', 'well', 'i', 'can', 'see', 'you', 'two', 'are', 'in', 'love', 'padme', 'im', 'worried', 'about', 'him', 'continuing', 'i', 'fear', 'your', 'relationship', 'has', 'confused', 'him', 'hes', 'changed', 'considerably', 'since', 'we', 'returned', 'continuing', 'padme', 'im', 'not', 'telling', 'the', 'council', 'about', 'any', 'of', 'this', 'i', 'i', 'hope', 'i', 'didnt', 'upset', 'you', 'were', 'all', 'friends', 'i', 'care', 'about', 'both', 'of', 'you', 'please', 'do', 'what', 'you', 'can', 'to', 'help', 'him', 'oh', 'i', 'agree', 'however', 'it', 'may', 'turn', 'out', 'just', 'to', 'be', 'a', 'wild', 'bantha', 'chase', 'you', 'are', 'strong', 'and', 'wise', 'anakin', 'and', 'i', 'am', 'very', 'proud', 'of', 'you', 'i', 'have', 'trained', 'you', 'since', 'you', 'were', 'a', 'small', 'boy', 'i', 'have', 'taught', 'you', 'everything', 'i', 'know', 'and', 'you', 'have', 'become', 'a', 'far', 'greater', 'jedi', 'than', 'i', 'could', 'ever', 'hope', 'to', 'be', 'and', 'you', 'have', 'saved', 'my', 'life', 'more', 'times', 'than', 'i', 'can', 'remember', 'but', 'be', 'patient', 'anakin', 'it', 'wont', 'be', 'long', 'before', 'the', 'council', 'makes', 'you', 'a', 'jedi', 'master', 'continuing', 'dont', 'worry', 'i', 'have', 'enough', 'clones', 'with', 'me', 'to', 'take', 'three', 'systems', 'the', 'size', 'of', 'utapau', 'i', 'think', 'ill', 'be', 'able', 'to', 'handle', 'the', 'situation', 'even', 'without', 'your', 'help', 'goodbye', 'old', 'friend', 'may', 'the', 'force', 'be', 'with', 'you', 'ill', 'keep', 'them', 'distracted', 'until', 'you', 'get', 'there', 'just', 'dont', 'take', 'too', 'long', 'laughing', 'cato', 'nemoidia', 'for', 'starters', 'very', 'well', 'the', 'burden', 'is', 'on', 'me', 'not', 'to', 'destroy', 'all', 'the', 'droids', 'before', 'you', 'get', 'there', 'save', 'your', 'energy', 'dont', 'give', 'up', 'padme', 'dont', 'give', 'up', 'unfortunately', 'the', 'war', 'with', 'your', 'kind', 'permission', 'i', 'should', 'like', 'some', 'fuel', 'and', 'to', 'use', 'your', 'city', 'as', 'a', 'base', 'as', 'i', 'search', 'nearby', 'systems', 'for', 'general', 'grievous', 'i', 'understand', 'tell', 'your', 'people', 'to', 'take', 'shelter', 'if', 'you', 'have', 'warriors', 'now', 'is', 'the', 'time', 'continuing', 'geenine', 'take', 'the', 'fighter', 'back', 'to', 'the', 'ship', 'im', 'staying', 'here', 'tell', 'cody', 'ive', 'made', 'contact', 'i', 'need', 'transportation', 'get', 'it', 'for', 'me', 'this', 'one', 'good', 'girl', 'boga', 'hello', 'there', 'your', 'move', 'you', 'forget', 'i', 'trained', 'the', 'jedi', 'that', 'defeated', 'count', 'dooku', 'i', 'may', 'not', 'defeat', 'your', 'droids', 'but', 'my', 'troops', 'certainly', 'will', 'i', 'dont', 'think', 'so', 'so', 'uncivilized', 'commander', 'contact', 'your', 'troops', 'tell', 'them', 'to', 'move', 'to', 'the', 'higher', 'levels', 'thank', 'you', 'cody', 'smiling', 'now', 'lets', 'get', 'a', 'move', 'on', 'weve', 'got', 'a', 'battle', 'to', 'win', 'here', 'emergency', 'code', 'nine', 'thirteen', 'i', 'have', 'no', 'contact', 'on', 'any', 'frequency', 'are', 'there', 'any', 'jedi', 'out', 'there', 'anywhere', 'continuing', 'ive', 'locked', 'on', 'repeat', 'senator', 'organa', 'my', 'clone', 'troops', 'turned', 'on', 'me', 'i', 'need', 'help', 'you', 'were', 'attacked', 'by', 'your', 'clones', 'also', 'how', 'many', 'other', 'jedi', 'managed', 'to', 'survive', 'have', 'we', 'had', 'any', 'contact', 'from', 'the', 'temple', 'well', 'then', 'we', 'must', 'go', 'back', 'if', 'there', 'are', 'other', 'stragglers', 'they', 'will', 'fall', 'into', 'the', 'trap', 'and', 'be', 'killed', 'yes', 'master', 'there', 'is', 'too', 'much', 'at', 'stake', 'here', 'and', 'we', 'need', 'a', 'clearer', 'picture', 'of', 'what', 'has', 'happened', 'no', 'i', 'dont', 'think', 'so', 'the', 'chancellor', 'will', 'not', 'be', 'able', 'to', 'control', 'the', 'thousands', 'of', 'star', 'systems', 'without', 'keeping', 'the', 'senate', 'intact', 'it', 'would', 'be', 'better', 'if', 'we', 'stayed', 'with', 'the', 'senator', 'there', 'are', 'several', 'battalions', 'of', 'clone', 'troopers', 'on', 'every', 'level', 'many', 'are', 'dressed', 'as', 'jedi', 'not', 'even', 'the', 'younglings', 'survived', 'who', 'who', 'could', 'have', 'done', 'this', 'ive', 'recalibrated', 'the', 'code', 'warning', 'all', 'surviving', 'jedi', 'to', 'stay', 'away', 'wait', 'master', 'there', 'is', 'something', 'i', 'must', 'know', 'i', 'must', 'know', 'the', 'truth', 'master', 'continuing', 'it', 'cant', 'be', 'it', 'cant', 'be', 'i', 'cant', 'watch', 'any', 'more', 'send', 'me', 'to', 'kill', 'the', 'emperor', 'i', 'will', 'not', 'kill', 'anakin', 'he', 'is', 'like', 'my', 'brother', 'i', 'cannot', 'do', 'it', 'how', 'could', 'it', 'have', 'come', 'to', 'this', 'i', 'do', 'not', 'know', 'where', 'the', 'emperor', 'has', 'sent', 'him', 'i', 'dont', 'know', 'where', 'to', 'look', 'may', 'the', 'force', 'be', 'with', 'you', 'master', 'yoda', 'has', 'anakin', 'been', 'here', 'the', 'republic', 'has', 'fallen', 'padme', 'the', 'jedi', 'order', 'is', 'no', 'more', 'i', 'believe', 'we', 'have', 'been', 'part', 'of', 'a', 'plot', 'hundreds', 'of', 'years', 'in', 'the', 'making', 'no', 'padme', 'its', 'over', 'the', 'sith', 'now', 'rule', 'the', 'galaxy', 'as', 'they', 'did', 'before', 'the', 'republic', 'im', 'here', 'looking', 'for', 'anakin', 'when', 'was', 'the', 'last', 'time', 'you', 'saw', 'him', 'and', 'do', 'you', 'know', 'where', 'he', 'is', 'now', 'padme', 'i', 'need', 'your', 'help', 'hes', 'in', 'grave', 'danger', 'from', 'himself', 'padme', 'anakin', 'has', 'turned', 'to', 'the', 'dark', 'side', 'i', 'have', 'seen', 'a', 'security', 'hologram', 'of', 'him', 'killing', 'younglings', 'he', 'was', 'deceived', 'by', 'a', 'lie', 'we', 'all', 'were', 'it', 'appears', 'that', 'the', 'chancellor', 'is', 'behind', 'everything', 'including', 'the', 'war', 'palpatine', 'is', 'the', 'sith', 'lord', 'weve', 'been', 'looking', 'for', 'after', 'the', 'death', 'of', 'count', 'dooku', 'anakin', 'became', 'his', 'new', 'apprentice', 'padme', 'i', 'must', 'find', 'him', 'he', 'has', 'become', 'a', 'very', 'great', 'threat', 'anakin', 'is', 'the', 'father', 'isnt', 'he', 'continuing', 'im', 'so', 'sorry', 'let', 'her', 'go', 'anakin', 'let', 'her', 'go', 'you', 'have', 'done', 'that', 'yourself', 'your', 'anger', 'and', 'your', 'lust', 'for', 'power', 'have', 'already', 'done', 'that', 'continuing', 'you', 'have', 'allowed', 'this', 'dark', 'lord', 'to', 'twist', 'your', 'mind', 'until', 'now', 'until', 'now', 'you', 'have', 'become', 'the', 'very', 'thing', 'you', 'swore', 'to', 'destroy', 'your', 'new', 'empire', 'anakin', 'my', 'allegiance', 'is', 'to', 'the', 'republic', 'to', 'democracy', 'only', 'a', 'sith', 'lord', 'deals', 'in', 'absolutes', 'i', 'will', 'do', 'what', 'i', 'must', 'ive', 'heard', 'that', 'before', 'anakin', 'but', 'i', 'never', 'thought', 'id', 'hear', 'it', 'from', 'you', 'continuing', 'the', 'flaw', 'of', 'power', 'is', 'arrogance', 'i', 'have', 'failed', 'you', 'anakin', 'i', 'was', 'never', 'able', 'to', 'teach', 'you', 'to', 'think', 'from', 'the', 'sith', 'anakin', 'chancellor', 'palpatine', 'is', 'evil', 'well', 'then', 'you', 'are', 'lost', 'its', 'over', 'anakin', 'i', 'have', 'the', 'high', 'ground', 'dont', 'try', 'it', 'continuing', 'you', 'were', 'the', 'chosen', 'one', 'it', 'was', 'said', 'that', 'you', 'would', 'destroy', 'the', 'sith', 'not', 'join', 'them', 'it', 'was', 'you', 'who', 'would', 'bring', 'balance', 'to', 'the', 'force', 'not', 'leave', 'it', 'in', 'darkness', 'you', 'were', 'my', 'brother', 'anakin', 'i', 'loved', 'you', 'shes', 'dying', 'dont', 'give', 'up', 'padme', 'you', 'have', 'twins', 'padme', 'they', 'need', 'you', 'hang', 'on', 'save', 'your', 'energy', 'we', 'must', 'take', 'them', 'somewhere', 'the', 'sith', 'will', 'not', 'sense', 'their', 'presence', 'and', 'what', 'of', 'the', 'boy', 'i', 'will', 'take', 'the', 'child', 'and', 'watch', 'over', 'him', 'master', 'yoda', 'do', 'you', 'think', 'anakins', 'twins', 'will', 'be', 'able', 'to', 'defeat', 'darth', 'sidious', 'training', 'who', 'quigon', 'but', 'how', 'could', 'he', 'accomplish', 'this', 'i', 'will', 'be', 'able', 'to', 'talk', 'with', 'him', 'hello', 'there', 'come', 'here', 'my', 'little', 'friend', 'dont', 'be', 'afraid', 'dont', 'worry', 'hell', 'be', 'all', 'right', 'rest', 'easy', 'son', 'youve', 'had', 'a', 'busy', 'day', 'youre', 'fortunate', 'youre', 'still', 'in', 'one', 'piece', 'the', 'jundland', 'wastes', 'are', 'not', 'to', 'be', 'traveled', 'lightly', 'tell', 'me', 'young', 'luke', 'what', 'brings', 'you', 'out', 'this', 'far', 'obiwan', 'kenobi', 'obiwan', 'now', 'thats', 'a', 'name', 'i', 'havent', 'heard', 'in', 'a', 'long', 'time', 'a', 'long', 'time', 'oh', 'hes', 'not', 'dead', 'no', 'not', 'yet', 'well', 'of', 'course', 'of', 'course', 'i', 'know', 'him', 'hes', 'me', 'i', 'havent', 'gone', 'by', 'the', 'name', 'obiwan', 'since', 'oh', 'before', 'you', 'were', 'born', 'dont', 'seem', 'to', 'remember', 'ever', 'owning', 'a', 'droid', 'very', 'interesting', 'i', 'think', 'we', 'better', 'get', 'indoors', 'the', 'sand', 'people', 'are', 'easily', 'startled', 'but', 'they', 'will', 'soon', 'be', 'back', 'and', 'in', 'greater', 'numbers', 'quickly', 'son', 'theyre', 'on', 'the', 'move', 'thats', 'what', 'your', 'uncle', 'told', 'you', 'he', 'didnt', 'hold', 'with', 'your', 'fathers', 'ideals', 'thought', 'he', 'should', 'have', 'stayed', 'here', 'and', 'not', 'gotten', 'involved', 'yes', 'i', 'was', 'once', 'a', 'jedi', 'knight', 'the', 'same', 'as', 'your', 'father', 'he', 'was', 'the', 'best', 'starpilot', 'in', 'the', 'galaxy', 'and', 'a', 'cunning', 'warrior', 'i', 'understand', 'youve', 'become', 'quite', 'a', 'good', 'pilot', 'yourself', 'and', 'he', 'was', 'a', 'good', 'friend', 'which', 'reminds', 'me', 'i', 'have', 'something', 'here', 'for', 'you', 'your', 'father', 'wanted', 'you', 'to', 'have', 'this', 'when', 'you', 'were', 'old', 'enough', 'but', 'your', 'uncle', 'wouldnt', 'allow', 'it', 'he', 'feared', 'you', 'might', 'follow', 'old', 'obiwan', 'on', 'some', 'damnedfool', 'idealistic', 'crusade', 'like', 'your', 'father', 'did', 'your', 'fathers', 'lightsaber', 'this', 'is', 'the', 'weapon', 'of', 'a', 'jedi', 'knight', 'not', 'as', 'clumsy', 'or', 'as', 'random', 'as', 'a', 'blaster', 'an', 'elegant', 'weapon', 'for', 'a', 'morecivilized', 'time', 'for', 'over', 'a', 'thousand', 'generations', 'the', 'jedi', 'knights', 'were', 'the', 'guardians', 'of', 'peace', 'and', 'justice', 'in', 'the', 'old', 'republic', 'before', 'the', 'dark', 'times', 'before', 'the', 'empire', 'a', 'young', 'jedi', 'named', 'darth', 'vader', 'who', 'was', 'a', 'pupil', 'of', 'mine', 'until', 'he', 'turned', 'to', 'evil', 'helped', 'the', 'empire', 'hunt', 'down', 'and', 'destroy', 'the', 'jedi', 'knights', 'he', 'betrayed', 'and', 'murdered', 'your', 'father', 'now', 'the', 'jedi', 'are', 'all', 'but', 'extinct', 'vader', 'was', 'seduced', 'by', 'the', 'dark', 'side', 'of', 'the', 'force', 'well', 'the', 'force', 'is', 'what', 'gives', 'the', 'jedi', 'his', 'power', 'its', 'an', 'energy', 'field', 'created', 'by', 'all', 'living', 'things', 'it', 'surrounds', 'us', 'and', 'penetrates', 'us', 'it', 'binds', 'the', 'galaxy', 'together', 'now', 'lets', 'see', 'if', 'we', 'cant', 'figure', 'out', 'what', 'you', 'are', 'my', 'little', 'friend', 'and', 'where', 'you', 'come', 'from', 'i', 'seem', 'to', 'have', 'found', 'it', 'you', 'must', 'learn', 'the', 'ways', 'of', 'the', 'force', 'if', 'youre', 'to', 'come', 'with', 'me', 'to', 'alderaan', 'i', 'need', 'your', 'help', 'luke', 'im', 'getting', 'too', 'old', 'for', 'this', 'sort', 'of', 'thing', 'she', 'needs', 'your', 'help', 'thats', 'your', 'uncle', 'talking', 'learn', 'about', 'the', 'force', 'luke', 'you', 'must', 'do', 'what', 'you', 'feel', 'is', 'right', 'of', 'course', 'they', 'didnt', 'but', 'we', 'are', 'meant', 'to', 'think', 'they', 'did', 'these', 'tracks', 'are', 'side', 'by', 'side', 'sand', 'people', 'always', 'ride', 'single', 'file', 'to', 'hide', 'there', 'numbers', 'and', 'these', 'blast', 'points', 'too', 'accurate', 'for', 'sand', 'people', 'only', 'imperial', 'stormtroopers', 'are', 'so', 'precise', 'wait', 'luke', 'its', 'too', 'dangerous', 'theres', 'nothing', 'you', 'could', 'have', 'done', 'luke', 'had', 'you', 'been', 'there', 'youd', 'have', 'been', 'killed', 'too', 'and', 'the', 'droids', 'would', 'now', 'be', 'in', 'the', 'hands', 'of', 'the', 'empire', 'mos', 'eisley', 'spaceport', 'you', 'will', 'never', 'find', 'a', 'more', 'wretched', 'hive', 'of', 'scum', 'and', 'villainy', 'we', 'must', 'be', 'cautious', 'theyre', 'for', 'sale', 'if', 'you', 'want', 'them', 'you', 'dont', 'need', 'to', 'see', 'his', 'identification', 'looking', 'for', 'he', 'can', 'go', 'about', 'his', 'business', 'move', 'along', 'the', 'force', 'can', 'have', 'a', 'strong', 'influence', 'on', 'the', 'weakminded', 'you', 'will', 'find', 'it', 'a', 'powerful', 'ally', 'well', 'most', 'of', 'the', 'best', 'freighter', 'pilots', 'can', 'be', 'found', 'here', 'only', 'watch', 'your', 'step', 'this', 'place', 'can', 'be', 'a', 'little', 'rough', 'this', 'little', 'one', 'isnt', 'worth', 'the', 'effort', 'come', 'let', 'me', 'buy', 'you', 'something', 'this', 'is', 'chewbacca', 'hes', 'firstmate', 'on', 'a', 'ship', 'that', 'might', 'suit', 'our', 'needs', 'yes', 'indeed', 'if', 'its', 'a', 'fast', 'ship', 'should', 'i', 'have', 'only', 'passengers', 'myself', 'the', 'boy', 'two', 'droids', 'and', 'no', 'questions', 'asked', 'lets', 'just', 'say', 'wed', 'like', 'to', 'avoid', 'any', 'imperial', 'entanglements', 'we', 'havent', 'that', 'much', 'with', 'us', 'but', 'we', 'could', 'pay', 'you', 'two', 'thousand', 'now', 'plus', 'fifteen', 'when', 'we', 'reach', 'alderaan', 'ninetyfour', 'youll', 'have', 'to', 'sell', 'your', 'speeder', 'it', 'will', 'be', 'enough', 'if', 'the', 'ships', 'as', 'fast', 'as', 'hes', 'boasting', 'we', 'ought', 'to', 'do', 'well', 'how', 'long', 'before', 'you', 'can', 'make', 'the', 'jump', 'to', 'light', 'speed', 'i', 'felt', 'a', 'great', 'disturbance', 'in', 'the', 'force', 'as', 'if', 'millions', 'of', 'voices', 'suddenly', 'cried', 'out', 'in', 'terror', 'and', 'were', 'suddenly', 'silenced', 'i', 'fear', 'something', 'terrible', 'has', 'happened', 'youd', 'better', 'get', 'on', 'with', 'your', 'exercises', 'remember', 'a', 'jedi', 'can', 'feel', 'the', 'force', 'flowing', 'through', 'him', 'partially', 'but', 'it', 'also', 'obeys', 'your', 'commands', 'i', 'suggest', 'you', 'try', 'it', 'again', 'luke', 'this', 'time', 'let', 'go', 'your', 'conscious', 'self', 'and', 'act', 'on', 'instinct', 'your', 'eyes', 'can', 'deceive', 'you', 'dont', 'trust', 'them', 'stretch', 'out', 'with', 'your', 'feelings', 'you', 'see', 'you', 'can', 'do', 'it', 'in', 'my', 'experience', 'theres', 'no', 'such', 'thing', 'as', 'luck', 'thats', 'good', 'you', 'have', 'taken', 'your', 'first', 'step', 'into', 'a', 'larger', 'world', 'destroyed', 'by', 'the', 'empire', 'its', 'an', 'imperial', 'fighter', 'no', 'its', 'a', 'short', 'range', 'fighter', 'itd', 'be', 'as', 'well', 'to', 'let', 'it', 'go', 'its', 'too', 'far', 'out', 'of', 'range', 'a', 'fighter', 'that', 'size', 'couldnt', 'get', 'this', 'deep', 'into', 'space', 'on', 'its', 'own', 'thats', 'no', 'moon', 'its', 'a', 'space', 'station', 'turn', 'the', 'ship', 'around', 'you', 'cant', 'win', 'but', 'there', 'are', 'alternatives', 'to', 'fighting', 'leave', 'that', 'to', 'me', 'whos', 'the', 'more', 'foolish', 'the', 'fool', 'or', 'the', 'fool', 'who', 'follows', 'him', 'plug', 'in', 'he', 'should', 'be', 'able', 'to', 'interpret', 'the', 'entire', 'imperial', 'computer', 'network', 'i', 'dont', 'think', 'you', 'boys', 'can', 'help', 'i', 'must', 'go', 'alone', 'be', 'patient', 'luke', 'stay', 'and', 'watch', 'over', 'the', 'droids', 'they', 'must', 'be', 'delivered', 'safely', 'or', 'other', 'star', 'systems', 'will', 'suffer', 'the', 'same', 'fate', 'as', 'alderaan', 'your', 'destiny', 'lies', 'along', 'a', 'different', 'path', 'from', 'mine', 'the', 'force', 'will', 'be', 'with', 'you', 'always', 'only', 'a', 'master', 'of', 'evil', 'darth', 'you', 'cant', 'win', 'darth', 'if', 'you', 'strike', 'me', 'down', 'i', 'shall', 'become', 'more', 'powerful', 'than', 'you', 'can', 'possibly', 'imagine', 'run', 'luke', 'run', 'luke', 'the', 'force', 'will', 'be', 'with', 'you', 'use', 'the', 'force', 'luke', 'let', 'go', 'luke', 'luke', 'trust', 'me', 'remember', 'the', 'force', 'will', 'be', 'with', 'you', 'always', 'luke', 'luke', 'you', 'will', 'go', 'to', 'the', 'dagobah', 'system', 'there', 'you', 'will', 'learn', 'from', 'yoda', 'the', 'jedi', 'master', 'who', 'instructed', 'me', 'he', 'will', 'learn', 'patience', 'was', 'i', 'any', 'different', 'when', 'you', 'taught', 'me', 'so', 'was', 'i', 'if', 'youll', 'remember', 'you', 'dont', 'know', 'that', 'even', 'yoda', 'cannot', 'see', 'their', 'fate', 'but', 'you', 'cannot', 'control', 'it', 'this', 'is', 'a', 'dangerous', 'time', 'for', 'you', 'when', 'you', 'will', 'be', 'tempted', 'by', 'the', 'dark', 'side', 'of', 'the', 'force', 'it', 'is', 'you', 'and', 'your', 'abilities', 'the', 'emperor', 'wants', 'that', 'is', 'why', 'your', 'friends', 'are', 'made', 'to', 'suffer', 'luke', 'i', 'dont', 'want', 'to', 'lose', 'you', 'to', 'the', 'emperor', 'the', 'way', 'i', 'lost', 'vader', 'patience', 'if', 'you', 'choose', 'to', 'face', 'vader', 'you', 'will', 'do', 'it', 'alone', 'i', 'cannot', 'interfere', 'luke', 'dont', 'give', 'in', 'to', 'hate', 'that', 'leads', 'to', 'the', 'dark', 'side', 'that', 'boy', 'is', 'our', 'last', 'hope', 'yoda', 'will', 'always', 'be', 'with', 'you', 'you', 'father', 'was', 'seduced', 'by', 'the', 'dark', 'side', 'of', 'the', 'force', 'he', 'ceased', 'to', 'be', 'anakin', 'skywalker', 'and', 'became', 'darth', 'vader', 'when', 'that', 'happened', 'the', 'good', 'man', 'who', 'was', 'your', 'father', 'was', 'destroyed', 'so', 'what', 'i', 'have', 'told', 'you', 'was', 'true', 'from', 'a', 'certain', 'point', 'of', 'view', 'luke', 'youre', 'going', 'to', 'find', 'that', 'many', 'of', 'the', 'truths', 'we', 'cling', 'to', 'depend', 'greatly', 'on', 'our', 'own', 'point', 'of', 'view', 'i', 'dont', 'blame', 'you', 'for', 'being', 'angry', 'if', 'i', 'was', 'wrong', 'in', 'what', 'i', 'did', 'it', 'certainly', 'wouldnt', 'have', 'been', 'for', 'the', 'first', 'time', 'you', 'see', 'what', 'happened', 'to', 'your', 'father', 'was', 'my', 'fault', 'anakin', 'was', 'a', 'good', 'friend', 'when', 'i', 'first', 'knew', 'him', 'your', 'father', 'was', 'already', 'a', 'great', 'pilot', 'but', 'i', 'was', 'amazed', 'how', 'strongly', 'the', 'force', 'was', 'with', 'him', 'i', 'took', 'it', 'upon', 'myself', 'to', 'train', 'him', 'as', 'a', 'jedi', 'i', 'thought', 'that', 'i', 'could', 'instruct', 'him', 'just', 'as', 'well', 'as', 'yoda', 'i', 'was', 'wrong', 'my', 'pride', 'has', 'had', 'terrible', 'consequences', 'for', 'the', 'galaxy', 'i', 'also', 'thought', 'he', 'could', 'be', 'turned', 'back', 'to', 'the', 'good', 'side', 'it', 'couldnt', 'be', 'done', 'he', 'is', 'more', 'machine', 'now', 'than', 'man', 'twisted', 'and', 'evil', 'you', 'cannot', 'escape', 'your', 'destiny', 'vader', 'humbled', 'you', 'when', 'first', 'you', 'met', 'him', 'luke', 'but', 'that', 'experience', 'was', 'part', 'of', 'your', 'training', 'it', 'taught', 'you', 'among', 'other', 'things', 'the', 'value', 'of', 'patience', 'had', 'you', 'not', 'been', 'so', 'impatient', 'to', 'defeat', 'vader', 'then', 'you', 'could', 'have', 'finished', 'your', 'training', 'here', 'with', 'yoda', 'you', 'would', 'have', 'been', 'prepared', 'and', 'did', 'you', 'help', 'them', 'it', 'was', 'they', 'who', 'had', 'to', 'save', 'you', 'you', 'achieved', 'little', 'by', 'rushing', 'back', 'prematurely', 'i', 'fear', 'to', 'be', 'a', 'jedi', 'luke', 'you', 'must', 'confront', 'and', 'then', 'go', 'beyond', 'the', 'dark', 'side', 'the', 'side', 'your', 'father', 'couldnt', 'get', 'past', 'impatience', 'is', 'the', 'easiest', 'door', 'for', 'you', 'like', 'your', 'father', 'only', 'your', 'father', 'was', 'seduced', 'by', 'what', 'he', 'found', 'on', 'the', 'other', 'side', 'of', 'the', 'door', 'and', 'you', 'have', 'held', 'firm', 'youre', 'no', 'longer', 'so', 'reckless', 'now', 'luke', 'you', 'are', 'strong', 'and', 'patient', 'and', 'now', 'you', 'must', 'face', 'darth', 'vader', 'again', 'then', 'the', 'emperor', 'has', 'already', 'won', 'you', 'were', 'our', 'only', 'hope', 'the', 'other', 'he', 'spoke', 'of', 'is', 'your', 'twin', 'sister', 'hmm', 'to', 'protect', 'you', 'both', 'from', 'the', 'emperor', 'you', 'were', 'hidden', 'from', 'your', 'father', 'when', 'you', 'were', 'born', 'the', 'emperor', 'knew', 'as', 'i', 'did', 'if', 'anakin', 'were', 'to', 'have', 'any', 'offspring', 'they', 'would', 'be', 'a', 'threat', 'to', 'him', 'that', 'is', 'the', 'reason', 'why', 'your', 'sister', 'remains', 'safely', 'anonymous', 'your', 'insight', 'serves', 'you', 'well', 'bury', 'your', 'feelings', 'deep', 'down', 'luke', 'they', 'do', 'you', 'credit', 'but', 'they', 'could', 'be', 'made', 'to', 'serve', 'the', 'emperor', 'when', 'your', 'father', 'left', 'he', 'didnt', 'know', 'your', 'mother', 'was', 'pregnant', 'your', 'mother', 'and', 'i', 'knew', 'he', 'would', 'find', 'out', 'eventually', 'but', 'we', 'wanted', 'to', 'keep', 'you', 'both', 'as', 'safe', 'as', 'possible', 'for', 'as', 'long', 'as', 'possible', 'so', 'i', 'took', 'you', 'to', 'live', 'with', 'my', 'brother', 'owen', 'on', 'tatooine', 'and', 'your', 'mother', 'took', 'leia', 'to', 'live', 'as', 'the', 'daughter', 'of', 'senator', 'organa', 'on', 'alderaan', 'the', 'organa', 'household', 'was', 'highborn', 'and', 'politically', 'quite', 'powerful', 'in', 'that', 'system', 'leia', 'became', 'a', 'princess', 'by', 'virtue', 'of', 'lineage', 'no', 'one', 'knew', 'shed', 'been', 'adopted', 'of', 'course', 'but', 'it', 'was', 'a', 'title', 'without', 'real', 'power', 'since', 'alderaan', 'had', 'long', 'been', 'a', 'democracy', 'even', 'so', 'the', 'family', 'continued', 'to', 'be', 'politically', 'powerful', 'and', 'leia', 'following', 'in', 'her', 'foster', 'fathers', 'path', 'became', 'a', 'senator', 'as', 'well', 'thats', 'not', 'all', 'she', 'became', 'of', 'course', 'she', 'became', 'the', 'leader', 'of', 'her', 'cell', 'in', 'the', 'alliance', 'against', 'the', 'corrupt', 'empire', 'and', 'because', 'she', 'had', 'diplomatic', 'immunity', 'she', 'was', 'a', 'vital', 'link', 'for', 'getting', 'information', 'to', 'the', 'rebel', 'cause', 'thats', 'what', 'she', 'was', 'doing', 'when', 'her', 'path', 'crossed', 'yours', 'for', 'her', 'foster', 'parents', 'had', 'always', 'told', 'her', 'to', 'contact', 'me', 'on', 'tatooine', 'if', 'her', 'troubles', 'became', 'desperate', 'she', 'hasnt', 'been', 'trained', 'in', 'the', 'ways', 'of', 'the', 'jedi', 'the', 'way', 'you', 'have', 'luke', 'but', 'the', 'force', 'is', 'strong', 'with', 'her', 'as', 'it', 'is', 'with', 'all', 'of', 'your', 'family', 'there', 'is', 'no', 'avoiding', 'the', 'battle', 'you', 'must', 'face', 'and', 'destroy', 'vader']\n",
            "Total # of tokens(words)\n",
            "6196\n",
            "Total # of unique tokens(words)\n",
            "1450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6iy1wHJx-1c"
      },
      "source": [
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(obi_wan_token_list)\n",
        "unique_words = set(obi_wan_token_list)\n",
        "sequences_tokenised = tokenizer.texts_to_sequences(obi_wan_tokens)\n",
        "\n",
        "n_vocab = len(unique_words)\n",
        "n_sentences = len(obi_wan_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k70LlvXmyiQd",
        "outputId": "47852f9c-ca18-4a4c-9ea2-123e504db94a"
      },
      "source": [
        "print(unique_words)\n",
        "print(sequences_tokenised)\n",
        "\n",
        "print(n_vocab)\n",
        "print(n_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'you', 'life', 'highborn', 'go', 'kyberdart', 'must', 'these', 'sieges', 'command', 'ive', 'uphere', 'obiwan', 'warrior', 'troops', 'situation', 'continuingwait', 'atmosphere', 'alderaan', 'shortcut', 'prematurely', 'easily', 'jedis', 'analysisarchive', 'tight', 'serve', 'getting', 'abilities', 'stars', 'angry', 'whooooaaa', 'city', 'padme', 'activity', 'damnedfool', 'astar', 'offspring', 'emotionalconnection', 'palpatine', 'eighteen', 'seeing', 'foundry', 'once', 'act', 'happen', 'worry', 'cruiser', 'follows', 'fromthe', 'guildsare', 'strong', 'able', 'shall', 'jango', 'gotten', 'knights', 'disturb', 'ships', 'behavior', 'word', 'interrogation', 'elegant', 'gungan', 'message', 'wiped', 'actually', 'hyperdrive', 'bethe', 'quadrant', 'destroyed', 'fried', 'regroup', 'admit', 'anything', 'ceased', 'very', 'gothrough', 'archive', 'paid', 'thought', 'ideals', 'alternatives', 'droids', 'beginning', 'id', 'interfere', 'shes', 'report', 'picked', 'overdid', 'dangerous', 'insightinto', 'bows', 'levels', 'queens', 'confused', 'gained', 'havent', 'perhaps', 'uneasy', 'seduced', 'palpatines', 'eight', 'some', 'we\\x92re', 'respect', 'focusing', 'cell', 'continuingfollow', 'face', 'and', 'split', 'lies', 'fett', 'traveled', 'acommitment', 'including', 'boys', 'sent', 'world', 'pledged', 'escapes', 'created', 'code', 'imagine', 'senate', 'assassin', 'victory', 'numbers', 'sixtyforty', 'sudden', 'looking', 'briefing', 'the', 'make', 'plot', 'soon', 'help', 'folks', 'fix', 'minister', 'dex', 'themysterious', 'could', 'entanglements', 'returned', 'hive', 'troubles', 'vital', 'need', 'jangofett', 'nearby', 'theirarmies', 'moody', 'thatsenators', 'dear', 'bring', 'yours', 'at', 'fear', 'detached', 'quigons', 'easy', 'past', 'firm', 'for', 'ninth', 'immunity', 'years', 'using', 'isescorting', 'trained', 'organa', 'full', 'indeed', 'much', 'surviving', 'attract', 'behind', 'here', 'managed', 'involved', 'exercises', 'this', 'doesn\\x92t', 'supplies', 'miss', 'platform', 'unconscious', 'executive', 'low', 'i\\x92m', 'promise', 'several', 'position', 'whos', 'enthusiasm', 'conscious', 'artoo', 'given', 'hello', 'add', 'saved', 'feel', 'teach', 'smiling', 'until', 'information', 'gone', 'water', 'thenegative', 'high', 'their', 'certainly', 'look', 'instruct', 'aaaggghhh', 'thoughts', 'continuingever', 'place', 'sidious', 'forms', 'corrupt', 'clumsy', 'larger', 'cause', 'office', 'never', 'its', 'depend', 'yet', 'prove', 'worriedabout', 'hes', 'fall', 'leader', 'disturbance', 'confront', 'wasfor', 'senator', 'sister', 'sound', 'troopers', 'excuse', 'late', 'thing', 'jump', 'named', 'precise', 'allow', 'sidodyas', 'hangar', 'one', 'meet', 'off', 'fighter', 'generator', 'fancy', 'strike', 'things', 'assistance', 'plus', 'absolutes', 'mission', 'generators', 'inthe', 'spring', 'noddingaccording', 'identification', 'thats', 'afraidanakin', 'containers', 'howd', 'penetrates', 'were', 'ten', 'done', 'idont', 'consequences', 'now', 'nowhes', 'dealings', 'we', 'odd', 'afraid', 'commerce', 'v', 'ready', 'self', 'door', 'brings', 'needs', 'surprisedreally', 'pleasure', 'less', 'atfollowing', 'power', 'poster', 'view', 'vos', 'close', 'forget', 'wants', 'codefive', 'quigon', 'which', 'stormtroopers', 'risky', 'terrible', 'trusts', 'suit', 'boga', 'our', 'ouch', 'does', 'since', 'so', 'systems', 'making', 'engage', 'speed', 'possibly', 'knew', 'moon', 'battle', 'doabsolutely', 'same', 'politician', 'single', 'sworn', 'taken', 'wastes', 'wardrobe', 'helped', 'shelter', 'pathetic', 'met', 'yourthoughts', 'clever', 'way', 'strongly', 'southwestern', 'beyond', 'always', 'personally', 'them', 'believe', 'only', 'iveobserved', 'furiousbut', 'top', 'eyes', 'all', 'entire', 'flying', 'vote', 'delivered', 'then', 'signal', 'therishi', 'starships', 'relay', 'boasting', 'clonesit', 'foolish', 'last', 'know', 'who', 'failed', 'inbound', 'strange', 'spot', 'blame', 'fins', 'trying', 'dark', 'network', 'center', 'regardless', 'binds', 'indoors', 'inaction', 'whyit', 'impressive', 'as', 'leftthe', 'observation', 'cautious', 'virtue', 'meant', 'fatal', 'amidala', 'wait', 'breath', 'possible', 'apprentice', 'queen', 'proud', 'continuingyiiii', 'plan', 'twenty', 'crazy', 'again', 'mine', 'clones', 'hologram', 'can', 'homefor', 'small', 'betrusted', 'with', 'considerably', 'accomplish', 'interferes', 'goodbye', 'thosefunds', 'sort', 'jealousy', 'ought', 'clearer', 'thedeal', 'head', 'surface', 'arrogance', 'uphold', 'rushing', 'tell', 'cody', 'protect', 'brave', 'hidden', 'had', 'sarcasticoh', 'b', 'asyou', 'back', 'truly', 'activate', 'freighter', 'fund', 'hired', 'nameddarth', 'utapau', 'hunter', 'approach', 'depends', 'becoming', 'emperor', 'interpret', 'taking', 'im', 'any', 'hit', 'truth', 'courscant', 'spoke', 'timeworking', 'cloners', 'youd', 'politicians', 'gifted', 'likely', 'after', 'choose', 'father', 'outer', 'sitting', 'term', 'quadrantsomewhere', 'seen', 'friendly', 'dressed', 'weapon', 'over', 'ourmandate', 'fathers', 'hurry', 'yes', 'spire', 'neither', 'heating', 'combat', 'armies', 'made', 'bait', 'anythinganakin', 'points', 'contact', 'andprejudices', 'jokes', 'picture', 'thisexercise', 'obeys', 'felt', 'certain', 'panel', 'lie', 'storms', 'arfour', 'rival', 'jundland', 'hands', 'wevelost', 'alltimes', 'lineage', 'darth', 'why', 'chewbacca', 'title', 'tricks', 'mentionedhe', 'ours', 'down', 'in', 'grievous', 'igave', 'padawanyou', 'little', 'peace', 'another', 'welcomes', 'hundreds', 'many', 'value', 'him', 'her', 'pilots', 'am', 'concerned', 'computer', 'dischargers', 'eventually', 'trifighters', 'think\\x92what', 'experience', 'grave', 'comlink', 'blast', 'pity', 'army', 'into', 'worth', 'whats', 'dragging', 'sell', 'myself', 'influence', 'course', 'sabers', 'charge', 'nowwhat', 'of', 'hiding', 'noe', 'telling', 'mostprecious', 'looks', 'cantd', 'fifteen', 'onpleasing', 'system', 'you\\x92velost', 'best', 'anger', 'sarcasticallyoh', 'skills', 'flyable', 'fighting', 'trouble', 'wit', 'probably', 'he', 'pupil', 'open', 'uncle', 'was', 'bad', 'impossibleisn\\x92t', 'good', 'youdo', 'south', 'accurate', 'voices', 'slow', 'more', 'quickly', 'mind', 'join', 'hang', 'fell', 'maze', 'become', 'orders', 'theinterior', 'weakminded', 'wrong', 'thank', 'attuned', 'needtreatment', 'anakin', 'mindful', 'justice', 'upon', 'focused', 'own', 'threat', 'random', 'repeat', 'machine', 'lost', 'gravity', 'powercouplings', 'twin', 'nine', 'stretch', 'warilyim', 'daughter', 'hasnt', 'i', 'leia', 'send', 'almost', 'sixty', 'work', 'come', 'flaps', 'steady', 'grows', 'training', 'space', 'ah', 'stalledand', 'trials', 'survived', 'between', 'toward', 'protectthe', 'cant', 'approved', 'impossible', 'tired', 'everyone', 'that\\x92s', 'doesntseem', 'busy', 'midichlorian', 'beworking', 'stuck', 'family', 'difficult', 'strips', 'not', 'commands', 'apolitician', 'temple', 'stay', 'roomcheck', 'new', 'me', 'became', 'overshot', 'starters', 'attention', 'locate', 'theres', 'deep', 'closing', 'piece', 'range', 'theyre', 'trap', 'unfortunately', 'grind', 'by', 'presence', 'willtry', 'crusade', 'commander', 'none', 'invisiblemlady', 'heis', 'wentcompletely', 'boy', 'carrying', 'an', 'passengers', 'security', 'humbled', 'temp', 'youmay', 'pod', 'inwardto', 'feelings', 'reasons', 'happens', 'scum', 'owning', 'wild', 'defeat', 'may', 'nothing', 'gave', 'quite', 'hate', 'stayed', 'alliance', 'recruited', 'politically', 'parents', 'understood', 'would', 'adopted', 'obvious', 'oh', 'mos', 'worried', 'switch', 'loved', 'heads', 'delicate', 'firstmate', 'step', 'vader', 'stage', 'force', 'ever', 'following', 'viceroy', 'yousenator', 'sorry', 'destroy', 'learner', 'carefullytell', 'requested', 'friends', 'great', 'senators', 'crush', 'continuing', 'whether', 'around', 'dexter', 'yoda', 'learn', 'stake', 'told', 'household', 'suffer', 'oops', 'other', 'squad', 'wise', 'elevator', 'realise', 'chancellors', 'better', 'different', 'hero', 'imperial', 'than', 'order', 'patience', 'tradefederation', 'search', 'think', 'luke', 'battledroids', 'thirteen', 'side', 'ground', 'tatooine', 'warned', 'didnt', 'feared', 'havetheir', 'pay', 'easyeasy', 'amgoing', 'record', 'princess', 'kind', 'hide', 'battalions', 'powerful', 'careful', 'start', 'dart', 'job', 'son', 'surei', 'buy', 'padmei', 'credit', 'kamino', 'direction', 'surrounds', 'recently', 'galaxy', 'handle', 'flaw', 'attacked', 'bury', 'ride', 'also', 'anakinwe', 'appears', 'hatches', 'coruscant', 'has', 'fireships', 'deck', 'familiar', 'rule', 'shields', 'guardians', 'do', 'thoughtyes', 'continued', 'greatly', 'moving', 'timejango', 'hope', 'againplease', 'investigate', 'speeder', 'stop', 'friendship', 'before', 'tempted', 'few', 'intact', 'drink', '\\x91you', 'interesting', 'either', 'terror', 'aninvestigation', 'extinct', 'wretched', 'take', 'council', 'bigger', 'sure', 'tension', 'live', 'wouldnt', 'his', 'put', 'rough', 'hear', 'fortunate', 'circle', 'rest', 'imcrazy', 'link', 'base', 'planet', 'happened', 'deceive', 'symbiont', 'slowlyon', 'death', 'deliveries', 'attitude', 'quit', 'mentioned', 'seems', 'expected', 'silenced', 'sith', 'hey', 'destiny', 'alternative', 'should', 'poor', 'idea', 'get', 'away', 'bottom', 'nemoidia', 'route', 'shield', 'ahead', 'rash', 'trick', 'are', 'understand', 'mymaster', 'swing', 'bridge', 'endless', 'ways', 'rescued', 'saber', 'talking', 'passions', 'speeches', 'hell', 'luck', 'did', 'dooku', 'swore', 'blaster', 'disrespect', 'love', 'withoutchecking', 'smarter', 'invisible', 'path', 'held', 'personal', 'transmission', 'those', 'warriors', 'rebel', 'wary', 'slightest', 'dead', 'main', 'helping', 'found', 'talk', 'figure', 'far', 'deathsticks', 'please', 'along', 'right', 'chosen', 'or', 'doesnt', 'growthacceleration', 'remember', 'future', 'morethan', 'together', 'amazed', 'todo', 'reach', 'real', 'attack', 'banished', 'chancellor', 'capacity', 'glorious', 'arrogant', 'want', 'murdered', 'however', 'lords', 'man', 'theyve', 'ill', 'fly', 'light', 'plug', 'feels', 'burden', 'deserve', 'fate', 'control', 'instructed', 'running', 'greater', 'calm', 'recalibrated', 'politics', 'such', 'hold', 'friend', 'o', 'fails', 'near', 'fool', 'shooting', 'glad', 'suggestions', 'patient', 'against', 'bongo', 'deserves', 'fair', 'butwhat', 'crossed', 'owen', 'stupid', 'home', 'youve', 'hasinstructed', 'true', 'lightsaber', 'check', 'clear', 'anakins', 'asked', 'youre', 'next', 'bay', 'hunt', 'turned', 'form', 'pass', 'prepared', 'continuingi', 'pride', 'warning', 'file', 'tracks', 'cried', 'takes', 'trust', 'just', 'impatience', 'people', 'three', 'goodjob', 'hoping', 'it', 'level', 'won', 'generations', 'principles', 'times', 'couldnt', 'powers', 'bolts', 'move', 'uncivilized', 'exceed', 'bounty', 'youngapprentice', 'folkshome', 'betrayed', 'enough', 'five', 'if', 'upset', 'everything', 'left', 'frowningwere', 'turn', 'illtake', 'suicide', 'thisout', 'killed', 'appointment', 'youhad', 'maps', 'quietly', 'safely', 'boz', 'reason', 'because', 'councils', 'time', 'haveerased', 'astonishedthe', 'being', 'stressed', 'energy', 'spend', 'cunning', 'willed', 'today', 'stragglers', 'pilot', 'lust', 'diplomatic', 'save', 'welcome', 'forward', 'dying', 'group', 'fogging', 'droid', 'thearchives', 'first', 'talked', 'hmm', 'impatient', 'reply', 'morecivilized', 'survive', 'relationship', 'starpilot', 'achieved', 'twist', 'arent', 'see', 'land', 'noticed', 'continuingyou', 'truths', 'transportation', 'bucket', 'wounds', 'broken', 'wanted', 'every', 'safe', 'contactedyou', 'danger', 'affairs', 'brother', 'idealistic', 'besides', 'hah', 'matter', 'clone', 'sale', 'absolutely', 'remains', 'how', 'wed', 'frequency', 'negotiations', 'avoiding', 'toxic', 'repubic', 'from', 'villainy', 'day', 'agree', 'gives', 'moved', 'unaltered', 'relations', 'buzz', 'learned', 'defeated', 'age', 'partially', 'listen', 'war', 'landing', 'said', 'even', 'weretrying', 'still', 'fuel', 'deals', 'going', 'name', 'trade', 'serious', 'staying', 'happy', 'desperate', 'ship', 'part', 'laughing', 'what', 'break', 'outwhats', 'cockpits', 'instinct', 'deceived', 'eisley', 'suddenly', 'thatmight', 'sand', 'star', 'deal', 'will', 'old', 'escape', 'where', 'possession', 'no', 'itd', 'leave', 'something', 'lord', 'spare', 'someones', 'representative', 'killing', 'tyranus', 'bantha', 'cling', 'spaceport', 'extended', 'learnyour', 'too', 'that', 'show', 'find', 'distracted', 'vicroy', 'questions', 'ally', 'continuingwell', 'ninetyfour', 'shut', 'is', 'willcome', 'balance', 'size', 'scramble', 'ongeonosis', 'walked', 'maybe', 'sense', 'chase', 'tail', 'thousand', 'willpay', 'foundries', 'girl', 'itshould', 'aware', 'locked', 'changed', 'improvisingthat', 'dreams', 'have', 'straight', 'allegiance', 'born', 'weve', 'kill', 'himself', 'called', 'beback', 'saw', 'ball', 'point', 'living', 'emergency', 'prime', 'theold', 'already', 'been', 'sensesarent', 'child', 'padawan', 'might', 'someonewho', 'feelingsget', 'minute', 'like', 'captain', 'charts', 'navigation', 'they', 'reckless', 'asking', 'losing', 'follow', 'coming', 'welaughingkamino', 'foster', 'keep', 'thearchive', 'oblivion', 'goafter', 'thanks', 'insight', 'offhand', 'incomplete', 'copy', 'dont', 'pullingall', 'alone', 'news', 'flowing', 'out', 'young', 'angrilyit', 'tooold', 'lead', 'anywhere', 'describedto', 'jar', 'long', 'stopped', 'there', 'wont', 'curious', 'six', 'got', 'wentdown', 'due', 'fact', 'downstairs', 'most', 'suggest', 'general', 'controls', 'evil', 'without', 'absolutly', 'whohired', 'sixtytwenty', 'continuinghere', 'darkness', 'empire', 'let', 'well', 'wonder', 'guiding', 'permission', 'allowed', 'beengiven', 'use', 'gonnablast', 'afford', 'cannot', 'ofyou', 'seem', 'knight', 'train', 'lose', 'isn\\x92t', 'among', 'tahyts', 'anonymous', 'aswordsman', 'your', 'two', 'dagobah', 'serves', 'knowwhere', 'but', 'theybetray', 'run', 'honor', 'itspersonal', 'uptwelve', 'opinionated', 'enoughmen', 'half', 'wars', 'startled', 'specialty', 'button', 'field', 'democracy', 'avoid', 'longer', 'rethink', 'cato', 'business', 'us', 'give', 'nods', 'thinking', 'be', 'when', 'thisplot', 'master', 'intruder', 'trusthim', 'topof', 'republic', 'fallen', 'warm', 'twisted', 'reminds', 'count', 'planetsystem', 'mlady', 'fast', 'heard', 'millions', 'unusual', 'lets', 'waysto', 'thousands', 'feeling', 'beenthere', 'finished', 'twins', 'extra', 'shed', 'took', 'speak', 'saleucami', 'keeping', 'says', 'wewouldnt', 'defy', 'she', 'station', 'up', 'drag', 'believed', 'through', 'try', 'younglings', 'eye', 'theniceties', 'rim', 'watch', 'expired', 'youll', 'fault', 'iam', 'yourself', 'my', 'lightly', 'shutting', 'typho', 'os', 'to', 'hits', 'theircampaigns', 'press', 'isnt', 'mean', 'someone', 'about', 'pregnant', 'jedi', 'willing', 'say', 'doing', 'mark', 'a', 'showup', 'naboo', 'negotiaion', 'anythingfoolish', 'skywalker', 'geenine', 'missed', 'effort', 'higher', 'assignment', 'leads', 'care', 'areforming', 'somewhere', 'tests', 'againmlady', 'degrees', 'relieve', 'affected', 'area', 'both', 'taught', 'on', 'short', 'kenobi', 'makes', 'fire', 'four', 'easiest', 'win', 'mother', 'came'}\n",
            "[[4, 12, 5, 275, 276, 91, 15], [29, 13, 91, 1, 277, 23, 29], [23, 79, 550, 4, 124, 11, 384, 7, 1, 551], [98, 23, 125, 27, 2, 68, 1, 552, 553, 25, 554, 30], [555, 385, 143, 15, 277, 10, 386, 1, 556, 557], [42, 12, 558, 559], [29, 111, 560, 278], [2, 44, 50, 91, 87, 160, 23, 1, 561, 44], [144, 23, 1, 562, 563, 37, 228], [98, 23], [279, 15], [59, 42, 99, 74, 42, 25, 564, 74, 565, 74, 145, 161], [28, 191, 15, 43, 13, 60, 112, 280, 26, 566, 567], [281, 162, 80, 88, 192, 7, 1, 568, 42, 25, 146], [2, 8, 1, 282, 283, 5, 569, 570, 31, 571, 3, 572, 7], [23, 279, 5, 573], [24, 22, 284, 7, 51, 23], [23, 75, 27, 2, 163, 574, 164, 387, 165, 575, 285], [75, 44, 2, 388, 166, 166], [42, 388, 2, 229, 69, 389], [193, 21, 1, 286], [44, 576, 167], [390, 100], [2, 577, 9], [31, 10, 9], [47, 81, 39], [18, 19, 287], [230, 88, 288, 7, 18, 168, 578, 391, 1, 579, 7], [45, 169, 39, 8, 163, 63, 7, 289], [39, 23, 231, 29, 290, 63, 7, 1, 101, 580, 1], [29, 392, 32, 170, 34, 581], [582, 9, 231], [144, 583, 50, 194, 232, 233, 584, 585], [586, 1, 587, 588, 10, 291, 24, 25, 82, 5, 292], [4, 589, 9, 234, 23], [590, 591, 5, 592, 593, 7, 594, 1, 393, 595, 596], [15, 597, 61, 3, 293, 102, 83], [29, 5, 598, 235, 34, 394, 235, 34, 294, 7, 76, 295], [1, 599, 10, 296, 32, 600, 34, 394, 19, 395], [31, 40, 9, 10, 396, 8, 1, 195, 22, 397], [31, 40, 15, 297, 601, 23, 24, 70, 11, 602, 39, 26, 5], [98, 23], [126, 5, 398], [38, 50, 171, 172, 9], [127, 52, 11, 236, 30, 1, 294], [603, 1, 294, 237, 3, 11, 20, 92, 196, 32, 1], [128, 23, 79, 197, 12, 5, 604, 173, 18, 238], [31, 298, 9, 399], [75, 27, 4, 198, 174, 605, 81, 400, 387, 165, 283], [103, 41, 401, 606], [31, 19, 9], [1, 114, 25, 13, 299, 1, 607, 608, 23, 8, 2, 53], [28, 609, 1, 64, 23, 13, 199], [23, 2, 70, 11, 610, 16, 1, 64, 71, 45, 40, 2, 84], [611, 402, 3, 239, 1, 612], [9, 10, 13, 613, 23, 9, 10, 1, 403], [1, 114, 10, 175, 42, 38, 198, 9, 75, 115, 2], [1, 614, 103, 34, 82, 26, 9, 45], [5, 240, 192, 104], [166, 166, 10, 16, 89, 101, 3, 1, 615, 404, 23], [27, 2, 68, 1, 393, 300, 25, 405], [46, 46, 144, 26, 37, 616, 23, 9, 10, 13, 37, 241, 3], [77, 617, 42, 44, 618, 63], [8, 170, 111, 128, 619, 406, 40, 1, 620, 621, 6], [23, 23], [34], [98, 23], [242, 622, 20, 36, 4, 301, 20, 242], [23, 79, 623, 242, 37, 624, 4, 25, 407, 14, 200], [17, 10, 87, 30, 1, 48, 14, 2, 52, 93, 65], [4, 176, 6, 23, 45, 2, 25, 177, 5, 35, 4, 625], [29, 92, 3, 55, 2, 56, 166, 166], [29, 5, 178, 626, 3, 55, 2, 627], [46, 628, 1, 35, 64, 629, 630], [112, 302, 25, 11, 631], [632, 39, 3, 408, 633, 13, 3, 409, 634], [24, 22, 13, 61, 3, 635, 636, 37, 201, 410, 637], [24, 22, 13, 61, 202, 638, 199, 14, 2, 639, 411, 3, 37, 640], [31], [24, 25, 27, 33, 1, 64, 641, 8, 2, 25, 642, 241, 201, 87], [14, 69, 643, 16, 644, 199, 11, 384, 7, 645, 116, 19, 646, 3, 55, 74, 45, 129, 647, 1, 412, 39], [648, 649, 43, 77, 147, 650, 651, 34, 652, 653, 18, 101, 76, 654, 655], [279, 61, 16], [31, 10, 116, 656], [29, 13, 111, 657, 46, 658, 41, 22, 203, 117, 659, 243, 5, 148], [69, 660, 94, 33, 303], [29, 56, 392, 8, 6, 661, 18, 662, 413], [414], [2, 130, 663], [229, 7, 6, 244], [664, 299, 20, 51], [245, 6, 304, 14, 665, 2, 305, 131, 415, 3, 1, 35, 196, 415, 13, 416, 666, 8, 28, 179, 246, 667, 149, 13, 3, 668], [29, 60, 37, 247, 669, 22, 105, 670, 671, 162, 66, 672, 673, 8, 42, 22, 674, 675, 3, 179, 676, 7, 306, 3, 47, 677], [678, 5, 679, 680, 18, 17, 10, 78, 681, 682, 1, 683, 684, 7, 1, 685], [4, 198, 9, 56], [4, 12, 5, 275, 276, 91, 15], [31, 248, 2, 62, 118], [40, 307, 686, 33, 119, 687, 16, 6, 417, 688, 689, 27, 16, 6, 690, 201, 691, 84, 692, 23, 79, 33, 693], [105, 20, 6, 245, 37, 78, 413, 204, 308, 205], [4, 28, 245, 309, 694, 69, 206, 10, 695], [150, 293, 83], [696, 28, 65, 20, 41], [697, 53, 4, 28, 85, 9, 59, 698, 18], [31, 22, 2, 206, 95, 699, 21], [18, 19, 56, 287], [31], [700, 28, 27, 18, 701, 85, 9, 59, 2, 27, 18], [29, 175, 702, 162, 703, 293, 83, 28, 704, 41], [705, 31, 22, 2, 206], [706, 18, 19, 92], [151], [46, 418, 46, 418, 707], [419, 106, 405, 174, 708, 2, 709, 172, 74, 207], [78, 419, 19, 710], [711, 2, 106, 8, 45, 712, 36], [132, 22, 2, 61, 17, 713, 41, 1, 117, 101], [31, 27, 2, 399, 714, 715, 295, 7, 716, 17, 717, 1, 117, 101, 718, 36], [719, 62, 420, 75, 720, 61, 721, 236, 722], [723, 2, 310, 36], [14], [208], [724, 421, 51, 152, 13, 3, 311, 9], [5, 725, 417, 10, 89, 726, 727], [17, 52, 163, 9, 30, 36, 107, 728], [15, 228, 10, 6, 165], [32, 2, 249, 729, 730], [75, 27, 4, 68, 2, 22, 61, 3, 731, 422, 7, 21], [133, 75, 28, 2, 312, 3, 21], [27, 2, 55, 36, 36], [133, 11, 732, 204, 733, 3, 5, 734, 9, 63], [3, 47, 5, 735], [2, 28, 134, 3, 423, 21, 736], [2, 134, 3, 65, 313, 8, 737, 6, 165], [27, 2, 53, 66, 9, 19, 2, 738, 3, 243], [66, 739, 2], [15, 740, 61, 3, 741], [424, 425], [14, 2, 169, 314, 16, 282, 27, 13, 742, 76, 411, 743, 209, 744, 20, 30, 21, 180, 1, 64], [3, 745, 25, 47, 3, 1, 746, 7, 747, 426, 748, 194, 749, 39, 20, 34, 51], [153, 1, 48, 11, 30, 2], [4, 181, 17, 197, 152, 750], [29, 5, 424, 425, 4, 82, 3, 751, 9, 427, 57, 8, 66, 131, 9], [752, 21, 70, 2, 152, 753], [754, 26, 6, 755, 756, 13, 11, 135, 3, 315, 757, 32, 4, 68, 4, 53, 758, 316], [46, 210, 26, 759], [95, 13, 20, 289, 760], [308, 250], [317, 2], [308, 250], [2, 54, 211, 21, 31, 15, 10], [27, 2, 53, 132, 9, 427, 57], [428, 761, 4, 429, 762, 106, 430, 81, 20, 76, 763], [58, 250, 40, 80, 70, 68, 764, 11, 39, 84, 765, 197, 766, 767, 10, 9, 318, 7, 1, 182], [768, 22, 42, 769], [16, 31, 250], [319, 20, 770, 98, 4, 96], [4, 136, 771, 75, 17, 772, 105, 773, 35, 12, 320, 774, 196], [31, 154], [775], [431, 46, 137, 13, 776, 321], [98, 46, 777, 3, 99, 5, 778, 779, 428, 9, 780, 3, 430, 322, 76, 7, 781, 782], [783, 3, 37, 323, 784, 11, 20, 15, 785, 150, 786, 7, 787, 788], [38, 324, 789], [790], [325, 18, 11, 16, 432], [90, 791, 792, 433, 22, 793], [4, 176, 144, 3, 794, 2, 23], [795, 210, 26, 5, 796, 797, 21, 71, 111, 138, 139, 4, 798, 32, 1, 251, 799, 800, 16, 1, 801, 802], [15, 10, 132, 9, 434, 3, 11, 32, 9, 326, 803, 10, 804, 1, 805, 20, 15, 806, 807, 15, 808, 41, 124, 11, 809, 39, 32, 41, 326], [32, 23, 79, 66, 70, 810, 323, 57, 433, 420, 811, 9], [4, 176, 812, 26, 37, 410, 813, 13, 402, 3, 11, 16, 89, 327], [32, 17, 137, 43, 119, 3, 140, 8, 89, 435, 12, 131, 36, 58, 814, 4, 815, 816, 2, 8, 23, 79, 212, 817, 436, 1, 114, 19, 818, 3, 409, 1, 252, 8], [23, 17, 124, 13, 12, 819, 15, 328, 46, 820, 183, 11, 135, 3, 821, 148], [17, 43, 5, 111, 822, 30, 94, 29, 823, 213, 17, 19, 114, 824, 437, 319], [43, 23, 79, 825, 76, 826, 827, 180, 13, 15, 253, 828, 91], [829, 438], [168, 439], [2, 232, 21, 254, 329, 830], [831, 10, 92, 832], [46, 144, 23], [108, 98, 214], [90, 75, 46, 39], [132, 10, 15, 833, 834, 45], [835], [4, 84, 85, 3, 836, 15, 837], [2, 838, 839], [8, 164], [78, 440], [840, 21, 841, 842, 59, 843, 214, 155, 844, 96, 17, 143, 1, 196, 845, 330, 180], [846, 847], [184, 184], [184, 848, 15, 849, 850, 3, 851, 288, 7, 852, 853, 313], [6, 255, 22, 78, 440, 2, 52, 11, 78, 441], [854, 24, 38], [855, 131, 6, 101, 33, 215, 145, 856, 33, 857], [858], [133, 2, 52, 53, 23, 214], [214, 326, 17, 1, 35, 859, 2, 26, 15, 216], [34, 4, 217], [860], [9, 237, 3, 21, 29, 6, 278, 442, 18, 42, 22, 38, 255, 861], [4, 130, 862, 3, 863, 102, 864, 317, 2, 26, 6, 865], [4, 183, 179], [866, 443, 3, 138, 867], [868, 57, 1, 869, 8, 1, 870, 871, 872, 873, 7, 874, 57, 1, 875, 876], [1, 240, 877, 237, 3, 878, 107, 879, 880, 4, 881, 3, 65, 83, 8, 882, 4, 25, 331, 444, 883, 100, 884, 885], [87, 77, 160, 444, 886, 19, 887, 71, 445, 888, 889, 76, 300, 66, 890, 11], [27, 2, 301, 17, 70, 11, 891, 214, 66, 131, 892, 26, 1, 332, 278], [93, 21, 55, 40, 4, 54, 315, 893, 61, 16, 155], [446, 12, 894, 895, 3, 173, 185, 8, 896, 111, 126, 897, 898, 4, 28, 232, 9, 899], [4, 19, 436, 3, 429, 40, 900, 447, 37, 901], [9, 333, 85, 69, 61, 5, 902, 62, 215], [88, 1, 87, 1, 218, 903, 1, 87, 16, 1, 50], [9, 333, 85, 246, 256, 16, 904, 233], [905, 172, 3, 448, 83, 164, 80], [156, 16, 130, 103, 41], [34, 179, 94, 24, 12, 3, 906, 185], [28, 93, 6, 907, 908, 20, 1, 101, 174, 172, 5, 216, 909], [14, 246, 38, 50, 130], [910, 18, 449], [3, 911, 157, 20, 334, 2, 912, 1], [14, 34], [4, 28, 68, 62], [2, 25, 12, 3, 913, 9], [14], [16, 89, 101, 100, 3, 282, 17, 914, 148, 915, 313], [916, 52, 917, 200, 1, 918, 84, 13, 12, 60, 5, 919], [130, 63, 335, 80, 920], [174, 172, 3, 921, 102, 81], [205, 26, 2, 3, 143, 75, 176, 4, 158, 1, 303], [14, 149, 38, 103, 21], [46, 61, 83, 16, 1, 922], [450, 81, 4, 28, 85, 15], [67, 923], [67, 28, 128, 152, 3, 924, 9, 184, 171, 448, 9, 83], [421, 51, 69, 1, 303, 45, 129, 99, 1, 336, 104, 8, 47, 16, 30, 9], [4, 55, 9, 108, 15, 10, 61, 3, 11, 205], [13, 15, 51, 170, 56, 119, 107, 451, 24, 82, 109, 925, 926, 27, 2, 257], [927, 37, 452, 8, 283, 6, 928, 81, 453, 21], [929, 337, 930, 16, 1, 50], [88, 9, 205, 14], [337, 77, 16, 1, 50], [46, 61, 238, 8, 50], [169, 30, 21, 931, 100, 8, 50, 109, 21, 454, 100, 455, 93, 102, 299, 338, 74], [38, 50, 454, 8, 450, 164, 80, 22, 38, 103, 21, 85, 5, 932], [14, 2, 12, 335, 16, 6, 933], [335, 77, 934, 57, 6, 218], [456, 50, 8, 65, 238], [3, 330, 17, 137, 43, 119, 3, 140], [56, 175, 155, 35, 457], [150, 163, 21, 219, 156, 16, 13, 339, 45, 456, 218], [67, 194, 136, 47, 202, 41, 14, 29, 56, 935], [258, 1, 48, 68, 259, 202, 1, 104, 25, 458], [34, 34, 42, 22, 206, 340, 216, 62, 24, 54, 27, 936, 459, 26, 1, 336, 104], [42, 937, 74], [38, 50, 184, 34, 34, 209, 56, 938], [309, 10, 26, 80], [67, 46, 460, 14], [184, 11, 204, 2, 12, 87], [108, 939, 149, 940, 83, 38, 1, 286], [1, 277, 47, 3, 1, 336, 104, 47, 1, 72, 46, 941, 63, 7, 942, 39], [67, 20, 1, 341, 7], [14, 156, 6, 461, 156, 6, 461, 69, 13, 943, 39], [126, 126, 4, 115, 55, 5, 160, 37, 944, 945, 149, 38, 103, 21, 14], [156, 16, 14, 69, 61, 3, 47, 74, 220, 207, 47, 63, 7, 39, 170, 209, 77, 2, 54, 27], [462, 9, 4, 115, 55, 37, 286, 22, 291], [73, 460, 1, 463, 80, 946, 947], [178, 73], [12, 2, 948, 1, 949, 22, 137, 81], [108, 4, 12, 5, 275, 276, 91, 15], [73, 950, 1, 72], [1, 342, 951, 10, 464, 57, 50, 41, 1, 952, 953, 107, 1, 954, 7, 18, 955], [4, 198, 5, 343], [956, 1, 343], [39, 88, 15, 8, 126, 26, 957], [24, 22, 39, 3, 958, 2, 7, 72, 186, 13, 465, 36], [14], [344, 1, 959, 960, 57, 112, 961, 25, 235, 15, 104, 145, 962, 90, 75, 963, 964, 965], [6, 300, 7, 466, 10, 13, 1, 345, 33, 346], [966, 80, 464, 112, 101], [58, 136, 47, 202, 18, 29, 56, 290], [14, 15, 10, 34, 51, 26, 967, 44, 20, 968, 289, 39], [18, 183, 156, 59, 1, 344, 969, 162, 167, 970], [2, 137, 12, 119, 3, 140, 14], [38, 50, 2, 260, 4, 12, 119, 3, 140, 129, 65], [96, 2, 971, 1, 151, 972], [34], [24, 28, 134, 3, 47, 63, 24, 134, 3, 47, 973, 73, 73, 27, 2, 257, 974, 975, 333, 107, 192, 976], [73], [67, 158, 16, 1, 157], [467, 73, 977, 16, 1, 978, 73, 27, 2, 347, 21, 73, 24, 979, 2, 5, 216, 3, 27, 73], [151, 151, 73, 24, 82, 3, 11, 61, 81], [467, 73, 27, 2, 257, 73, 27, 2, 347, 21, 73, 24, 82, 3, 11, 61, 81, 13, 83], [151, 73, 24, 82, 3, 65, 81, 151, 151], [45, 90, 261], [108, 29, 2], [58, 73, 43, 60], [96, 4, 143, 468], [4, 106, 143, 468], [980, 72], [981, 3, 14, 15, 51, 24, 25, 27, 9, 334], [72, 186, 141, 982, 22, 112, 983], [2, 183, 47, 469, 15, 51, 185], [96, 4, 984, 127], [31, 10, 18], [67, 985], [56, 986, 470], [129, 55, 40, 24, 54, 99, 127, 20, 1, 987, 988, 90, 137, 989, 146, 16], [990, 7, 162, 348, 25, 47, 74, 471], [28, 130, 107, 21, 4, 28, 53], [58, 459, 391, 1, 991, 8, 55, 40, 24, 54, 99, 111, 349, 992], [126, 5, 398, 993, 15, 994, 44, 995, 147, 15], [108, 62, 38, 7, 5, 996, 29, 37, 472], [46, 473, 3, 997, 39], [208, 90, 6, 297, 10, 9], [27, 2, 12, 5, 297, 998], [18, 999, 322, 6, 350, 7, 351, 1000], [14, 152, 13, 3, 296, 36, 24, 12, 5, 216, 3, 27], [13, 15, 51, 8, 15, 51, 2, 183, 349], [474, 54, 2, 401, 5, 1001, 85, 15], [58], [219, 1002, 1003, 1004], [475, 219, 1005, 473, 1006, 1007, 1008, 1009], [13, 3, 191, 44, 137, 309, 1010, 1, 104], [204, 44, 1011, 1012, 120, 476, 120], [352, 262, 1013, 352, 262, 1014, 352, 262, 1015, 475, 1016, 120, 477, 120, 44, 20, 1, 1017], [219, 219], [337, 120, 324, 120, 263, 120, 1018, 16, 1, 218, 8, 1, 50], [257, 18, 478, 1019, 1020, 1021], [1022], [400, 1023, 478], [108, 34, 46, 13, 1024, 264, 26, 1025, 4, 12, 3, 353, 3, 1, 64, 1026, 445, 354, 3, 11, 1, 1027, 114], [144, 138, 139, 93, 74, 13, 179, 18, 2, 479, 21, 57, 1, 463, 80, 8, 2, 207, 173, 185, 8, 2, 479, 1, 72, 1028, 21, 1029, 16, 6, 100, 8, 2, 355, 3, 1030, 18, 1031, 7, 1032, 356], [1, 1033, 1034, 14, 129, 11, 1035, 480, 2, 22, 1, 1036, 8, 2, 1037, 6, 1038, 280, 30, 1, 1039], [1040, 51, 18, 481, 16, 482, 483, 197, 173, 230, 55, 2, 107, 1, 1041], [2, 1042, 1, 353, 16, 1, 1043, 1044, 1045], [20, 284, 42, 22, 61, 78, 58, 1046, 43, 484, 8, 23, 1047, 43, 1048, 89, 265, 3, 1049, 1050], [1, 357, 10, 438, 3, 1051, 77, 1052, 390, 3, 1, 72, 480], [14, 11, 204, 7, 6, 139, 186], [17, 43, 1053, 6, 302], [17, 84, 13, 143], [38, 7, 15, 10, 1054, 8, 29, 485, 21, 254, 1055, 69, 1056, 1057, 18, 1058, 338, 1, 64, 8, 1, 72, 22, 1059], [1, 48, 1060, 113, 14, 8, 24, 22, 38, 1061, 71, 9, 11, 1062, 7, 6, 221], [9, 153, 88, 358, 51, 24, 27, 13, 12, 203, 348, 3, 1063], [8, 339, 9, 84, 11, 1064, 26, 74, 3, 486, 1, 240, 1065, 3, 1066], [95, 50, 18, 10, 5, 251, 24, 187, 1067, 3, 311, 29, 1, 1068, 1069, 1070, 26, 1, 1071, 1072], [1073, 83, 14, 2, 12, 60, 1074, 5, 178, 1075, 3, 11, 16, 1, 64, 107, 6, 1076, 29, 136, 154, 121, 312, 3, 21, 14, 1, 1077, 7, 1, 1078, 10, 69, 56, 287, 3, 1, 72, 1, 64, 197, 85, 9, 59, 17, 1079, 20, 35, 1080], [32, 29, 31, 2, 359, 6, 1081, 30, 72, 186, 237, 3, 12, 1082, 455], [14, 1083, 7, 125, 9, 154, 2, 99, 259, 20, 5, 1084, 360], [4, 1085, 2, 41, 19, 1086, 338, 1, 64, 8, 1, 72, 4, 19, 78, 1087, 75, 106, 2, 312, 2, 1088, 50, 145, 9], [34, 9, 10, 13, 14, 4, 191, 59, 2, 1089, 7, 1090, 8, 487, 162, 22, 13, 35, 304, 149, 175, 113, 304], [4, 181, 62], [14, 1, 105, 488, 1, 64, 43, 1091, 6, 1092, 10, 229, 1, 72, 1093, 2], [14, 130, 4, 176, 16, 6, 97, 4, 106, 134, 3, 55, 2, 314, 20, 15, 360], [1094, 5, 361, 1095, 1, 64, 489, 2, 3, 353, 16, 38, 7, 1, 342, 1096, 42, 134, 3, 53, 31, 95, 81, 3], [24, 22, 107, 253, 14, 1, 35, 64, 10, 1097, 3, 1098, 1, 1099, 7, 1, 182, 128, 40, 1, 72, 298, 13], [15, 328, 10, 13, 3, 11, 16, 432, 1, 64, 490, 21, 3, 1100, 2, 16, 15, 1101], [18, 10, 75, 2, 52, 109, 74, 14, 112, 491, 10, 3, 1, 357, 13, 3, 29, 492, 66, 43, 355, 3, 169, 20, 1102, 118, 281, 89, 1103, 43, 1104], [98, 32, 258, 6, 221, 14, 127, 10, 63, 7, 241], [1, 64, 10, 1105, 2], [14, 96, 13, 88, 3, 89, 328, 30, 119, 1106], [493, 11, 38, 50, 4, 362, 36, 30, 37, 165], [30, 38, 1107, 1108, 23, 10, 17, 13, 1, 494, 87, 10, 17, 13, 3, 188, 1, 141, 8, 331, 495, 3, 1, 48], [17, 25, 13, 93, 21, 83, 17, 136, 43], [298, 1109, 496], [108, 34, 69, 363, 50, 32, 4, 27, 12, 1, 329, 247, 30, 89, 364, 7, 1110], [43, 14, 60, 3, 55, 2], [4, 53, 17, 1111, 9, 17, 10, 497, 222, 1112, 78, 1113, 32, 1114, 1115], [2, 124, 11, 5, 35, 110], [29, 14, 95, 1116, 1117, 8, 1118, 95, 60, 314, 20, 5, 1119, 452, 33, 1, 342, 1120, 32, 4, 68, 29, 77, 147, 18, 4, 19, 1121, 17, 153, 12, 1122, 3, 2], [1123, 7, 2, 10, 78, 92, 107, 1124, 6, 221, 1125], [4, 53, 125, 17, 1126, 91, 2], [209, 17, 106, 12, 3], [4, 53, 2, 220, 56, 58, 4, 54, 55, 2, 263, 22, 20, 1127, 110, 46, 1128, 91, 36], [67, 4, 365, 6, 1129, 43, 437, 36, 95, 1130, 1131, 213, 24, 1132], [67, 110, 46, 13, 1133, 1, 64, 91, 76, 7, 15, 4, 4, 181, 4, 106, 296, 2, 44, 38, 498, 4, 288, 91, 220, 7, 2], [1134, 27, 31, 2, 54, 3, 109, 36], [108, 4, 496, 1135, 9, 153, 499, 63, 150, 3, 11, 5, 1136, 1137, 1138], [2, 22, 222, 8, 1139, 14, 8, 4, 176, 78, 441, 7, 2, 4, 12, 366, 2, 213, 2, 44, 5, 290, 114, 4, 12, 367, 2, 500, 4, 53, 8, 2, 12, 177, 5, 215, 501, 35, 147, 4, 70, 320, 181, 3, 11, 8, 2, 12, 1140, 37, 165, 77, 502, 147, 4, 54, 223, 32, 11, 368, 14, 9, 183, 11, 118, 121, 1, 64, 1141, 2, 5, 35, 23], [67, 28, 191, 4, 12, 264, 255, 30, 21, 3, 88, 324, 266, 1, 503, 7, 1142, 4, 68, 230, 11, 135, 3, 1143, 1, 360, 128, 200, 6, 109], [1144, 138, 139, 153, 1, 48, 11, 30, 2], [230, 163, 102, 319, 267, 2, 47, 41, 150, 28, 88, 56, 118], [1145, 482, 483, 26, 1146], [78, 58, 1, 1147, 10, 16, 21, 13, 3, 188, 38, 1, 80, 121, 2, 47, 41], [369, 6, 370], [28, 193, 81, 110, 28, 193, 81], [1148, 1, 253], [30, 6, 295, 1149, 4, 124, 85, 358, 344, 8, 3, 258, 6, 404, 33, 5, 1150, 33, 4, 1151, 1152, 266, 26, 1153, 474], [4, 321], [211, 6, 195, 3, 88, 1154, 40, 2, 12, 1155, 45, 10, 1, 51], [67, 1156, 88, 1, 268, 100, 3, 1, 104, 46, 1157, 39, 211, 504, 171, 131, 224], [4, 82, 1158], [47, 9, 26, 21], [15, 87], [92, 1159, 1160], [505, 41], [6, 157], [2, 179, 4, 366, 1, 35, 18, 1161, 173, 185], [4, 153, 13, 371, 6, 80, 32, 37, 265, 506, 25], [4, 28, 68, 62], [62, 1162], [1163, 224, 6, 265, 211, 102, 3, 157, 3, 1, 1164, 1165], [317, 2, 504, 1166, 45, 129, 47, 5, 157, 16, 174, 172, 5, 507, 3, 260, 39], [1167, 508, 477, 476, 4, 12, 34, 224, 16, 76, 1168, 22, 41, 76, 35, 63, 41, 471], [67, 171, 1169, 16, 1170], [148, 372, 37, 332, 265, 269, 16, 21, 4, 82, 109], [2, 44, 1171, 71, 6, 255, 234], [125, 203, 117, 35, 355, 3, 1172], [12, 24, 122, 76, 224, 57, 1, 1173], [58, 133, 24, 52, 65, 100, 40, 41, 22, 117, 1174, 42, 25, 1175, 145, 1, 343, 8, 11, 207], [98, 23, 41, 10, 56, 119, 107, 451, 39, 8, 24, 82, 5, 1176, 1177, 7, 31, 43, 154], [34, 4, 28, 68, 62, 1, 72, 25, 13, 11, 135, 3, 192, 1, 1178, 7, 509, 266, 200, 1179, 1, 357, 1180], [9, 84, 11, 261, 40, 24, 510, 30, 1, 148], [41, 22, 1181, 1182, 7, 332, 1183, 16, 1184, 1185, 203, 22, 1186, 33, 35], [13, 128, 1, 511, 1187], [66, 66, 70, 12, 225, 15], [171, 1188, 1, 508, 1189, 38, 1190, 35, 3, 169, 469], [126, 23, 41, 10, 127, 4, 52, 53], [4, 52, 53, 1, 403, 23], [67, 9, 115, 11, 9, 115, 11], [4, 115, 270, 76, 77], [235, 21, 3, 243, 1, 142, 4, 25, 13, 243, 14], [17, 10, 85, 37, 373, 4, 187, 27, 9], [125, 70, 9, 12, 146, 3, 15], [4, 27, 13, 53, 132, 1, 142, 43, 395, 36, 4, 28, 53, 132, 3, 130], [153, 1, 48, 11, 30, 2, 23, 79], [43, 14, 60, 39], [1, 182, 43, 484, 110, 1, 35, 196, 10, 34, 77], [4, 301, 24, 12, 60, 318, 7, 5, 1191, 1192, 7, 1193, 20, 1, 485], [34, 110, 29, 103, 1, 141, 45, 457, 1, 271, 33, 42, 96, 121, 1, 182], [46, 39, 210, 26, 14, 59, 19, 1, 512, 51, 2, 1194, 36], [8, 27, 2, 53, 132, 17, 10, 45], [110, 4, 82, 6, 109, 95, 20, 1195, 406], [57, 330, 110, 14, 43, 269, 3, 1, 113, 97], [4, 12, 1196, 5, 412, 1197, 7, 36, 1198, 511], [17, 19, 1199, 71, 5, 1200, 24, 38, 44, 9, 1201, 18, 1, 72, 10, 453, 500, 1202, 1, 253, 186, 10, 1, 141, 374, 174, 60, 210, 26, 281, 1, 422, 7, 173, 185, 14, 159, 89, 292, 1203], [110, 4, 52, 99, 36], [17, 43, 177, 5, 78, 178, 513], [14, 10, 1, 86, 514, 17], [67, 46, 62, 144], [93, 94, 65, 14], [93, 94, 65], [2, 12, 225, 18, 259], [6, 1204, 8, 6, 1205, 26, 167, 12, 256, 225, 18], [67, 2, 12, 1206, 15, 113, 374, 3, 1207, 6, 245, 267, 45, 267, 45, 2, 12, 177, 1, 78, 160, 2, 1208, 3, 188], [6, 292, 189], [14, 37, 491, 10, 3, 1, 182, 3, 306], [105, 5, 141, 374, 1209, 20, 1210, 4, 25, 27, 31, 4, 52], [171, 515, 18, 121, 14, 32, 4, 136, 217, 385, 347, 9, 57, 2], [67, 1, 1211, 7, 167, 10, 1212], [4, 12, 1213, 2, 14, 4, 19, 136, 135, 3, 1214, 2, 3, 68], [57, 1, 141, 14, 72, 186, 10, 272], [58, 133, 2, 22, 310], [29, 103, 14, 4, 12, 1, 238, 1215], [28, 152, 9], [67, 2, 44, 1, 494, 87, 9, 19, 1216, 18, 2, 84, 188, 1, 141, 13, 465, 102, 9, 19, 2, 66, 84, 331, 495, 3, 1, 48, 13, 516, 9, 20, 1217], [2, 44, 37, 373, 14, 4, 1218, 2], [246, 397], [28, 193, 81, 110], [2, 12, 517, 110, 42, 82, 2, 1219, 16], [369, 6, 370], [24, 52, 88, 102, 1220, 1, 141, 25, 13, 198, 340, 302], [8, 31, 7, 1, 114], [4, 25, 88, 1, 1221, 8, 270, 103, 36, 23, 79, 27, 2, 68, 1222, 517, 25, 11, 135, 3, 371, 190, 1223], [252], [66], [242, 32, 125, 70, 17, 1224, 15], [4, 25, 11, 135, 3, 1225, 30, 36], [505, 41, 146, 39, 37, 161, 139, 28, 11, 1226], [28, 191, 493, 11, 38, 50], [1227, 205, 518, 305, 122, 5, 1228, 280, 69, 1229, 69, 137, 20, 87, 1230], [1, 1231, 1232, 22, 13, 3, 11, 1233, 1234, 211, 21, 201, 49, 31, 1235, 2, 63, 15, 215], [168, 439, 168, 45, 90, 5, 341, 4, 249, 515, 20, 5, 118, 51, 5, 118, 51], [108, 95, 13, 1236, 34, 13, 339], [58, 7, 226, 7, 226, 4, 53, 36, 95, 21, 4, 249, 291, 71, 1, 341, 168, 213, 108, 121, 2, 44, 519], [28, 520, 3, 223, 320, 1237, 5, 240, 78, 431], [4, 68, 24, 261, 47, 1238, 1, 375, 195, 22, 416, 1239, 32, 42, 25, 1240, 11, 100, 8, 20, 501, 521], [426, 518, 149, 16, 1, 157], [90, 31, 6, 376, 377, 2, 17, 106, 156, 30, 6, 378, 1241, 217, 17, 124, 12, 510, 39, 8, 13, 447, 1242], [98, 4, 19, 1243, 5, 35, 522, 1, 345, 33, 6, 86], [17, 19, 1, 523, 1244, 20, 1, 271, 8, 5, 1245, 1246, 4, 321, 305, 177, 363, 5, 92, 524, 259, 8, 17, 19, 5, 92, 139, 1247, 1248, 21], [4, 12, 127, 39, 26, 2, 6, 86, 359, 2, 3, 12, 15, 59, 2, 44, 138, 264, 32, 6, 376, 325, 486, 9, 17, 1249, 2, 316, 458, 138, 168, 16, 358, 1250, 1251, 1252, 85, 6, 86, 96], [6, 378, 1253, 15, 10, 1, 228, 7, 5, 35, 522, 13, 33, 389, 180, 33, 1254, 33, 5, 1255], [111, 1256, 228, 26, 5, 1257, 51, 26, 103, 5, 120, 1258, 1, 35, 525, 44, 1, 1259, 7, 1260, 8, 1261, 20, 1, 138, 182, 121, 1, 113, 502, 121, 1, 189], [5, 201, 35, 1262, 190, 123, 66, 19, 5, 1263, 7, 346, 267, 17, 269, 3, 272, 1264, 1, 189, 1265, 83, 8, 188, 1, 35, 525, 17, 1266, 8, 1267, 6, 86, 45, 1, 35, 22, 38, 32, 1268, 123, 19, 379, 71, 1, 113, 97, 7, 1, 48], [58, 1, 48, 10, 31, 1269, 1, 35, 89, 167, 29, 111, 370, 1270, 1271, 71, 38, 1272, 233, 9, 1273, 74, 8, 1274, 74, 9, 1275, 1, 271, 334], [45, 129, 55, 40, 24, 115, 315, 63, 31, 2, 22, 37, 161, 139, 8, 132, 2, 146, 57], [4, 520, 3, 12, 380, 9], [2, 52, 140, 1, 364, 7, 1, 48, 40, 69, 3, 146, 30, 21, 3, 227], [4, 82, 6, 109, 49, 46, 526, 56, 138, 26, 15, 1276, 7, 160, 116, 354, 6, 109], [90, 6, 376, 1277], [140, 91, 1, 48, 49], [2, 52, 27, 31, 2, 254, 10, 50, 7, 226], [42, 106, 32, 24, 22, 1278, 3, 68, 42, 96, 164, 1279, 22, 97, 71, 97, 375, 195, 158, 1280, 1281, 1282, 3, 1283, 41, 521], [8, 164, 462, 1284, 56, 1285, 26, 375, 195, 105, 273, 1286, 22, 62, 1287], [126, 49, 29, 56, 175], [170, 209, 2, 70, 12, 225, 49, 122, 2, 60, 41, 307, 12, 60, 207, 56, 8, 1, 80, 84, 45, 11, 20, 1, 1288, 7, 1, 189], [1289, 1290, 1291, 2, 25, 136, 99, 5, 77, 1292, 1293, 7, 1294, 8, 1295, 24, 52, 11, 1296], [149, 26, 1297, 40, 2, 134, 102], [2, 28, 82, 3, 55, 89, 1298], [210, 26], [17, 54, 65, 91, 89, 481], [157, 285], [1, 48, 54, 12, 5, 222, 1299, 16, 1, 1300, 2, 25, 99, 9, 5, 274, 1301], [58, 329, 7, 1, 523, 1302, 1303, 54, 11, 380, 39, 105, 270, 6, 527, 15, 241, 54, 11, 5, 161, 1304], [15, 161, 87, 514, 1305, 1, 1306, 146, 93, 21, 1307, 2, 127], [15, 10, 1308, 95, 1309, 16, 5, 104, 18, 316, 1310, 112, 354], [98, 1311, 40, 29, 5, 528, 104], [124, 4, 12], [105, 1312, 529, 1, 114, 263, 80, 8, 34, 1313, 490], [129, 150, 143, 1314, 85, 3, 1315, 76, 273, 1316], [24, 249, 18, 119, 30, 74, 32, 24, 70, 1317, 2, 263, 120, 45, 262, 1318, 59, 24, 1319, 227], [1320], [194, 12, 3, 423, 6, 449], [9, 25, 11, 264], [40, 1, 348, 33, 528, 33, 95, 1321, 24, 434, 3, 27, 58], [125, 118, 121, 2, 54, 232, 1, 470, 3, 443, 1322], [4, 1323, 5, 178, 1324, 20, 1, 48, 33, 40, 1325, 7, 1326, 530, 1327, 63, 20, 1328, 8, 44, 530, 1329, 4, 365, 127, 531, 43, 154], [307, 261, 47, 16, 30, 6, 1330], [223, 5, 35, 54, 254, 1, 48, 1331, 202, 36], [1332, 32, 9, 234, 1333, 6, 1334], [4, 1335, 2, 152, 9, 199, 49], [15, 51, 93, 65, 6, 1336, 1337, 8, 1338, 16, 1339], [6, 1340, 54, 1341, 2, 28, 362, 102], [1342, 63, 30, 6, 221], [2, 55, 2, 54, 27, 9], [20, 37, 247, 170, 34, 1343, 160, 33, 1344], [90, 92, 2, 12, 1345, 6, 155, 527, 145, 5, 1346, 1347], [532, 71, 1, 189], [29, 111, 273, 268], [34, 29, 5, 284, 533, 268], [1348, 11, 33, 58, 3, 93, 9, 65, 29, 56, 215, 63, 7, 533], [5, 268, 18, 503, 381, 47, 15, 361, 145, 534, 16, 29, 327], [90, 34, 1349, 29, 5, 534, 1350], [499, 1, 104, 1351], [2, 115, 260, 32, 41, 22, 1352, 3, 1353], [516, 18, 3, 21], [1354, 1, 77, 1355, 1, 535, 180, 1, 535, 66, 1356, 36], [1357, 20, 17, 124, 11, 135, 3, 1358, 1, 1359, 273, 1360, 1361], [4, 28, 68, 2, 1362, 54, 109, 4, 52, 65, 536], [11, 368, 49, 169, 8, 270, 103, 1, 80], [42, 52, 11, 1363, 356, 180, 117, 509, 266, 25, 537, 1, 345, 538, 33, 227, 6, 539, 1364, 285, 5, 540, 382, 57, 346, 1, 48, 25, 11, 30, 2, 158], [105, 5, 23, 7, 272, 190], [2, 115, 260, 190, 40, 2, 1365, 21, 83, 4, 1366, 177, 77, 274, 147, 2, 54, 414, 1367], [541, 49, 541], [49, 1, 48, 25, 11, 30, 2], [258, 1, 48, 49], [93, 65, 49], [49, 362, 21], [223, 1, 48, 25, 11, 30, 2, 158], [49, 49], [2, 25, 65, 3, 1, 1368, 251], [41, 2, 25, 140, 57, 79, 1, 35, 23, 66, 1369, 21], [17, 25, 140, 208], [19, 4, 76, 540, 59, 2, 367, 21], [62, 19, 4, 40, 194, 223], [2, 28, 53, 18], [128, 79, 187, 55, 340, 538], [32, 2, 187, 192, 9, 15, 10, 5, 175, 51, 26, 2, 59, 2, 25, 11, 1370, 71, 1, 113, 97, 7, 1, 48], [9, 10, 2, 8, 6, 435, 1, 142, 489, 18, 10, 75, 6, 498, 22, 131, 3, 537], [49, 4, 28, 134, 3, 311, 2, 3, 1, 142, 1, 101, 4, 310, 123], [208], [40, 2, 1371, 3, 239, 123, 2, 25, 27, 9, 536, 4, 187, 1372], [49, 28, 193, 20, 3, 1373, 18, 1374, 3, 1, 113, 97], [18, 114, 10, 112, 512, 181], [79, 25, 158, 11, 30, 2], [2, 86, 19, 379, 71, 1, 113, 97, 7, 1, 48, 17, 1375, 3, 11, 14, 1376, 8, 159, 190, 123, 59, 18, 154, 1, 92, 542, 66, 19, 6, 86, 19, 532, 62, 31, 4, 12, 377, 2, 19, 396, 57, 5, 1377, 350, 7, 351], [49, 69, 61, 3, 99, 18, 203, 7, 1, 1378, 24, 1379, 3, 1380, 1381, 16, 112, 327, 350, 7, 351], [4, 28, 1382, 2, 26, 442, 1383, 40, 4, 19, 236, 20, 31, 4, 96, 9, 506, 325, 12, 60, 26, 1, 155, 51, 2, 55, 31, 154, 3, 6, 86, 19, 37, 472], [14, 19, 5, 92, 139], [59, 4, 155, 212, 36, 6, 86, 19, 256, 5, 178, 524, 32, 4, 19, 1384, 125, 1385, 1, 48, 19, 30, 36, 4, 248, 9, 322, 529, 3, 407, 36, 33, 5, 35, 4, 217, 18, 4, 70, 1386, 36, 150, 33, 58, 33, 79, 4, 19, 236, 37, 487, 43, 122, 531, 1387, 26, 1, 271], [4, 234, 217, 17, 70, 11, 269, 100, 3, 1, 92, 97, 9, 381, 11, 225, 17, 10, 77, 1388, 45, 147, 542, 1389, 8, 272], [2, 187, 349, 6, 539], [123, 1390, 2, 59, 155, 2, 1391, 36, 49, 32, 18, 247, 19, 318, 7, 6, 252, 9, 367, 2, 1392, 117, 233, 1, 1393, 7, 208, 122, 2, 13, 60, 62, 497, 3, 371, 123, 133, 2, 70, 12, 1394, 6, 252, 39, 30, 79, 2, 84, 12, 60, 1395], [8, 96, 2, 109, 102, 9, 19, 42, 66, 122, 3, 369, 2, 2, 1396, 161, 71, 1397, 100, 1398, 4, 365], [3, 11, 5, 35, 49, 2, 52, 1399, 8, 133, 65, 1400, 1, 113, 97, 1, 97, 6, 86, 381, 47, 386, 1401, 10, 1, 1402, 543, 26, 2, 85, 6, 86, 105, 6, 86, 19, 379, 71, 31, 17, 380, 16, 1, 117, 97, 7, 1, 543, 8, 2, 12, 1403, 1404, 69, 34, 1405, 62, 1406, 45, 49, 2, 22, 222, 8, 368, 8, 45, 2, 52, 239, 190, 123, 199], [133, 1, 142, 43, 256, 1407, 2, 44, 112, 105, 181], [1, 117, 17, 1408, 7, 10, 6, 1409, 544], [1410, 3, 408, 2, 220, 57, 1, 142, 2, 44, 1411, 57, 6, 86, 59, 2, 44, 519, 1, 142, 212, 33, 4, 96, 40, 14, 44, 3, 12, 76, 1412, 42, 84, 11, 5, 513, 3, 36, 18, 10, 1, 488, 75, 6, 544, 1413, 356, 1414], [6, 1415, 1416, 2, 58, 1417, 6, 221, 361, 83, 49, 42, 27, 2, 1418, 32, 42, 70, 11, 131, 3, 1419, 1, 142], [59, 6, 86, 218, 17, 106, 53, 6, 244, 19, 1420, 6, 244, 8, 4, 212, 17, 84, 99, 63, 1421, 32, 24, 359, 3, 163, 2, 220, 33, 466, 33, 545, 26, 33, 118, 33, 545, 62, 4, 248, 2, 3, 546, 30, 37, 373, 1422, 16, 231, 8, 6, 244, 248, 383, 3, 546, 33, 1, 1423, 7, 148, 372, 16, 227], [1, 372, 1424, 19, 1425, 8, 547, 363, 274, 20, 18, 251, 383, 159, 5, 1426, 71, 1427, 7, 1428, 34, 87, 212, 1429, 60, 1430, 7, 226, 32, 9, 19, 5, 1431, 200, 1432, 167, 213, 227, 122, 118, 60, 5, 306, 128, 62, 1, 548, 1433, 3, 11, 547, 274, 8, 383, 1434, 20, 94, 549, 378, 382, 159, 5, 148, 33, 58, 90, 13, 38, 116, 159, 7, 226, 116, 159, 1, 492, 7, 94, 1435, 20, 1, 446, 1436, 1, 1437, 189, 8, 229, 116, 122, 1438, 1439, 116, 19, 5, 1440, 1441, 26, 526, 323, 3, 1, 1442, 1443, 90, 31, 116, 19, 206, 59, 94, 382, 1444, 1445, 26, 94, 549, 1446, 122, 158, 377, 94, 3, 224, 21, 16, 231, 40, 94, 1447, 159, 1448], [116, 1449, 60, 366, 20, 1, 364, 7, 1, 35, 1, 101, 2, 12, 49, 32, 1, 48, 10, 222, 30, 94, 33, 9, 10, 30, 38, 7, 6, 548, 41, 10, 34, 1450, 1, 507, 2, 52, 239, 8, 188, 123]]\n",
            "1450\n",
            "583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y80rDO8Dylvs"
      },
      "source": [
        "#Create sliding windows\n",
        "seq_length = 5\n",
        "dataX = []\n",
        "dataY = []\n",
        "for dialogue in sequences_tokenised:\n",
        "  window = []\n",
        "  for i in range(len(dialogue)-5):\n",
        "    dataX.append(dialogue[i:i+5])\n",
        "    dataY.append(dialogue[i+5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkiKTnv50oaZ",
        "outputId": "628dfa95-3471-4b96-f4b3-ec0bc9a54443"
      },
      "source": [
        "print(len(dataX[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kSyEpL20ta0",
        "outputId": "106c15f3-622e-473b-fbdd-8065ac66618b"
      },
      "source": [
        "print(len(dataY))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U3cLt2yLIpe",
        "outputId": "af0bab4c-b1eb-4e4b-bcdb-621f36b96f39"
      },
      "source": [
        "np.asarray(dataY).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3596,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "PKnQ8XTJLNQk",
        "outputId": "38e28680-a1f1-4422-ff13-e3da6bf2f20a"
      },
      "source": [
        "pd.get_dummies(np.asarray(dataY))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>1404</th>\n",
              "      <th>1405</th>\n",
              "      <th>1406</th>\n",
              "      <th>1407</th>\n",
              "      <th>1409</th>\n",
              "      <th>1411</th>\n",
              "      <th>1412</th>\n",
              "      <th>1413</th>\n",
              "      <th>1414</th>\n",
              "      <th>1417</th>\n",
              "      <th>1418</th>\n",
              "      <th>1419</th>\n",
              "      <th>1420</th>\n",
              "      <th>1421</th>\n",
              "      <th>1422</th>\n",
              "      <th>1423</th>\n",
              "      <th>1426</th>\n",
              "      <th>1427</th>\n",
              "      <th>1428</th>\n",
              "      <th>1429</th>\n",
              "      <th>1430</th>\n",
              "      <th>1431</th>\n",
              "      <th>1432</th>\n",
              "      <th>1433</th>\n",
              "      <th>1434</th>\n",
              "      <th>1435</th>\n",
              "      <th>1436</th>\n",
              "      <th>1437</th>\n",
              "      <th>1438</th>\n",
              "      <th>1439</th>\n",
              "      <th>1440</th>\n",
              "      <th>1441</th>\n",
              "      <th>1442</th>\n",
              "      <th>1443</th>\n",
              "      <th>1444</th>\n",
              "      <th>1445</th>\n",
              "      <th>1446</th>\n",
              "      <th>1447</th>\n",
              "      <th>1448</th>\n",
              "      <th>1450</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3591</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3592</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3593</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3594</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3595</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3596 rows  1100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      1     2     3     4     5     6     ...  1444  1445  1446  1447  1448  1450\n",
              "0        0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "1        0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "2        0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "3        0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "4        0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "...    ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
              "3591     0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "3592     0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "3593     0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "3594     0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "3595     0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "\n",
              "[3596 rows x 1100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sib8cyHxKbo3",
        "outputId": "bef05938-a8dc-4f34-caa0-87b07cd1a28b"
      },
      "source": [
        "# define model\n",
        "model_2 = Sequential([\n",
        "    Embedding(n_vocab+1, 50, input_length=5),\n",
        "    LSTM(100, return_sequences=True),\n",
        "    LSTM(100),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(1100, activation='softmax')\n",
        "])\n",
        "\n",
        "# Train model with checkpoints\n",
        "model_2.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "filepath = \"./model_2_weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "history = model_2.fit(np.asarray(dataX),\n",
        "         pd.get_dummies(np.asarray(dataY)),\n",
        "         epochs = 300,\n",
        "         batch_size = 128,\n",
        "         callbacks = callbacks_list,\n",
        "         verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "29/29 [==============================] - 4s 32ms/step - loss: 6.9741 - accuracy: 0.0248\n",
            "\n",
            "Epoch 00001: loss improved from inf to 6.89914, saving model to ./model_2_weights.hdf5\n",
            "Epoch 2/300\n",
            "29/29 [==============================] - 1s 31ms/step - loss: 6.1317 - accuracy: 0.0417\n",
            "\n",
            "Epoch 00002: loss improved from 6.89914 to 6.14770, saving model to ./model_2_weights.hdf5\n",
            "Epoch 3/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.9960 - accuracy: 0.0470\n",
            "\n",
            "Epoch 00003: loss improved from 6.14770 to 5.99249, saving model to ./model_2_weights.hdf5\n",
            "Epoch 4/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.8996 - accuracy: 0.0520\n",
            "\n",
            "Epoch 00004: loss improved from 5.99249 to 5.94814, saving model to ./model_2_weights.hdf5\n",
            "Epoch 5/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.8949 - accuracy: 0.0523\n",
            "\n",
            "Epoch 00005: loss improved from 5.94814 to 5.90963, saving model to ./model_2_weights.hdf5\n",
            "Epoch 6/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.8402 - accuracy: 0.0524\n",
            "\n",
            "Epoch 00006: loss improved from 5.90963 to 5.88460, saving model to ./model_2_weights.hdf5\n",
            "Epoch 7/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.8226 - accuracy: 0.0410\n",
            "\n",
            "Epoch 00007: loss improved from 5.88460 to 5.85053, saving model to ./model_2_weights.hdf5\n",
            "Epoch 8/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.7916 - accuracy: 0.0549\n",
            "\n",
            "Epoch 00008: loss improved from 5.85053 to 5.80133, saving model to ./model_2_weights.hdf5\n",
            "Epoch 9/300\n",
            "29/29 [==============================] - 1s 31ms/step - loss: 5.7736 - accuracy: 0.0493\n",
            "\n",
            "Epoch 00009: loss improved from 5.80133 to 5.78271, saving model to ./model_2_weights.hdf5\n",
            "Epoch 10/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.7561 - accuracy: 0.0472\n",
            "\n",
            "Epoch 00010: loss improved from 5.78271 to 5.77164, saving model to ./model_2_weights.hdf5\n",
            "Epoch 11/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 5.7421 - accuracy: 0.0505\n",
            "\n",
            "Epoch 00011: loss improved from 5.77164 to 5.74063, saving model to ./model_2_weights.hdf5\n",
            "Epoch 12/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 5.6899 - accuracy: 0.0593\n",
            "\n",
            "Epoch 00012: loss improved from 5.74063 to 5.72301, saving model to ./model_2_weights.hdf5\n",
            "Epoch 13/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.6831 - accuracy: 0.0541\n",
            "\n",
            "Epoch 00013: loss did not improve from 5.72301\n",
            "Epoch 14/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.7058 - accuracy: 0.0528\n",
            "\n",
            "Epoch 00014: loss improved from 5.72301 to 5.70873, saving model to ./model_2_weights.hdf5\n",
            "Epoch 15/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 5.6738 - accuracy: 0.0547\n",
            "\n",
            "Epoch 00015: loss improved from 5.70873 to 5.69144, saving model to ./model_2_weights.hdf5\n",
            "Epoch 16/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 5.6943 - accuracy: 0.0480\n",
            "\n",
            "Epoch 00016: loss improved from 5.69144 to 5.68886, saving model to ./model_2_weights.hdf5\n",
            "Epoch 17/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.6580 - accuracy: 0.0538\n",
            "\n",
            "Epoch 00017: loss improved from 5.68886 to 5.67703, saving model to ./model_2_weights.hdf5\n",
            "Epoch 18/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 5.6266 - accuracy: 0.0546\n",
            "\n",
            "Epoch 00018: loss improved from 5.67703 to 5.65793, saving model to ./model_2_weights.hdf5\n",
            "Epoch 19/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.6106 - accuracy: 0.0596\n",
            "\n",
            "Epoch 00019: loss improved from 5.65793 to 5.64782, saving model to ./model_2_weights.hdf5\n",
            "Epoch 20/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 5.6161 - accuracy: 0.0521\n",
            "\n",
            "Epoch 00020: loss improved from 5.64782 to 5.63231, saving model to ./model_2_weights.hdf5\n",
            "Epoch 21/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 5.5788 - accuracy: 0.0503\n",
            "\n",
            "Epoch 00021: loss improved from 5.63231 to 5.62152, saving model to ./model_2_weights.hdf5\n",
            "Epoch 22/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 5.5671 - accuracy: 0.0551\n",
            "\n",
            "Epoch 00022: loss improved from 5.62152 to 5.59998, saving model to ./model_2_weights.hdf5\n",
            "Epoch 23/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 5.5646 - accuracy: 0.0505\n",
            "\n",
            "Epoch 00023: loss improved from 5.59998 to 5.57976, saving model to ./model_2_weights.hdf5\n",
            "Epoch 24/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 5.5656 - accuracy: 0.0503\n",
            "\n",
            "Epoch 00024: loss improved from 5.57976 to 5.54936, saving model to ./model_2_weights.hdf5\n",
            "Epoch 25/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.5340 - accuracy: 0.0462\n",
            "\n",
            "Epoch 00025: loss improved from 5.54936 to 5.53434, saving model to ./model_2_weights.hdf5\n",
            "Epoch 26/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 5.4388 - accuracy: 0.0605\n",
            "\n",
            "Epoch 00026: loss improved from 5.53434 to 5.48345, saving model to ./model_2_weights.hdf5\n",
            "Epoch 27/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 5.4015 - accuracy: 0.0552\n",
            "\n",
            "Epoch 00027: loss improved from 5.48345 to 5.43096, saving model to ./model_2_weights.hdf5\n",
            "Epoch 28/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 5.3556 - accuracy: 0.0544\n",
            "\n",
            "Epoch 00028: loss improved from 5.43096 to 5.39288, saving model to ./model_2_weights.hdf5\n",
            "Epoch 29/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.3483 - accuracy: 0.0561\n",
            "\n",
            "Epoch 00029: loss improved from 5.39288 to 5.33165, saving model to ./model_2_weights.hdf5\n",
            "Epoch 30/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.2538 - accuracy: 0.0574\n",
            "\n",
            "Epoch 00030: loss improved from 5.33165 to 5.27361, saving model to ./model_2_weights.hdf5\n",
            "Epoch 31/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 5.2059 - accuracy: 0.0522\n",
            "\n",
            "Epoch 00031: loss improved from 5.27361 to 5.20874, saving model to ./model_2_weights.hdf5\n",
            "Epoch 32/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 5.1132 - accuracy: 0.0479\n",
            "\n",
            "Epoch 00032: loss improved from 5.20874 to 5.13921, saving model to ./model_2_weights.hdf5\n",
            "Epoch 33/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 5.0751 - accuracy: 0.0580\n",
            "\n",
            "Epoch 00033: loss improved from 5.13921 to 5.07868, saving model to ./model_2_weights.hdf5\n",
            "Epoch 34/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 4.9568 - accuracy: 0.0579\n",
            "\n",
            "Epoch 00034: loss improved from 5.07868 to 4.99174, saving model to ./model_2_weights.hdf5\n",
            "Epoch 35/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 4.8984 - accuracy: 0.0613\n",
            "\n",
            "Epoch 00035: loss improved from 4.99174 to 4.93232, saving model to ./model_2_weights.hdf5\n",
            "Epoch 36/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 4.8440 - accuracy: 0.0623\n",
            "\n",
            "Epoch 00036: loss improved from 4.93232 to 4.86426, saving model to ./model_2_weights.hdf5\n",
            "Epoch 37/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 4.7628 - accuracy: 0.0763\n",
            "\n",
            "Epoch 00037: loss improved from 4.86426 to 4.80733, saving model to ./model_2_weights.hdf5\n",
            "Epoch 38/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 4.7203 - accuracy: 0.0709\n",
            "\n",
            "Epoch 00038: loss improved from 4.80733 to 4.73988, saving model to ./model_2_weights.hdf5\n",
            "Epoch 39/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 4.6620 - accuracy: 0.0739\n",
            "\n",
            "Epoch 00039: loss improved from 4.73988 to 4.67168, saving model to ./model_2_weights.hdf5\n",
            "Epoch 40/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 4.5528 - accuracy: 0.0730\n",
            "\n",
            "Epoch 00040: loss improved from 4.67168 to 4.60112, saving model to ./model_2_weights.hdf5\n",
            "Epoch 41/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 4.4987 - accuracy: 0.0739\n",
            "\n",
            "Epoch 00041: loss improved from 4.60112 to 4.54161, saving model to ./model_2_weights.hdf5\n",
            "Epoch 42/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 4.4470 - accuracy: 0.0773\n",
            "\n",
            "Epoch 00042: loss improved from 4.54161 to 4.47301, saving model to ./model_2_weights.hdf5\n",
            "Epoch 43/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 4.3864 - accuracy: 0.0849\n",
            "\n",
            "Epoch 00043: loss improved from 4.47301 to 4.40576, saving model to ./model_2_weights.hdf5\n",
            "Epoch 44/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 4.3158 - accuracy: 0.0775\n",
            "\n",
            "Epoch 00044: loss improved from 4.40576 to 4.33997, saving model to ./model_2_weights.hdf5\n",
            "Epoch 45/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 4.2525 - accuracy: 0.0946\n",
            "\n",
            "Epoch 00045: loss improved from 4.33997 to 4.29929, saving model to ./model_2_weights.hdf5\n",
            "Epoch 46/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 4.1955 - accuracy: 0.0950\n",
            "\n",
            "Epoch 00046: loss improved from 4.29929 to 4.23639, saving model to ./model_2_weights.hdf5\n",
            "Epoch 47/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 4.1071 - accuracy: 0.1005\n",
            "\n",
            "Epoch 00047: loss improved from 4.23639 to 4.17673, saving model to ./model_2_weights.hdf5\n",
            "Epoch 48/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 4.0864 - accuracy: 0.1037\n",
            "\n",
            "Epoch 00048: loss improved from 4.17673 to 4.11333, saving model to ./model_2_weights.hdf5\n",
            "Epoch 49/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 4.0150 - accuracy: 0.1124\n",
            "\n",
            "Epoch 00049: loss improved from 4.11333 to 4.06906, saving model to ./model_2_weights.hdf5\n",
            "Epoch 50/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 3.9395 - accuracy: 0.1203\n",
            "\n",
            "Epoch 00050: loss improved from 4.06906 to 4.00554, saving model to ./model_2_weights.hdf5\n",
            "Epoch 51/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 3.8886 - accuracy: 0.1171\n",
            "\n",
            "Epoch 00051: loss improved from 4.00554 to 3.92964, saving model to ./model_2_weights.hdf5\n",
            "Epoch 52/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 3.8779 - accuracy: 0.1122\n",
            "\n",
            "Epoch 00052: loss improved from 3.92964 to 3.88344, saving model to ./model_2_weights.hdf5\n",
            "Epoch 53/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 3.7491 - accuracy: 0.1362\n",
            "\n",
            "Epoch 00053: loss improved from 3.88344 to 3.80208, saving model to ./model_2_weights.hdf5\n",
            "Epoch 54/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 3.6937 - accuracy: 0.1308\n",
            "\n",
            "Epoch 00054: loss improved from 3.80208 to 3.74756, saving model to ./model_2_weights.hdf5\n",
            "Epoch 55/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 3.6599 - accuracy: 0.1373\n",
            "\n",
            "Epoch 00055: loss improved from 3.74756 to 3.70616, saving model to ./model_2_weights.hdf5\n",
            "Epoch 56/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 3.5957 - accuracy: 0.1497\n",
            "\n",
            "Epoch 00056: loss improved from 3.70616 to 3.66532, saving model to ./model_2_weights.hdf5\n",
            "Epoch 57/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 3.5715 - accuracy: 0.1543\n",
            "\n",
            "Epoch 00057: loss improved from 3.66532 to 3.61106, saving model to ./model_2_weights.hdf5\n",
            "Epoch 58/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 3.5228 - accuracy: 0.1583\n",
            "\n",
            "Epoch 00058: loss improved from 3.61106 to 3.56376, saving model to ./model_2_weights.hdf5\n",
            "Epoch 59/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 3.4392 - accuracy: 0.1737\n",
            "\n",
            "Epoch 00059: loss improved from 3.56376 to 3.49503, saving model to ./model_2_weights.hdf5\n",
            "Epoch 60/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 3.3613 - accuracy: 0.1823\n",
            "\n",
            "Epoch 00060: loss improved from 3.49503 to 3.43581, saving model to ./model_2_weights.hdf5\n",
            "Epoch 61/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 3.3551 - accuracy: 0.1831\n",
            "\n",
            "Epoch 00061: loss improved from 3.43581 to 3.40766, saving model to ./model_2_weights.hdf5\n",
            "Epoch 62/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 3.3153 - accuracy: 0.1848\n",
            "\n",
            "Epoch 00062: loss improved from 3.40766 to 3.35866, saving model to ./model_2_weights.hdf5\n",
            "Epoch 63/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 3.2115 - accuracy: 0.1937\n",
            "\n",
            "Epoch 00063: loss improved from 3.35866 to 3.29525, saving model to ./model_2_weights.hdf5\n",
            "Epoch 64/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 3.2166 - accuracy: 0.1938\n",
            "\n",
            "Epoch 00064: loss improved from 3.29525 to 3.26887, saving model to ./model_2_weights.hdf5\n",
            "Epoch 65/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 3.1537 - accuracy: 0.2033\n",
            "\n",
            "Epoch 00065: loss improved from 3.26887 to 3.20367, saving model to ./model_2_weights.hdf5\n",
            "Epoch 66/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 3.1096 - accuracy: 0.2184\n",
            "\n",
            "Epoch 00066: loss improved from 3.20367 to 3.15440, saving model to ./model_2_weights.hdf5\n",
            "Epoch 67/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 3.0543 - accuracy: 0.2322\n",
            "\n",
            "Epoch 00067: loss improved from 3.15440 to 3.11902, saving model to ./model_2_weights.hdf5\n",
            "Epoch 68/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 3.0346 - accuracy: 0.2225\n",
            "\n",
            "Epoch 00068: loss improved from 3.11902 to 3.08487, saving model to ./model_2_weights.hdf5\n",
            "Epoch 69/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.9926 - accuracy: 0.2391\n",
            "\n",
            "Epoch 00069: loss improved from 3.08487 to 3.02985, saving model to ./model_2_weights.hdf5\n",
            "Epoch 70/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 2.9053 - accuracy: 0.2568\n",
            "\n",
            "Epoch 00070: loss improved from 3.02985 to 2.98198, saving model to ./model_2_weights.hdf5\n",
            "Epoch 71/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.9144 - accuracy: 0.2513\n",
            "\n",
            "Epoch 00071: loss improved from 2.98198 to 2.95101, saving model to ./model_2_weights.hdf5\n",
            "Epoch 72/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.8193 - accuracy: 0.2663\n",
            "\n",
            "Epoch 00072: loss improved from 2.95101 to 2.87104, saving model to ./model_2_weights.hdf5\n",
            "Epoch 73/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.7765 - accuracy: 0.2848\n",
            "\n",
            "Epoch 00073: loss improved from 2.87104 to 2.81062, saving model to ./model_2_weights.hdf5\n",
            "Epoch 74/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.7381 - accuracy: 0.2930\n",
            "\n",
            "Epoch 00074: loss improved from 2.81062 to 2.78259, saving model to ./model_2_weights.hdf5\n",
            "Epoch 75/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.6840 - accuracy: 0.3061\n",
            "\n",
            "Epoch 00075: loss improved from 2.78259 to 2.75307, saving model to ./model_2_weights.hdf5\n",
            "Epoch 76/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.6129 - accuracy: 0.3097\n",
            "\n",
            "Epoch 00076: loss improved from 2.75307 to 2.68308, saving model to ./model_2_weights.hdf5\n",
            "Epoch 77/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 2.6077 - accuracy: 0.3314\n",
            "\n",
            "Epoch 00077: loss improved from 2.68308 to 2.66619, saving model to ./model_2_weights.hdf5\n",
            "Epoch 78/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 2.5388 - accuracy: 0.3398\n",
            "\n",
            "Epoch 00078: loss improved from 2.66619 to 2.63908, saving model to ./model_2_weights.hdf5\n",
            "Epoch 79/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.5323 - accuracy: 0.3341\n",
            "\n",
            "Epoch 00079: loss improved from 2.63908 to 2.61903, saving model to ./model_2_weights.hdf5\n",
            "Epoch 80/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 2.4784 - accuracy: 0.3503\n",
            "\n",
            "Epoch 00080: loss improved from 2.61903 to 2.54067, saving model to ./model_2_weights.hdf5\n",
            "Epoch 81/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 2.3908 - accuracy: 0.3856\n",
            "\n",
            "Epoch 00081: loss improved from 2.54067 to 2.49097, saving model to ./model_2_weights.hdf5\n",
            "Epoch 82/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.3982 - accuracy: 0.3719\n",
            "\n",
            "Epoch 00082: loss improved from 2.49097 to 2.44707, saving model to ./model_2_weights.hdf5\n",
            "Epoch 83/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.3453 - accuracy: 0.3759\n",
            "\n",
            "Epoch 00083: loss improved from 2.44707 to 2.39138, saving model to ./model_2_weights.hdf5\n",
            "Epoch 84/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 2.3011 - accuracy: 0.3863\n",
            "\n",
            "Epoch 00084: loss improved from 2.39138 to 2.36257, saving model to ./model_2_weights.hdf5\n",
            "Epoch 85/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.2676 - accuracy: 0.4107\n",
            "\n",
            "Epoch 00085: loss improved from 2.36257 to 2.31210, saving model to ./model_2_weights.hdf5\n",
            "Epoch 86/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.2088 - accuracy: 0.4134\n",
            "\n",
            "Epoch 00086: loss improved from 2.31210 to 2.26469, saving model to ./model_2_weights.hdf5\n",
            "Epoch 87/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 2.2115 - accuracy: 0.4072\n",
            "\n",
            "Epoch 00087: loss did not improve from 2.26469\n",
            "Epoch 88/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 2.1812 - accuracy: 0.4274\n",
            "\n",
            "Epoch 00088: loss improved from 2.26469 to 2.25205, saving model to ./model_2_weights.hdf5\n",
            "Epoch 89/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 2.1096 - accuracy: 0.4359\n",
            "\n",
            "Epoch 00089: loss improved from 2.25205 to 2.17778, saving model to ./model_2_weights.hdf5\n",
            "Epoch 90/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.1156 - accuracy: 0.4294\n",
            "\n",
            "Epoch 00090: loss improved from 2.17778 to 2.17175, saving model to ./model_2_weights.hdf5\n",
            "Epoch 91/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 2.0800 - accuracy: 0.4505\n",
            "\n",
            "Epoch 00091: loss improved from 2.17175 to 2.13916, saving model to ./model_2_weights.hdf5\n",
            "Epoch 92/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 2.0323 - accuracy: 0.4394\n",
            "\n",
            "Epoch 00092: loss improved from 2.13916 to 2.08330, saving model to ./model_2_weights.hdf5\n",
            "Epoch 93/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 2.0129 - accuracy: 0.4608\n",
            "\n",
            "Epoch 00093: loss improved from 2.08330 to 2.07013, saving model to ./model_2_weights.hdf5\n",
            "Epoch 94/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.9767 - accuracy: 0.4710\n",
            "\n",
            "Epoch 00094: loss improved from 2.07013 to 2.02015, saving model to ./model_2_weights.hdf5\n",
            "Epoch 95/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.9687 - accuracy: 0.4612\n",
            "\n",
            "Epoch 00095: loss improved from 2.02015 to 1.98402, saving model to ./model_2_weights.hdf5\n",
            "Epoch 96/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.8597 - accuracy: 0.4940\n",
            "\n",
            "Epoch 00096: loss improved from 1.98402 to 1.89099, saving model to ./model_2_weights.hdf5\n",
            "Epoch 97/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.8412 - accuracy: 0.4962\n",
            "\n",
            "Epoch 00097: loss improved from 1.89099 to 1.87809, saving model to ./model_2_weights.hdf5\n",
            "Epoch 98/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.8079 - accuracy: 0.5041\n",
            "\n",
            "Epoch 00098: loss improved from 1.87809 to 1.85654, saving model to ./model_2_weights.hdf5\n",
            "Epoch 99/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.7555 - accuracy: 0.5202\n",
            "\n",
            "Epoch 00099: loss improved from 1.85654 to 1.82794, saving model to ./model_2_weights.hdf5\n",
            "Epoch 100/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.7515 - accuracy: 0.5260\n",
            "\n",
            "Epoch 00100: loss improved from 1.82794 to 1.79352, saving model to ./model_2_weights.hdf5\n",
            "Epoch 101/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.7110 - accuracy: 0.5235\n",
            "\n",
            "Epoch 00101: loss did not improve from 1.79352\n",
            "Epoch 102/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.7040 - accuracy: 0.5334\n",
            "\n",
            "Epoch 00102: loss improved from 1.79352 to 1.75291, saving model to ./model_2_weights.hdf5\n",
            "Epoch 103/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 1.7147 - accuracy: 0.5345\n",
            "\n",
            "Epoch 00103: loss improved from 1.75291 to 1.74510, saving model to ./model_2_weights.hdf5\n",
            "Epoch 104/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.6504 - accuracy: 0.5490\n",
            "\n",
            "Epoch 00104: loss improved from 1.74510 to 1.68722, saving model to ./model_2_weights.hdf5\n",
            "Epoch 105/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.6052 - accuracy: 0.5728\n",
            "\n",
            "Epoch 00105: loss improved from 1.68722 to 1.64603, saving model to ./model_2_weights.hdf5\n",
            "Epoch 106/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.5485 - accuracy: 0.5762\n",
            "\n",
            "Epoch 00106: loss improved from 1.64603 to 1.59677, saving model to ./model_2_weights.hdf5\n",
            "Epoch 107/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.5543 - accuracy: 0.5770\n",
            "\n",
            "Epoch 00107: loss did not improve from 1.59677\n",
            "Epoch 108/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.5294 - accuracy: 0.5717\n",
            "\n",
            "Epoch 00108: loss improved from 1.59677 to 1.56279, saving model to ./model_2_weights.hdf5\n",
            "Epoch 109/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.5082 - accuracy: 0.5966\n",
            "\n",
            "Epoch 00109: loss improved from 1.56279 to 1.53585, saving model to ./model_2_weights.hdf5\n",
            "Epoch 110/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.4635 - accuracy: 0.5983\n",
            "\n",
            "Epoch 00110: loss improved from 1.53585 to 1.50057, saving model to ./model_2_weights.hdf5\n",
            "Epoch 111/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.4618 - accuracy: 0.5993\n",
            "\n",
            "Epoch 00111: loss did not improve from 1.50057\n",
            "Epoch 112/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.4407 - accuracy: 0.6147\n",
            "\n",
            "Epoch 00112: loss improved from 1.50057 to 1.48448, saving model to ./model_2_weights.hdf5\n",
            "Epoch 113/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.4064 - accuracy: 0.6161\n",
            "\n",
            "Epoch 00113: loss improved from 1.48448 to 1.44125, saving model to ./model_2_weights.hdf5\n",
            "Epoch 114/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.4019 - accuracy: 0.6155\n",
            "\n",
            "Epoch 00114: loss improved from 1.44125 to 1.41532, saving model to ./model_2_weights.hdf5\n",
            "Epoch 115/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.4017 - accuracy: 0.6232\n",
            "\n",
            "Epoch 00115: loss did not improve from 1.41532\n",
            "Epoch 116/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.3804 - accuracy: 0.6147\n",
            "\n",
            "Epoch 00116: loss improved from 1.41532 to 1.39317, saving model to ./model_2_weights.hdf5\n",
            "Epoch 117/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.3318 - accuracy: 0.6215\n",
            "\n",
            "Epoch 00117: loss improved from 1.39317 to 1.37535, saving model to ./model_2_weights.hdf5\n",
            "Epoch 118/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.3662 - accuracy: 0.6265\n",
            "\n",
            "Epoch 00118: loss did not improve from 1.37535\n",
            "Epoch 119/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.2799 - accuracy: 0.6502\n",
            "\n",
            "Epoch 00119: loss improved from 1.37535 to 1.31944, saving model to ./model_2_weights.hdf5\n",
            "Epoch 120/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.3096 - accuracy: 0.6260\n",
            "\n",
            "Epoch 00120: loss improved from 1.31944 to 1.29939, saving model to ./model_2_weights.hdf5\n",
            "Epoch 121/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.2450 - accuracy: 0.6601\n",
            "\n",
            "Epoch 00121: loss improved from 1.29939 to 1.25753, saving model to ./model_2_weights.hdf5\n",
            "Epoch 122/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.2817 - accuracy: 0.6394\n",
            "\n",
            "Epoch 00122: loss did not improve from 1.25753\n",
            "Epoch 123/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.2095 - accuracy: 0.6549\n",
            "\n",
            "Epoch 00123: loss improved from 1.25753 to 1.23671, saving model to ./model_2_weights.hdf5\n",
            "Epoch 124/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.1781 - accuracy: 0.6770\n",
            "\n",
            "Epoch 00124: loss improved from 1.23671 to 1.20724, saving model to ./model_2_weights.hdf5\n",
            "Epoch 125/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.1869 - accuracy: 0.6708\n",
            "\n",
            "Epoch 00125: loss improved from 1.20724 to 1.17104, saving model to ./model_2_weights.hdf5\n",
            "Epoch 126/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 1.1261 - accuracy: 0.6936\n",
            "\n",
            "Epoch 00126: loss improved from 1.17104 to 1.12488, saving model to ./model_2_weights.hdf5\n",
            "Epoch 127/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 1.0908 - accuracy: 0.7067\n",
            "\n",
            "Epoch 00127: loss improved from 1.12488 to 1.11739, saving model to ./model_2_weights.hdf5\n",
            "Epoch 128/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 1.0566 - accuracy: 0.7118\n",
            "\n",
            "Epoch 00128: loss improved from 1.11739 to 1.10427, saving model to ./model_2_weights.hdf5\n",
            "Epoch 129/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.1708 - accuracy: 0.6662\n",
            "\n",
            "Epoch 00129: loss did not improve from 1.10427\n",
            "Epoch 130/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.2052 - accuracy: 0.6631\n",
            "\n",
            "Epoch 00130: loss did not improve from 1.10427\n",
            "Epoch 131/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.0842 - accuracy: 0.7002\n",
            "\n",
            "Epoch 00131: loss did not improve from 1.10427\n",
            "Epoch 132/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 1.0107 - accuracy: 0.7326\n",
            "\n",
            "Epoch 00132: loss improved from 1.10427 to 1.05509, saving model to ./model_2_weights.hdf5\n",
            "Epoch 133/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.9773 - accuracy: 0.7305\n",
            "\n",
            "Epoch 00133: loss improved from 1.05509 to 1.01528, saving model to ./model_2_weights.hdf5\n",
            "Epoch 134/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.9690 - accuracy: 0.7277\n",
            "\n",
            "Epoch 00134: loss improved from 1.01528 to 1.00483, saving model to ./model_2_weights.hdf5\n",
            "Epoch 135/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.9822 - accuracy: 0.7358\n",
            "\n",
            "Epoch 00135: loss did not improve from 1.00483\n",
            "Epoch 136/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.9889 - accuracy: 0.7219\n",
            "\n",
            "Epoch 00136: loss improved from 1.00483 to 0.99858, saving model to ./model_2_weights.hdf5\n",
            "Epoch 137/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.9262 - accuracy: 0.7423\n",
            "\n",
            "Epoch 00137: loss improved from 0.99858 to 0.94324, saving model to ./model_2_weights.hdf5\n",
            "Epoch 138/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.9296 - accuracy: 0.7367\n",
            "\n",
            "Epoch 00138: loss improved from 0.94324 to 0.94319, saving model to ./model_2_weights.hdf5\n",
            "Epoch 139/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.9026 - accuracy: 0.7534\n",
            "\n",
            "Epoch 00139: loss improved from 0.94319 to 0.93622, saving model to ./model_2_weights.hdf5\n",
            "Epoch 140/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 0.9216 - accuracy: 0.7438\n",
            "\n",
            "Epoch 00140: loss did not improve from 0.93622\n",
            "Epoch 141/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.8476 - accuracy: 0.7617\n",
            "\n",
            "Epoch 00141: loss improved from 0.93622 to 0.89801, saving model to ./model_2_weights.hdf5\n",
            "Epoch 142/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 0.8581 - accuracy: 0.7662\n",
            "\n",
            "Epoch 00142: loss improved from 0.89801 to 0.89752, saving model to ./model_2_weights.hdf5\n",
            "Epoch 143/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.8924 - accuracy: 0.7503\n",
            "\n",
            "Epoch 00143: loss did not improve from 0.89752\n",
            "Epoch 144/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.8869 - accuracy: 0.7599\n",
            "\n",
            "Epoch 00144: loss improved from 0.89752 to 0.88901, saving model to ./model_2_weights.hdf5\n",
            "Epoch 145/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.8394 - accuracy: 0.7736\n",
            "\n",
            "Epoch 00145: loss improved from 0.88901 to 0.86858, saving model to ./model_2_weights.hdf5\n",
            "Epoch 146/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.8681 - accuracy: 0.7642\n",
            "\n",
            "Epoch 00146: loss improved from 0.86858 to 0.86489, saving model to ./model_2_weights.hdf5\n",
            "Epoch 147/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.8425 - accuracy: 0.7675\n",
            "\n",
            "Epoch 00147: loss improved from 0.86489 to 0.86374, saving model to ./model_2_weights.hdf5\n",
            "Epoch 148/300\n",
            "29/29 [==============================] - 1s 36ms/step - loss: 0.7932 - accuracy: 0.7851\n",
            "\n",
            "Epoch 00148: loss improved from 0.86374 to 0.82620, saving model to ./model_2_weights.hdf5\n",
            "Epoch 149/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.7835 - accuracy: 0.7900\n",
            "\n",
            "Epoch 00149: loss improved from 0.82620 to 0.78720, saving model to ./model_2_weights.hdf5\n",
            "Epoch 150/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 0.7731 - accuracy: 0.7925\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.78720\n",
            "Epoch 151/300\n",
            "29/29 [==============================] - 1s 32ms/step - loss: 0.7545 - accuracy: 0.7971\n",
            "\n",
            "Epoch 00151: loss did not improve from 0.78720\n",
            "Epoch 152/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.7768 - accuracy: 0.7721\n",
            "\n",
            "Epoch 00152: loss did not improve from 0.78720\n",
            "Epoch 153/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.7660 - accuracy: 0.7949\n",
            "\n",
            "Epoch 00153: loss improved from 0.78720 to 0.78113, saving model to ./model_2_weights.hdf5\n",
            "Epoch 154/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.7688 - accuracy: 0.7860\n",
            "\n",
            "Epoch 00154: loss improved from 0.78113 to 0.77335, saving model to ./model_2_weights.hdf5\n",
            "Epoch 155/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.6961 - accuracy: 0.8079\n",
            "\n",
            "Epoch 00155: loss improved from 0.77335 to 0.72136, saving model to ./model_2_weights.hdf5\n",
            "Epoch 156/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.6666 - accuracy: 0.8172\n",
            "\n",
            "Epoch 00156: loss improved from 0.72136 to 0.68203, saving model to ./model_2_weights.hdf5\n",
            "Epoch 157/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.6683 - accuracy: 0.8071\n",
            "\n",
            "Epoch 00157: loss did not improve from 0.68203\n",
            "Epoch 158/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.6676 - accuracy: 0.8141\n",
            "\n",
            "Epoch 00158: loss did not improve from 0.68203\n",
            "Epoch 159/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.6735 - accuracy: 0.8185\n",
            "\n",
            "Epoch 00159: loss did not improve from 0.68203\n",
            "Epoch 160/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.6494 - accuracy: 0.8207\n",
            "\n",
            "Epoch 00160: loss did not improve from 0.68203\n",
            "Epoch 161/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.6320 - accuracy: 0.8259\n",
            "\n",
            "Epoch 00161: loss improved from 0.68203 to 0.66330, saving model to ./model_2_weights.hdf5\n",
            "Epoch 162/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.6270 - accuracy: 0.8376\n",
            "\n",
            "Epoch 00162: loss improved from 0.66330 to 0.65947, saving model to ./model_2_weights.hdf5\n",
            "Epoch 163/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.6218 - accuracy: 0.8324\n",
            "\n",
            "Epoch 00163: loss improved from 0.65947 to 0.64491, saving model to ./model_2_weights.hdf5\n",
            "Epoch 164/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.6488 - accuracy: 0.8239\n",
            "\n",
            "Epoch 00164: loss did not improve from 0.64491\n",
            "Epoch 165/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.6235 - accuracy: 0.8446\n",
            "\n",
            "Epoch 00165: loss did not improve from 0.64491\n",
            "Epoch 166/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.6627 - accuracy: 0.8073\n",
            "\n",
            "Epoch 00166: loss did not improve from 0.64491\n",
            "Epoch 167/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.6335 - accuracy: 0.8230\n",
            "\n",
            "Epoch 00167: loss improved from 0.64491 to 0.63976, saving model to ./model_2_weights.hdf5\n",
            "Epoch 168/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.6246 - accuracy: 0.8238\n",
            "\n",
            "Epoch 00168: loss improved from 0.63976 to 0.63013, saving model to ./model_2_weights.hdf5\n",
            "Epoch 169/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.5598 - accuracy: 0.8474\n",
            "\n",
            "Epoch 00169: loss improved from 0.63013 to 0.59891, saving model to ./model_2_weights.hdf5\n",
            "Epoch 170/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.5203 - accuracy: 0.8624\n",
            "\n",
            "Epoch 00170: loss improved from 0.59891 to 0.55758, saving model to ./model_2_weights.hdf5\n",
            "Epoch 171/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.5608 - accuracy: 0.8450\n",
            "\n",
            "Epoch 00171: loss did not improve from 0.55758\n",
            "Epoch 172/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.5647 - accuracy: 0.8428\n",
            "\n",
            "Epoch 00172: loss did not improve from 0.55758\n",
            "Epoch 173/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.5986 - accuracy: 0.8354\n",
            "\n",
            "Epoch 00173: loss did not improve from 0.55758\n",
            "Epoch 174/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.5677 - accuracy: 0.8436\n",
            "\n",
            "Epoch 00174: loss did not improve from 0.55758\n",
            "Epoch 175/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.5578 - accuracy: 0.8448\n",
            "\n",
            "Epoch 00175: loss did not improve from 0.55758\n",
            "Epoch 176/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.5493 - accuracy: 0.8436\n",
            "\n",
            "Epoch 00176: loss did not improve from 0.55758\n",
            "Epoch 177/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.5519 - accuracy: 0.8558\n",
            "\n",
            "Epoch 00177: loss improved from 0.55758 to 0.54508, saving model to ./model_2_weights.hdf5\n",
            "Epoch 178/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.4970 - accuracy: 0.8755\n",
            "\n",
            "Epoch 00178: loss improved from 0.54508 to 0.50307, saving model to ./model_2_weights.hdf5\n",
            "Epoch 179/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.4577 - accuracy: 0.8746\n",
            "\n",
            "Epoch 00179: loss improved from 0.50307 to 0.49212, saving model to ./model_2_weights.hdf5\n",
            "Epoch 180/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.4816 - accuracy: 0.8735\n",
            "\n",
            "Epoch 00180: loss did not improve from 0.49212\n",
            "Epoch 181/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.4952 - accuracy: 0.8613\n",
            "\n",
            "Epoch 00181: loss did not improve from 0.49212\n",
            "Epoch 182/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.5232 - accuracy: 0.8579\n",
            "\n",
            "Epoch 00182: loss did not improve from 0.49212\n",
            "Epoch 183/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.5301 - accuracy: 0.8557\n",
            "\n",
            "Epoch 00183: loss did not improve from 0.49212\n",
            "Epoch 184/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.5334 - accuracy: 0.8593\n",
            "\n",
            "Epoch 00184: loss did not improve from 0.49212\n",
            "Epoch 185/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.4789 - accuracy: 0.8745\n",
            "\n",
            "Epoch 00185: loss improved from 0.49212 to 0.48908, saving model to ./model_2_weights.hdf5\n",
            "Epoch 186/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.4493 - accuracy: 0.8781\n",
            "\n",
            "Epoch 00186: loss improved from 0.48908 to 0.47979, saving model to ./model_2_weights.hdf5\n",
            "Epoch 187/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.4518 - accuracy: 0.8761\n",
            "\n",
            "Epoch 00187: loss improved from 0.47979 to 0.47091, saving model to ./model_2_weights.hdf5\n",
            "Epoch 188/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.4813 - accuracy: 0.8699\n",
            "\n",
            "Epoch 00188: loss did not improve from 0.47091\n",
            "Epoch 189/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.4689 - accuracy: 0.8687\n",
            "\n",
            "Epoch 00189: loss did not improve from 0.47091\n",
            "Epoch 190/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.4552 - accuracy: 0.8815\n",
            "\n",
            "Epoch 00190: loss improved from 0.47091 to 0.46272, saving model to ./model_2_weights.hdf5\n",
            "Epoch 191/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.4168 - accuracy: 0.8896\n",
            "\n",
            "Epoch 00191: loss improved from 0.46272 to 0.42621, saving model to ./model_2_weights.hdf5\n",
            "Epoch 192/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.4201 - accuracy: 0.8906\n",
            "\n",
            "Epoch 00192: loss did not improve from 0.42621\n",
            "Epoch 193/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.4415 - accuracy: 0.8786\n",
            "\n",
            "Epoch 00193: loss did not improve from 0.42621\n",
            "Epoch 194/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.4154 - accuracy: 0.8834\n",
            "\n",
            "Epoch 00194: loss improved from 0.42621 to 0.42448, saving model to ./model_2_weights.hdf5\n",
            "Epoch 195/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3784 - accuracy: 0.9006\n",
            "\n",
            "Epoch 00195: loss improved from 0.42448 to 0.40656, saving model to ./model_2_weights.hdf5\n",
            "Epoch 196/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3977 - accuracy: 0.8969\n",
            "\n",
            "Epoch 00196: loss improved from 0.40656 to 0.39943, saving model to ./model_2_weights.hdf5\n",
            "Epoch 197/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.3922 - accuracy: 0.8930\n",
            "\n",
            "Epoch 00197: loss did not improve from 0.39943\n",
            "Epoch 198/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3678 - accuracy: 0.9085\n",
            "\n",
            "Epoch 00198: loss improved from 0.39943 to 0.39323, saving model to ./model_2_weights.hdf5\n",
            "Epoch 199/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.3986 - accuracy: 0.8933\n",
            "\n",
            "Epoch 00199: loss did not improve from 0.39323\n",
            "Epoch 200/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.3542 - accuracy: 0.9040\n",
            "\n",
            "Epoch 00200: loss improved from 0.39323 to 0.37192, saving model to ./model_2_weights.hdf5\n",
            "Epoch 201/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3328 - accuracy: 0.9160\n",
            "\n",
            "Epoch 00201: loss improved from 0.37192 to 0.34577, saving model to ./model_2_weights.hdf5\n",
            "Epoch 202/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3536 - accuracy: 0.9065\n",
            "\n",
            "Epoch 00202: loss did not improve from 0.34577\n",
            "Epoch 203/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3342 - accuracy: 0.9134\n",
            "\n",
            "Epoch 00203: loss did not improve from 0.34577\n",
            "Epoch 204/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3827 - accuracy: 0.8965\n",
            "\n",
            "Epoch 00204: loss did not improve from 0.34577\n",
            "Epoch 205/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3610 - accuracy: 0.9004\n",
            "\n",
            "Epoch 00205: loss did not improve from 0.34577\n",
            "Epoch 206/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3353 - accuracy: 0.9115\n",
            "\n",
            "Epoch 00206: loss did not improve from 0.34577\n",
            "Epoch 207/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.3413 - accuracy: 0.9139\n",
            "\n",
            "Epoch 00207: loss did not improve from 0.34577\n",
            "Epoch 208/300\n",
            "29/29 [==============================] - 1s 36ms/step - loss: 0.3890 - accuracy: 0.8952\n",
            "\n",
            "Epoch 00208: loss did not improve from 0.34577\n",
            "Epoch 209/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.4055 - accuracy: 0.8953\n",
            "\n",
            "Epoch 00209: loss did not improve from 0.34577\n",
            "Epoch 210/300\n",
            "29/29 [==============================] - 1s 36ms/step - loss: 0.4810 - accuracy: 0.8795\n",
            "\n",
            "Epoch 00210: loss did not improve from 0.34577\n",
            "Epoch 211/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.4224 - accuracy: 0.8899\n",
            "\n",
            "Epoch 00211: loss did not improve from 0.34577\n",
            "Epoch 212/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.3956 - accuracy: 0.8903\n",
            "\n",
            "Epoch 00212: loss did not improve from 0.34577\n",
            "Epoch 213/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3508 - accuracy: 0.9069\n",
            "\n",
            "Epoch 00213: loss did not improve from 0.34577\n",
            "Epoch 214/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3471 - accuracy: 0.9040\n",
            "\n",
            "Epoch 00214: loss did not improve from 0.34577\n",
            "Epoch 215/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3182 - accuracy: 0.9187\n",
            "\n",
            "Epoch 00215: loss improved from 0.34577 to 0.32340, saving model to ./model_2_weights.hdf5\n",
            "Epoch 216/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3118 - accuracy: 0.9207\n",
            "\n",
            "Epoch 00216: loss did not improve from 0.32340\n",
            "Epoch 217/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.3145 - accuracy: 0.9178\n",
            "\n",
            "Epoch 00217: loss improved from 0.32340 to 0.31181, saving model to ./model_2_weights.hdf5\n",
            "Epoch 218/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3276 - accuracy: 0.9170\n",
            "\n",
            "Epoch 00218: loss did not improve from 0.31181\n",
            "Epoch 219/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2937 - accuracy: 0.9201\n",
            "\n",
            "Epoch 00219: loss improved from 0.31181 to 0.29819, saving model to ./model_2_weights.hdf5\n",
            "Epoch 220/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.2993 - accuracy: 0.9262\n",
            "\n",
            "Epoch 00220: loss improved from 0.29819 to 0.29669, saving model to ./model_2_weights.hdf5\n",
            "Epoch 221/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.3019 - accuracy: 0.9157\n",
            "\n",
            "Epoch 00221: loss did not improve from 0.29669\n",
            "Epoch 222/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2901 - accuracy: 0.9236\n",
            "\n",
            "Epoch 00222: loss did not improve from 0.29669\n",
            "Epoch 223/300\n",
            "29/29 [==============================] - 1s 36ms/step - loss: 0.2863 - accuracy: 0.9217\n",
            "\n",
            "Epoch 00223: loss improved from 0.29669 to 0.29163, saving model to ./model_2_weights.hdf5\n",
            "Epoch 224/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2764 - accuracy: 0.9227\n",
            "\n",
            "Epoch 00224: loss did not improve from 0.29163\n",
            "Epoch 225/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.2952 - accuracy: 0.9250\n",
            "\n",
            "Epoch 00225: loss did not improve from 0.29163\n",
            "Epoch 226/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2491 - accuracy: 0.9395\n",
            "\n",
            "Epoch 00226: loss improved from 0.29163 to 0.26517, saving model to ./model_2_weights.hdf5\n",
            "Epoch 227/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2815 - accuracy: 0.9276\n",
            "\n",
            "Epoch 00227: loss did not improve from 0.26517\n",
            "Epoch 228/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2580 - accuracy: 0.9392\n",
            "\n",
            "Epoch 00228: loss improved from 0.26517 to 0.26457, saving model to ./model_2_weights.hdf5\n",
            "Epoch 229/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2383 - accuracy: 0.9392\n",
            "\n",
            "Epoch 00229: loss improved from 0.26457 to 0.24334, saving model to ./model_2_weights.hdf5\n",
            "Epoch 230/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.2431 - accuracy: 0.9370\n",
            "\n",
            "Epoch 00230: loss did not improve from 0.24334\n",
            "Epoch 231/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2665 - accuracy: 0.9265\n",
            "\n",
            "Epoch 00231: loss did not improve from 0.24334\n",
            "Epoch 232/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.2339 - accuracy: 0.9359\n",
            "\n",
            "Epoch 00232: loss improved from 0.24334 to 0.24092, saving model to ./model_2_weights.hdf5\n",
            "Epoch 233/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2205 - accuracy: 0.9489\n",
            "\n",
            "Epoch 00233: loss improved from 0.24092 to 0.23804, saving model to ./model_2_weights.hdf5\n",
            "Epoch 234/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2341 - accuracy: 0.9415\n",
            "\n",
            "Epoch 00234: loss did not improve from 0.23804\n",
            "Epoch 235/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.2717 - accuracy: 0.9303\n",
            "\n",
            "Epoch 00235: loss did not improve from 0.23804\n",
            "Epoch 236/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2309 - accuracy: 0.9424\n",
            "\n",
            "Epoch 00236: loss did not improve from 0.23804\n",
            "Epoch 237/300\n",
            "29/29 [==============================] - 1s 36ms/step - loss: 0.2425 - accuracy: 0.9381\n",
            "\n",
            "Epoch 00237: loss improved from 0.23804 to 0.23685, saving model to ./model_2_weights.hdf5\n",
            "Epoch 238/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.2264 - accuracy: 0.9444\n",
            "\n",
            "Epoch 00238: loss improved from 0.23685 to 0.23529, saving model to ./model_2_weights.hdf5\n",
            "Epoch 239/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.2190 - accuracy: 0.9390\n",
            "\n",
            "Epoch 00239: loss improved from 0.23529 to 0.23088, saving model to ./model_2_weights.hdf5\n",
            "Epoch 240/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2134 - accuracy: 0.9458\n",
            "\n",
            "Epoch 00240: loss improved from 0.23088 to 0.22213, saving model to ./model_2_weights.hdf5\n",
            "Epoch 241/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1968 - accuracy: 0.9523\n",
            "\n",
            "Epoch 00241: loss improved from 0.22213 to 0.21359, saving model to ./model_2_weights.hdf5\n",
            "Epoch 242/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.2057 - accuracy: 0.9488\n",
            "\n",
            "Epoch 00242: loss did not improve from 0.21359\n",
            "Epoch 243/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2166 - accuracy: 0.9441\n",
            "\n",
            "Epoch 00243: loss did not improve from 0.21359\n",
            "Epoch 244/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.2081 - accuracy: 0.9477\n",
            "\n",
            "Epoch 00244: loss improved from 0.21359 to 0.21232, saving model to ./model_2_weights.hdf5\n",
            "Epoch 245/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.2170 - accuracy: 0.9423\n",
            "\n",
            "Epoch 00245: loss did not improve from 0.21232\n",
            "Epoch 246/300\n",
            "29/29 [==============================] - 1s 36ms/step - loss: 0.2325 - accuracy: 0.9362\n",
            "\n",
            "Epoch 00246: loss did not improve from 0.21232\n",
            "Epoch 247/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2183 - accuracy: 0.9448\n",
            "\n",
            "Epoch 00247: loss did not improve from 0.21232\n",
            "Epoch 248/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2140 - accuracy: 0.9450\n",
            "\n",
            "Epoch 00248: loss improved from 0.21232 to 0.20513, saving model to ./model_2_weights.hdf5\n",
            "Epoch 249/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.3108 - accuracy: 0.9091\n",
            "\n",
            "Epoch 00249: loss did not improve from 0.20513\n",
            "Epoch 250/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.3217 - accuracy: 0.9132\n",
            "\n",
            "Epoch 00250: loss did not improve from 0.20513\n",
            "Epoch 251/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2808 - accuracy: 0.9225\n",
            "\n",
            "Epoch 00251: loss did not improve from 0.20513\n",
            "Epoch 252/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.3011 - accuracy: 0.9196\n",
            "\n",
            "Epoch 00252: loss did not improve from 0.20513\n",
            "Epoch 253/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.2540 - accuracy: 0.9373\n",
            "\n",
            "Epoch 00253: loss did not improve from 0.20513\n",
            "Epoch 254/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2460 - accuracy: 0.9379\n",
            "\n",
            "Epoch 00254: loss did not improve from 0.20513\n",
            "Epoch 255/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.2111 - accuracy: 0.9446\n",
            "\n",
            "Epoch 00255: loss did not improve from 0.20513\n",
            "Epoch 256/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1784 - accuracy: 0.9552\n",
            "\n",
            "Epoch 00256: loss improved from 0.20513 to 0.19096, saving model to ./model_2_weights.hdf5\n",
            "Epoch 257/300\n",
            "29/29 [==============================] - 1s 36ms/step - loss: 0.1849 - accuracy: 0.9527\n",
            "\n",
            "Epoch 00257: loss improved from 0.19096 to 0.18342, saving model to ./model_2_weights.hdf5\n",
            "Epoch 258/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1827 - accuracy: 0.9544\n",
            "\n",
            "Epoch 00258: loss did not improve from 0.18342\n",
            "Epoch 259/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1765 - accuracy: 0.9561\n",
            "\n",
            "Epoch 00259: loss improved from 0.18342 to 0.17844, saving model to ./model_2_weights.hdf5\n",
            "Epoch 260/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1767 - accuracy: 0.9501\n",
            "\n",
            "Epoch 00260: loss improved from 0.17844 to 0.17438, saving model to ./model_2_weights.hdf5\n",
            "Epoch 261/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1920 - accuracy: 0.9515\n",
            "\n",
            "Epoch 00261: loss did not improve from 0.17438\n",
            "Epoch 262/300\n",
            "29/29 [==============================] - 1s 36ms/step - loss: 0.1605 - accuracy: 0.9642\n",
            "\n",
            "Epoch 00262: loss did not improve from 0.17438\n",
            "Epoch 263/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1874 - accuracy: 0.9513\n",
            "\n",
            "Epoch 00263: loss did not improve from 0.17438\n",
            "Epoch 264/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1996 - accuracy: 0.9490\n",
            "\n",
            "Epoch 00264: loss did not improve from 0.17438\n",
            "Epoch 265/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1741 - accuracy: 0.9548\n",
            "\n",
            "Epoch 00265: loss did not improve from 0.17438\n",
            "Epoch 266/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2017 - accuracy: 0.9454\n",
            "\n",
            "Epoch 00266: loss did not improve from 0.17438\n",
            "Epoch 267/300\n",
            "29/29 [==============================] - 1s 37ms/step - loss: 0.1754 - accuracy: 0.9525\n",
            "\n",
            "Epoch 00267: loss did not improve from 0.17438\n",
            "Epoch 268/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1710 - accuracy: 0.9560\n",
            "\n",
            "Epoch 00268: loss did not improve from 0.17438\n",
            "Epoch 269/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.1859 - accuracy: 0.9527\n",
            "\n",
            "Epoch 00269: loss did not improve from 0.17438\n",
            "Epoch 270/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.2418 - accuracy: 0.9357\n",
            "\n",
            "Epoch 00270: loss did not improve from 0.17438\n",
            "Epoch 271/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.2496 - accuracy: 0.9412\n",
            "\n",
            "Epoch 00271: loss did not improve from 0.17438\n",
            "Epoch 272/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1969 - accuracy: 0.9511\n",
            "\n",
            "Epoch 00272: loss did not improve from 0.17438\n",
            "Epoch 273/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.2142 - accuracy: 0.9464\n",
            "\n",
            "Epoch 00273: loss did not improve from 0.17438\n",
            "Epoch 274/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1916 - accuracy: 0.9539\n",
            "\n",
            "Epoch 00274: loss did not improve from 0.17438\n",
            "Epoch 275/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1532 - accuracy: 0.9604\n",
            "\n",
            "Epoch 00275: loss improved from 0.17438 to 0.16127, saving model to ./model_2_weights.hdf5\n",
            "Epoch 276/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1561 - accuracy: 0.9576\n",
            "\n",
            "Epoch 00276: loss did not improve from 0.16127\n",
            "Epoch 277/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1600 - accuracy: 0.9543\n",
            "\n",
            "Epoch 00277: loss did not improve from 0.16127\n",
            "Epoch 278/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1494 - accuracy: 0.9664\n",
            "\n",
            "Epoch 00278: loss did not improve from 0.16127\n",
            "Epoch 279/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1810 - accuracy: 0.9542\n",
            "\n",
            "Epoch 00279: loss did not improve from 0.16127\n",
            "Epoch 280/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1598 - accuracy: 0.9554\n",
            "\n",
            "Epoch 00280: loss improved from 0.16127 to 0.15235, saving model to ./model_2_weights.hdf5\n",
            "Epoch 281/300\n",
            "29/29 [==============================] - 1s 36ms/step - loss: 0.1688 - accuracy: 0.9561\n",
            "\n",
            "Epoch 00281: loss did not improve from 0.15235\n",
            "Epoch 282/300\n",
            "29/29 [==============================] - 1s 36ms/step - loss: 0.1662 - accuracy: 0.9538\n",
            "\n",
            "Epoch 00282: loss did not improve from 0.15235\n",
            "Epoch 283/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1608 - accuracy: 0.9591\n",
            "\n",
            "Epoch 00283: loss did not improve from 0.15235\n",
            "Epoch 284/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.1359 - accuracy: 0.9646\n",
            "\n",
            "Epoch 00284: loss improved from 0.15235 to 0.13654, saving model to ./model_2_weights.hdf5\n",
            "Epoch 285/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1355 - accuracy: 0.9695\n",
            "\n",
            "Epoch 00285: loss improved from 0.13654 to 0.13021, saving model to ./model_2_weights.hdf5\n",
            "Epoch 286/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1323 - accuracy: 0.9671\n",
            "\n",
            "Epoch 00286: loss improved from 0.13021 to 0.12981, saving model to ./model_2_weights.hdf5\n",
            "Epoch 287/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1387 - accuracy: 0.9606\n",
            "\n",
            "Epoch 00287: loss did not improve from 0.12981\n",
            "Epoch 288/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1309 - accuracy: 0.9676\n",
            "\n",
            "Epoch 00288: loss did not improve from 0.12981\n",
            "Epoch 289/300\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 0.1291 - accuracy: 0.9677\n",
            "\n",
            "Epoch 00289: loss improved from 0.12981 to 0.12544, saving model to ./model_2_weights.hdf5\n",
            "Epoch 290/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1129 - accuracy: 0.9688\n",
            "\n",
            "Epoch 00290: loss improved from 0.12544 to 0.11758, saving model to ./model_2_weights.hdf5\n",
            "Epoch 291/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1073 - accuracy: 0.9726\n",
            "\n",
            "Epoch 00291: loss improved from 0.11758 to 0.11134, saving model to ./model_2_weights.hdf5\n",
            "Epoch 292/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1219 - accuracy: 0.9696\n",
            "\n",
            "Epoch 00292: loss did not improve from 0.11134\n",
            "Epoch 293/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1122 - accuracy: 0.9743\n",
            "\n",
            "Epoch 00293: loss did not improve from 0.11134\n",
            "Epoch 294/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1446 - accuracy: 0.9625\n",
            "\n",
            "Epoch 00294: loss did not improve from 0.11134\n",
            "Epoch 295/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1649 - accuracy: 0.9540\n",
            "\n",
            "Epoch 00295: loss did not improve from 0.11134\n",
            "Epoch 296/300\n",
            "29/29 [==============================] - 1s 35ms/step - loss: 0.1378 - accuracy: 0.9614\n",
            "\n",
            "Epoch 00296: loss did not improve from 0.11134\n",
            "Epoch 297/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1267 - accuracy: 0.9715\n",
            "\n",
            "Epoch 00297: loss did not improve from 0.11134\n",
            "Epoch 298/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1324 - accuracy: 0.9678\n",
            "\n",
            "Epoch 00298: loss did not improve from 0.11134\n",
            "Epoch 299/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1275 - accuracy: 0.9601\n",
            "\n",
            "Epoch 00299: loss did not improve from 0.11134\n",
            "Epoch 300/300\n",
            "29/29 [==============================] - 1s 34ms/step - loss: 0.1341 - accuracy: 0.9632\n",
            "\n",
            "Epoch 00300: loss did not improve from 0.11134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0DbbzeAOLTd"
      },
      "source": [
        "reverse_word_dict = {v: k for k, v in tokenizer.word_index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87u_YbI1Kba7"
      },
      "source": [
        "def gen(model,seq,max_len = 6):\n",
        "    ''' Generates a sequence given a string seq using specified model until the total sequence length\n",
        "    reaches max_len'''\n",
        "    # Tokenize the input string\n",
        "    tokenized_sent = tokenizer.texts_to_sequences([seq])\n",
        "    max_len = max_len+len(tokenized_sent[0])\n",
        "    # If sentence is not as long as the desired sentence length, we need to 'pad sequence' so that\n",
        "    # the array input shape is correct going into our LSTM. the `pad_sequences` function adds \n",
        "    # zeroes to the left side of our sequence until it becomes 19 long, the number of input features.\n",
        "    while len(tokenized_sent[0]) < max_len:\n",
        "        padded_sentence = pad_sequences(tokenized_sent[-5:],maxlen=6)\n",
        "        op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
        "        tokenized_sent[0].append(op.argmax()+1)\n",
        "    return \" \".join(map(lambda x : reverse_word_dict[x],tokenized_sent[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jIFg-P3kKbQa",
        "outputId": "bb5f800b-8fe9-4d5e-af41-e56c72ead85a"
      },
      "source": [
        "gen(model_2, 'i got')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i got in and how distracted as seems'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    }
  ]
}